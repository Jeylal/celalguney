---
title: "Notes on the methodology of the WPID"
format: html
editor: visual
image: plot.png
---

```{r}
#| warning: false
#| message: false
#| echo: false


library(tidyverse)
library(haven)
library(gt)
library(lorenz)
library(REAT)
library(cowplot)
library(plot3logit)
library(arm)
library(marginaleffects)
library(gtsummary)
library(DescTools)
library(weights)
setwd("F:/myblog/posts/Techinal notes of political cleavages and inequality")
options(scipen = 999)
```

## How far can we go with income brackets? (in progress)

To build their database on [World political cleavages and inequality](https://wpid.world/), Piketty and his team had to use electoral survey data. One problem with these sources is that they collect income data through brackets, without reporting the overall income average of the sample or the average per bracket. This Thus poses the question of how far one can go in terms of statistical analysis with only income brackets as a source of information for income. I will here explore what can be done with such a variable as well as the [technical note](https://wid.world/document/building-the-world-political-cleavages-and-inequality-database-a-new-dataset-on-electoral-behaviors-in-50-democracies-1948-2020-world-inequality-lab-technical-note-2021-01/) that Piketty et al. (2021) provides to explain how they computed the vote share for income and education decile, which they claim is one of their main contributions on political cleavages and inequality.

### A first look on the WPID dataset

The wpid is based on an impressive [dataset](https://wpid.world/resources/en/zip/wpid-micro.zip) compiling electoral survey data of 500 elections since 1948. Since the technical note takes Canada's 2015 election as an example, I will use the latter here.

```{r}
#| warning: false
#| message: false

rm(list = ls())
ca <- read_dta("ca.dta")
ca2015 <- ca %>% 
  filter(year == 2015)
rm(ca)

sort(unique(ca2015$inc))

```

Note that there is already something going on here: in the dataset, the variable income has 19 brackets/categories here whereas it has 18 in the technical note. Furthermore, income brackets "jump" from 8 to 10. I will ignore these issues and still work with this dataset, we just won't have the same results as in Piketty & al's example.

A first step in analyzing such a variable is to compute the frequency, relative frequency and the cumulative frequencies. Note that I only take the individuals who vote the the New Democratic Party (NDP) as in the technical note's example:

```{r}
#| warning: false
#| message: false


ca2015 %>% filter(votendp == 1) %>% 
  count(inc) %>% 
  drop_na() %>% 
  ungroup() %>% 
  mutate(
    cum.n = cumsum(n),
    prop = n/sum(n),
    rangeleft = lag(cumsum(prop), default = 0),
    cumrelfreqN = cumsum(prop),
    cumrelfreqInc = cumsum(inc/sum(inc))) -> table.income

table.income %>% 
  gt(caption = "Distribution of income groups")
```

We can then plot income groups against their proportion

```{r}
#| fig-cap: "Distribution of Income groups: Canadian 2015 election survey"
#| fig-cap-location: top

table.income %>% 
  ggplot()+
  aes(x = factor(inc), y = prop)+
  geom_col()+
  theme_bw()+
  xlab("Income group") -> income.group.plot

income.group.plot
```

If we plot the income group against the relative cumulative frequency, we obtain the cumulative distribution of income groups. The cumulative distribution can also be directly plotted with plot(ecdf()):

```{r}
table.income %>% 
  ggplot()+
  aes(x = factor(inc), y = cumrelfreqN)+
  geom_point()+
  theme_bw()+
  xlab("Income group")+
  labs(title = "Cumulative distribution of Income group: 2015 Canadian election survey")

```

If we plot this relative cumulative distribution of observation against the one for income, we get the Lorenz curve:

```{r}
table.income %>% 
  ggplot()+
  aes(x = cumrelfreqN, y = cumrelfreqInc)+
  geom_point()+
  geom_line()+
  geom_abline(intercept = 0, slope = 1, color = "blue")+
  theme_bw()+
  labs("Lorenz curve")
```

However, doing such graphs and computations directly from income brakcets has many flaws as we will see below.

What makes Piketty's team approach special and interesting is their systematic analysis in terms of quantile groups. This is, according to them, their main contribution and this approach has the advantage to allow for systematic comparison accross space and time. We will try to reproduce here their conversion of income group into quantiles.

In R, the decile for each observation can be added to the dataset with the function ntile():

```{r}
ca2015 <- ca2015 %>% 
  mutate(
  decile = ntile(inc, 10)
)
```

Now, the last column of ca2015 is the decile for each observation in the dataset.

```{r}
ca2015 %>% filter(votendp == 1) %>% 
  group_by(inc) %>% 
  count(decile) %>% 
  drop_na() %>% 
  ungroup() %>% 
  mutate(prop = n/sum(n),
         cumsumprop = cumsum(prop)) -> table.income.vote

table.income.vote %>% gt(caption = "Decile and income bracket")
```

However, it is straightforward to see that the ntile() function is not perfect in computing decile. More generally, computing income decile when the income variable is in brackets seems complicated, but the technical note proposes a re-weighting average approach to partially solve this problem.

To see how the re-weighing approach works, let's go back to the first table:

```{r}
table.income %>%
  mutate(id = 1:length(table.income$inc)) %>% 
  gt()
```

We can directly see the problem posed by income brackets: for example, we can observe that all of income bracket one belongs to the first decile since its relative range is between 0 and 0.0723. However, the relative range of bracket two is \[0.0723 - 0.1487\]. Some part of it belong to the first decile (\[0 - 0.1\]), but some belong to the second (\[0 - 0.2\]). The approach to compute the proportion of observations belonging to the any given decile is to compute the share of each income bracket belonging to this decile and then compute a weighted average. For example, if I want to compute the share of observation of the first decile (D10), I already know that 100% of income bracket one belongs to D10 but I need to know the share of bracket 2 (B2) belonging to D10.

To estimate this, let's assume the distribution of B2 is uniform $x \sim U[0.0723; 0.1487]$, with x the observation within this range. We want to know $P(x<0.1)$, that is to say, the probability that x belongs to the first decile. We use the uniform cumulative distribution function with parameters min = 0.073 and max = 0.1487: $P(x<0.1) = \frac{0.1-0.0723}{0.1487-0.0723} = 0.3626$. This means that 36.26% of B2 belongs to D10. Then, the weighted average for the proportion of observation within D10: $\frac{1*0.0723+0.3626*0.0764}{1+0.3626} = 0.0734$. 7.34% of total observations belong to the first decile.

Here is the computation in R

```{r}
punif(0.1, table.income$cumrelfreqN[1], table.income$cumrelfreqN[2])
```

Unfortunately, there is to my knowledge no function in R that will compute the shares automatically. I can nonetheless compute them through a tedious for loop:

```{r}
weight <- rep(NA, length(table.income$inc))

for (i in 1:length(table.income$inc)) {
  weight[i] <- ifelse(table.income$cumrelfreqN[i] < 0.1 | table.income$cumrelfreqN[i] == 1, 1,
                  ifelse(table.income$cumrelfreqN[i] > 0.1 & table.income$cumrelfreqN[i] < 0.2, punif(0.1, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]),
                         ifelse(table.income$cumrelfreqN[i] > 0.2 & table.income$cumrelfreqN[i] < 0.3, punif(0.2, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]),
                                ifelse(table.income$cumrelfreqN[i] > 0.3 & table.income$cumrelfreqN[i] < 0.4, punif(0.3, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]),
                                       ifelse(table.income$cumrelfreqN[i] > 0.4 & table.income$cumrelfreqN[i] < 0.5, punif(0.4, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]), 
                                              ifelse(table.income$cumrelfreqN[i] > 0.5 & table.income$cumrelfreqN[i] < 0.6, punif(0.5, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]), 
                                                     ifelse(table.income$cumrelfreqN[i] > 0.6 & table.income$cumrelfreqN[i] < 0.7, punif(0.6, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]),
                                                            ifelse(table.income$cumrelfreqN[i] > 0.7 & table.income$cumrelfreqN[i] < 0.8, punif(0.7, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]),
                                                                   ifelse(table.income$cumrelfreqN[i] > 0.8 & table.income$cumrelfreqN[i] < 0.9, punif(0.8, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]), 1)))))))))
}
weight

```

I replace the 0 with 1:

```{r}
weight <- ifelse(weight == 0, 1, weight)

table.income <- table.income %>% select(-cumrelfreqInc) %>% 
  mutate(share_decile = weight)

table.income %>% 
  mutate(share_rest = 1 - weight) -> table.income

table.income %>% gt()
```

There can be mistakes, but the results seem to make sense

Since I do not want to ever do this computation again (ðŸ˜…), I put all of this into a function:

```{r}
weight_share <- function(x){
  weight <- rep(NA, length(x))
  
  for (i in 1:length(x)) {
   weight[i] <-  ifelse(x[i] < 0.1 | x[i] == 1, 1,
                  ifelse(x[i] > 0.1 & x[i] < 0.2, punif(0.1, min = x[i-1], max = x[i]),
                         ifelse(x[i] > 0.2 & x[i] < 0.3, punif(0.2, min = x[i-1], max = x[i]),
                                ifelse(x[i] > 0.3 & x[i] < 0.4, punif(0.3, min = x[i-1], max = x[i]),
                                       ifelse(x[i] > 0.4 & x[i] < 0.5, punif(0.4, min = x[i-1], max = x[i]), 
                                              ifelse(x[i] > 0.5 & x[i] < 0.6, punif(0.5, min = x[i-1], max = x[i]), 
                                                     ifelse(x[i] > 0.6 & x[i] < 0.7, punif(0.6, min = x[i-1], max = x[i]),
                                                            ifelse(x[i] > 0.7 & x[i] < 0.8, punif(0.7, min = x[i-1], max = x[i]),
                                                                   ifelse(x[i] > 0.8 & x[i] < 0.9, punif(0.8, min = x[i-1], max = x[i]), 1)))))))))
  }
  weight <- ifelse(weight == 0, 1, weight)
  print(weight)
}
```

Let's check if the function works

```{r}
weight_share(x = table.income$cumrelfreqN)
```

We are almost done, there are only the weighted averages for each decile left (in progress). One further step is to compute dummy variables to show to which decile income brackets belong to. This will produce a table close to the one from the technical note.

```{r}

table.income %>% 
  mutate(d1 = ifelse(table.income$rangeleft >= 0 & table.income$rangeleft < 0.1, 1, 0),
         d2 = ifelse(table.income$rangeleft %[]% c(0.1, 0.2) | table.income$cumrelfreqN %[]% c(0.1, 0.2), 1, 0),  # the %[]% is an within bracket operator from the Desctools package. for example, x %[]% c(a, b) checks whether x belong to the interval [a, b] with a<b 
         d3 = ifelse(table.income$rangeleft %[]% c(0.2, 0.3) | table.income$cumrelfreqN %[]% c(0.2, 0.3), 1, 0),
         d4 = ifelse(table.income$rangeleft %[]% c(0.3, 0.4) | table.income$cumrelfreqN %[]% c(0.3, 0.4), 1, 0),
         d5 = ifelse(table.income$rangeleft %[]% c(0.4, 0.5) | table.income$cumrelfreqN %[]% c(0.4, 0.5), 1, 0),
         d6 = ifelse(table.income$rangeleft %[]% c(0.5, 0.6) | table.income$cumrelfreqN %[]% c(0.5, 0.6), 1, 0),
         d7 = ifelse(table.income$rangeleft %[]% c(0.6, 0.7) | table.income$cumrelfreqN %[]% c(0.6, 0.7), 1, 0),
         d8 = ifelse(table.income$rangeleft %[]% c(0.7, 0.8) | table.income$cumrelfreqN %[]% c(0.7, 0.8), 1, 0),
         d9 = ifelse(table.income$rangeleft %[]% c(0.8, 0.9) | table.income$cumrelfreqN %[]% c(0.8, 0.9), 1, 0),
         d10 = ifelse(table.income$rangeleft %[]% c(0.9, 1) | table.income$cumrelfreqN %[]% c(0.9, 1), 1, 0)) -> table.income

table.income %>% gt()
```



```{r}
weighted.mean(x = table.income$prop[table.income$d1 == 1], w = table.income$share_decile[table.income$d1 == 1]) # d1
```

This compute the proportion of the first decile directly from the table above

Let's try to compute the 3 first decile in a way that can then be put into a for loop or even into a function later
```{r}
# for d_i

weighted.mean(x = c(table.income$prop[table.income[,9] == 1]),
              w = c(table.income$share_decile[table.income[,9] == 1])) #d1

weighted.mean(x = c(table.income$prop[table.income[,10] == 1]),
              w = c(table.income$share_rest[table.income[,9] == 1 & table.income[, 10] == 1],
                    table.income$share_decile[table.income[,9] == 0 & table.income[,10] == 1])) #d2

weighted.mean(x = c(table.income$prop[table.income[,11] == 1]),
              w = c(table.income$share_rest[table.income[,11-1] == 1 & table.income[, 11] == 1], #d3
                    table.income$share_decile[table.income[11-1] == 0 & table.income[,11] == 1]))


```

Let's try the for loop

```{r}
decile_vec <- rep(NA, 10)
decile <- c()
#9:18 are the decile dummies columns in the dataset
decile_vec <- capture.output(for (i in 9:18) {

  if(i == 9){
    
  decile =  c(weighted.mean(x = c(table.income$prop[table.income[,i] == 1]),
              w = c(table.income$share_decile[table.income[,i] == 1])))
  }else{
    
   decile = c(weighted.mean(x = c(table.income$prop[table.income[,i] == 1]),
              w = c(table.income$share_rest[table.income[,i-1] == 1 & table.income[, i] == 1], 
                    table.income$share_decile[table.income[i-1] == 0 & table.income[,i] == 1])))
  }
 
cat(decile,"\n")
})

decile_vec <- as.numeric(decile_vec)
decile <- data.frame(decile = 1:10,
                         prop = decile_vec)
decile

```
This seems to work, the 3 first values are the same as the ones computed above

Normally, the vector should sum to one:
```{r}
sum(decile$prop)
decile %>% 
  ggplot()+
  aes(x = factor(decile), y = prop)+
  geom_col()
```
The decile vector does not sum to one, is it due to a mistake?

For now, let's put the for loop into a function: this function would require the decile dummies columns, the columns for the proportion, the column for decile share and the column for the rest's share:

```{r}
decile_vec <- function(data, columns, prop, share_decile, share_rest){
  
  decile_vec <- rep(NA, 10)
decile <- c()

decile_vec <- capture.output(for (i in min(columns):max(columns)) {

  if(i == min(columns)){
    
  decile =  c(weighted.mean(x = c(data$prop[data[,i] == 1]),
              w = c(data$share_decile[data[,i] == 1])))
  }else{
    
   decile = c(weighted.mean(x = c(data$prop[data[,i] == 1]),
              w = c(data$share_rest[data[,i-1] == 1 & data[, i] == 1], 
                    data$share_decile[data[i-1] == 0 & data[,i] == 1])))
  }
 
cat(decile,"\n")
})

decile_vec <- as.numeric(decile_vec)
decile <- data.frame(decile = 1:10,
                         prop = decile_vec)
decile
}
```

```{r}
decile_vec(data = table.income, columns = 9:18, prop = table.income$prop, share_decile = table.income$share_decile, share_rest = table.income$share_rest)
```
The function seems to work, but requires a specific dataframe (formatted as table.income).

To check if those operations are correct, let's compare with WPID"s dataset:

```{r}
cainc <- read_dta("ca-inc.dta")
unique(cainc$inc[cainc$year == 2015])
```

```{r}
cainc %>% filter(year == 2015 & votendp == 1) %>% 
  group_by(dinc) %>% 
  count(votendp) %>% 
  drop_na() %>% 
  ungroup() %>% 
  mutate(prop = n/sum(n))
```



Finally, one can also try to fit a "Pareto line" to the income bracket data. Vilfredo Pareto (1848-1923) is well-known for being one of the first economist to have computed inequality indices (his famous alpha coefficient) directly from personal income distribution data. His relationship $log(N) = A-\alpha log(x)$, with N the number of people earning more than income level x, is a famous relationship and almost every course on income inequality measurement starts with it.

First, a table need to be constructed from the data, we count the number of observation per income bracket and compute the inverse of the cumulative relative frequencies, which is the same as N is Pareto's equation.

```{r}
ca2015 %>% 
  count(inc) %>% 
  mutate(prop = n/sum(n),
         cumrelfreq = cumsum(prop),
         inverse_rcdf = rev(cumrelfreq)) -> paretotable

paretotable %>% 
  gt()

```

Finally, one can plot the log of the inverse of the relative cumulative distribution function against the log of the income groups:

```{r}
#| warning: false
#| message: false
paretotable %>% 
  ggplot()+
  aes(x = log(inc), y = log(rev(cumrelfreq)))+
  geom_point()+
  geom_smooth(method = "lm")+
  theme_bw()
```

We can see that the line does not fit the data very well. As explained by [Milanovic](https://glineq.blogspot.com/2015/02/what-remains-of-pareto.html?m=1), the Pareto line fits well only for the top of income distribution. In fact, the data on personal income distribution that Pareto had only collected income data of the very rich. If he had data covering more than the top 1 percent, he would have probably made similar graphs as here and as in Milanovic's blog post.

## Reproducing WPID results: France (in progress)
