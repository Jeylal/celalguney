---
title: "The vote for the left in the 2015 Swiss National Council Election: a short analysis"
format: html
editor: visual
bibliography: biblio.bib
image: image.png
execute: 
  freeze: true
---

## Note: this post is not finished and still in progress

Two years ago, Piketty and his team published a fascinating book about the evolution of political cleavages in almost all democratic countries in the world and even beyond. The publication of this book and of its [online database](https://wpid.world/resources/) got me very interested in the study of political cleavages from a political economy approach. This book contains huge amount of findings and observations, but there is one which is really emphasized by the authors: from roughly the 1980s until, the class-based political divide has become a multidimensional one incorporating a "educational" or, in Inglehart terms, a "postmaterial" or "cultural" cleavage [@gethin_political_2021] [@inglehart_silent_1971]. Very shortly, They found that the electoral support for the left shifted from the low income and low education classes to the highly educated ones. Regarding the electoral support for the right, the latter remains positively correlated with income.

This post is going to simply test this finding for Switzerland using the post-electoral survey for the National Council election in 2015. The following analysis is based on data analysis of the Swiss Election Study (Selects) of 2019. The dataset can be found [here](https://www.swissubase.ch/en/catalogue/studies/13846/18585/overview). I will test if the support for the left is linked positively with the education level and negatively with income.

```{r}
#| warning: false
#| message: false
#| echo: false
setwd("F:/myblog/posts/Vote for the left in Switzerland a simple analysis")
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
library(cli)
library(tidyverse)
library(nnet)
library(haven)
library(sjlabelled)
library(knitr)
library(gridExtra)
library(stargazer)
library(arm)
library(RColorBrewer)
library(DescTools)
library(ggplot2)
library(plot3logit)
library(marginaleffects)
library(sjPlot)
library(ggeffects)
library(GGally)
library(Amelia)
library(texreg)
library(gtsummary)
library(modelsummary)
library(viridis)
library(brms)
library(lme4)
options(scipen = 999)
```

```{r}
#| warning: false
#| message: false
#| echo: false
#| include: false

panel2019 <- read_sav("panel2019.sav")
names(panel2019)[which(colnames(panel2019) == "W1_4_f28910")] <- "Gross.monthly.hh.income"
names(panel2019)[137] <- "Education.level"

panel2019$Education.level <- ifelse(panel2019$Education.level == 14, NA, panel2019$Education.level)

panel2019$religion <- ifelse(panel2019$W1_f20750 == 1, 1,
                             ifelse(panel2019$W1_f20750 == 2, 0, NA))

panel2019$gender <- ifelse(panel2019$W1_sex == 1, 0, 1)

panel2019$vote.left <- ifelse(panel2019$W1_f10850rec %in% c(30, 50, 130, 140), 1,
                              ifelse(is.na(panel2019$W1_f10850rec), NA, 0))

panel2019$gender <- factor(panel2019$gender, levels = c(0,1), labels = c("Male", "Female"))

datareg <- panel2019 %>% dplyr::select("vote.left", "Education.level", "Gross.monthly.hh.income", "religion", "gender", "W1_f10000", "W1_f20900", "W1_age", "W1_f10250", "W1_3_smt9", "W1_3_canton_sample")

datareg$W1_3_canton_sample <- as_label(datareg$W1_3_canton_sample)

datareg <- datareg %>% 
  rename(region = "W1_f10000",
         nonreligious = "W1_f20900", 
         age = "W1_age")
```

To analyze the link between vote for the left (dependent variable) and income and education, I create a dummy variable from the variable "f10300" which asked for which party the respondent voted in the 2015 election. From this variable, I create a dummy variable taking value one if the respondant voted for either the socialist party (PS), the green (PES), solidarité or Swiss labor party (PST-POP). Note that those choices can be controversial and a matter of debate because I don't include the Social-christian party and I include the greens. This is a matter of debate if those parties can be classified as left or not, but I will not go further about this.

Regarding the independ variables, the variable "f28910" asks the gross monthly houshold income of the individual and the variable has 15 income brackets (we thus do not have directly the income of the respondant). For education, "f21310" asks the highest level of achieved education. Here is below descriptive statistics for these variables:

```{r}
#| warning: false
#| message: false
#| echo: false
#| fig-cap: Education level

datareg %>% 
  count(Education.level) %>% 
  mutate(prop = n/sum(n)) %>% 
  ggplot()+
  aes(x = as.factor(Education.level), y = prop)+
  geom_col()+
  xlab("Education level")
```

```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-cap: Gross monthly houshold income

datareg %>% count(as_label(Gross.monthly.hh.income)) %>% 
  mutate(prop = n/sum(n)) %>% 
  ggplot()+
  aes(x = `as_label(Gross.monthly.hh.income)`, y = prop) %>% 
  geom_col()+
  theme_bw()+
  xlab("")+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))


```

## First model: binary logistic regression

Let's first start with a logistic regression. I simply regress the vote for the left with income and education. I leave education and income coded as numerical variables for now, since they have enough categories this is not big problem. Of course, that would have been better if I had directly the income of each individual and not brackets. Moreover, I could still do a Pareto interpolation, but I can't due to lack of information: I don't have the average income (total and per bracket) of the sample.

The model is thus:

$$
Log(\frac{P(left)}{1 - P(left)}) = \beta_0 + \beta_1income_i + \beta_2educ_i + \epsilon_i
$$

Note that this is a very first step, I will step by step complexify this model.

Here is the regression table:

```{r}
#| warning: false
#| message: false
#| echo: false
#| results: asis

reg <- glm(data = datareg, vote.left ~ Gross.monthly.hh.income + Education.level, family = "binomial")


stargazer(reg, type = "html", title = "Model 1: binary logit regression (odd ratios)", apply.coef = exp)

```

Here is an odd ratios plot to have a better visualization of the coefficients:

```{r}
#| warning: false
#| message: false
#| echo: false
#| results: asis
library(GGally)

ggcoef_model(reg, exponentiate = TRUE)

```

Here is what this coefficient plot tells: the odds that a Swiss voter vote for a left-wing party for the National Council election of 2015 are linked negatively with income (odd ratio below 1) and positively with education (odd ratio \> 1). The coefficients are statistically significant at the one percent level, which is not a surprise because the sample is rather large. Inference will become more of a problem when I consider education or even income as a factor/qualitative variable (income here is coded in brackets, so it is rather an ordinal variable, and there is to my knowledge to way to solve this problem).

Odd ratios are known to be rather difficult to interpret. In effect, odd ratios are not what the literature calls "quantity of interest", that is to say, the quantity of the dependent variable which is the most easy to interpret. I this model, the quantity of interest is the probability to vote for a left wing party and not the odds. A lot of economists and social scientists prefer to have a look directly at the marginal effects and predicted probability to have a better view of the relationships between the variables and of the quantity of interest.

I first plot simple graphs of the estimated curves. To do so, I use the function Invlogit from the plot3logit package and put the estimated coefficient into this function. To do such graphs, one has to make the explanatory variable on the x axis to vary while the other explanatory variables are held constant. A choice has thus to be made about which fixed value of the other factors (of Income for the education level plot and conversely), I decided to choose the median value.

```{r}
#| warning: false
#| message: false
#| echo: false
#| fig-cap: Probability of voting for the left - curves from estimated coefficients


GHI = 1:15
ggplot()+
  xlim(1, 15)+
  ylim(0, 1)+
  geom_line(aes(x = GHI, y = invlogit(coef(reg)[1] + coef(reg)[2]*GHI + coef(reg)[3]*median(datareg$Education.level, na.rm = TRUE))))+
  geom_point(aes(x = jitter(Gross.monthly.hh.income), y = vote.left), data = datareg)+
  theme_bw()+
  ylab("probability")+
  xlab("Gross monthly household income group") -> prob.plot.income

EL = 1:14

ggplot()+
  xlim(1,14)+
  ylim(0,1)+
  geom_line(aes(x = EL, y = invlogit(coef(reg)[1] + coef(reg)[2]*median(datareg$Gross.monthly.hh.income, na.rm = TRUE) + coef(reg)[3]*EL)))+
  geom_point(aes(x = jitter(Education.level), y = vote.left), data = datareg)+
  theme_bw()+
  ylab("")+
  xlab("Education level") -> prob.plot.educ


cowplot::plot_grid(prob.plot.income, prob.plot.educ)
```

We can see that the slope of the education level curve is steeper than the one for income: this means that the positive link between the level of education and the probability to vote for the left is greater than the negative one for income. But let's have a look directly at the marginal effects.

There are a lot of different ways to compute marginal effects, which make the latter sometimes confusing because we don't know which type of marginal effects we are talking about. I will here consider one type of marginal effects:

-   Group-average marginal effects: slope estimates are produced for each row of the dataset used in computing the model. Then, the estimates can be grouped by the values of one of the regressor and the average for each group is computed.

A first step in group-average marginal effects in R is to use the function "slopes" which calculate estimates of the slopes (marginal effects) for each observation used to compute the model in the first place. The term "variables" is for the variable for which the slopes are estimated and "by" the argument for

```{r}
#| warning: false
#| message: false
marginaleffectseduc <- slopes(reg, variables = "Education.level")
head(marginaleffectseduc)
```

```{r}
dim(marginaleffectseduc)
```

The dataframe has 5607 rows which is the same number of observation used in the model. We can then used the different values of income level (from 1 to 15) as grouped within which estimates are averaged:

```{r}
#| warning: false
#| message: false
marginaleffectseduc %>% 
  group_by(Gross.monthly.hh.income) %>% 
  summarise(mean.slopes.educ = mean(estimate),
            conf.high = mean(conf.high), ## this is the same for the confidence interval
            conf.low = mean(conf.low)) %>% 
  ungroup() -> game.educ
head(game.educ)
```

A plot can then be made to have a better view of the average marginal effects/slopes of education for each group of income:

```{r}
#| warning: false
#| message: false
game.educ %>% 
  ggplot()+
  aes(x = Gross.monthly.hh.income, y = mean.slopes.educ)+
  geom_point()+
  geom_line()+
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.5)+
  theme_bw()+
  labs(title = "Group-average marginal effect of education for each level of income group")+
  ylab("Slopes education")
```

The slope of education level decreases on average with higher values of income group. This means that even if the probability to vote for the left is linked positively with education level, this link is weaker for higher income groups. However, it is not so much weaker because even though th line is downward slopping, it remains rather flat.

Normally, the function plot_slope should produce the same graph:

```{r}
#| warning: false
#| message: false
plot_slopes(reg, variables = "Education.level", by = "Gross.monthly.hh.income")
```

Let's do the same for income:

```{r}
#| message: false
#| warning: false

marginaleffectsinc <- slopes(reg, variables = "Gross.monthly.hh.income")
game.inc <- marginaleffectsinc %>% 
  group_by(Education.level) %>% 
  summarise(mean.slopes.inc = mean(estimate),
            conf.high = mean(conf.high),
            conf.low = mean(conf.low)) %>% 
  ungroup() -> game.inc

```

```{r}
#| message: false
#| warning: false
game.inc %>% 
  ggplot()+
  aes(x = Education.level, y = mean.slopes.inc)+
  geom_point()+
  geom_line()+
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.5)+
  theme_bw()+
  labs(title = "Group-average marginal effects of income for each level of education")+
  ylab("Slopes income")
```

Here the result is more interesting: the average marginal slope of income is negative for each education level but this average decreases with higher level of education. This implies that the probability to vote for the left is linked negatively with income group and that this negative link is strengthened by higher level of education. Rich and highly educated people have thus a very low probability to vote for the left.

Another way to look at the effect of the two independent variable on the probability to vote for the left is to look at the predictions.

```{r}
predictionseduc <- predictions(reg, variables = c("Education.level", "Gross.monthly.hh.income"))
predictionseduc <- predictions(reg, by = c("Education.level", "Gross.monthly.hh.income"))
```

```{r}
plot_predictions(reg, condition = c("Education.level", "Gross.monthly.hh.income"))+
  scale_color_brewer(palette = "Set1")+
  scale_fill_brewer(palette = "Set1")+
  theme_bw()+
  theme(legend.position = c(0.5, 0.7),
        legend.background = element_blank())+
  ylab("probability vote left") -> plotpredicteduc

plot_predictions(reg, condition = c("Gross.monthly.hh.income", "Education.level"))+
  scale_color_brewer(palette = "Set1")+
  scale_fill_brewer(palette = "Set1")+
  theme_bw()+
  theme(legend.position = c(0.8, 0.8),
        legend.background = element_blank())+
  ylab("") -> plotpredictincome

cowplot::plot_grid(plotpredicteduc, plotpredictincome)
```

Those plots are essentially the same the first one, but with the confidence interval and for different values of the regressor considered fixed for certain values.

## Complexifying the model

### Hierarchical model

One possible and interesting way to complexify the model is to include the different Swiss canton into the regression. In fact, the previous regression model can be considered as a "complete pooling" model in which I made the asumption that the slopes of the coefficients do not vary by cantons. However, cantons represents an important level of analysis in Switzerland, because elections and politics are a lot structures at the cantonal level. Cantons can thus be considered as levels in which the observed individuals in our sample are grouped.

According to @gelman_data_2007 there are two main differents ways to consider these groups in regression analysis:

1.  No pooling models: the slopes and/or the intercepts are allowed to vary across the groups freely.

2.  Partial pooling models: the slopes and/or the intercepts are allowed to vary, but they are modeled (we consider that they follow a normal distribution)

In the present analysis, I will consider a partial pooling model in which the slopes and intercepts for income and education can vary. In R, partial pooling models can be estimated with the function lmer() from the lme4 package.

```{r}
#| include: false
#| warning: false
#| message: false


glmer1 <- glmer(data = datareg, vote.left ~ Gross.monthly.hh.income + Education.level + (1 + Education.level + Gross.monthly.hh.income | W1_3_canton_sample), family=binomial(link="logit"))
```

The results of this model can be then represented through a table with tab_model()

```{r}
#| message: false
#| warning: false

tab_model(glmer1, transform = NULL, title = "Partial Pooling model", digits = 3)
```

The random effects coefficients at the cantons level for income and education are 0, which implies that there is very low variation between cantons. Furthermore, the fixed effect coefficients are almost the same than the previous model: this new model is thus not a big improvement and shows that including cantons as a group does not change the model a lot.

However, it is still interesting to plot predicted probabilities to have a better overview:

```{r}
#| fig-width: 10
#| fig-height: 10



plot_predictions(glmer1, condition = c("Education.level", "Gross.monthly.hh.income", "W1_3_canton_sample"))+facet_wrap(~W1_3_canton_sample)+
  aes(linetype = Gross.monthly.hh.income)+
  scale_colour_brewer(palette = "Set1")+
  labs(title = "Probabilités prédites selon le niveau d'éducation, pour chaque canton et pour différent niveau de revenu",
       subtitle = "Election Conseil National 2015")+
  geom_hline(yintercept = 0.5, alpha = 0.8)
```

The relationship between the predicted probabilities to vote for the left and education is positive for every cantons. There are a lot of cantons that show similar relationships and curves and whose predicted probabilities, despite the positive link with education, do not go above 0.5 (with some exception such as Aargau, Thurgau, Tessin, Valais, Glarus...). The cantons which show different patterns are Basel-Stadt, Geneva, Jura, Vaud and Neuchâtel with higher predicted probabilities.

### Adding control variables

Let's now add some control variables to the model. The models above are flawed by the fact that there are only two regressors. One important assumption of regression models are the mean independence of the covariates with the error term. The model is assumed to include all the important explanatory variables in the model. If not, the estimates will be biased, so one can suspect that the coefficients estimates for this first model are heavily biased.

I expand the first model by including a dummy for gender (1= female, 0 = male), age and nonreligiosity (takes value 1 to 7, 1 is for going to the church a lot, 7 is for never at all)

```{r}
#| results: asis

reg2 <- glm(data = datareg, vote.left ~ Gross.monthly.hh.income + Education.level + age + nonreligious + gender, family=binomial(link="logit"))
stargazer(reg, reg2, type = "html", title = "Model 2: include control variables | log(odds)")
```

The estimated income coefficient is even closer to zero after controls, suggesting an even lower link between income group and the probability to vote for the left. However, the coefficient for education level has increased. Age is very close to 0 while non religiosity is linked positively. The coefficient for gender implies that, ceteris paribus, women vote on probabiliy 0.43 more than men for the left. It would be interesting to see how the model changes if an interaction term between education and income is included. This interaction would allow the the slope for education and income to vary according to each other's values:

```{r}
#| results: asis

reg3 <- glm(data = datareg, vote.left ~ Gross.monthly.hh.income + Education.level + age + nonreligious + gender + Gross.monthly.hh.income:Education.level, family=binomial(link="logit"))

stargazer(reg, reg2, reg3, type = "html", title = "Model 3: include control variables & interaction | log(odds)")

```

### Vizualizations

```{r}
#| include: false
reg3effect <- slopes(reg3, variables = c("Gross.monthly.hh.income"))
```

```{r}

plot_slopes(reg3, variables = "Education.level", by = "Gross.monthly.hh.income")
plot_slopes(reg3, variables = "Gross.monthly.hh.income", by = "Education.level")

reg3 %>% plot_predictions(condition = c("Gross.monthly.hh.income", "Education.level")) + facet_wrap(~Education.level)

reg3 %>% plot_predictions(condition = c("Education.level", "Gross.monthly.hh.income")) + facet_wrap(~Gross.monthly.hh.income)



```
