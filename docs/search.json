[
  {
    "objectID": "posts/web scraping/web_scraping.html",
    "href": "posts/web scraping/web_scraping.html",
    "title": "Web Scraping with R",
    "section": "",
    "text": "Web scraping is nowadays rife in statistics and data science. This data collection method has become fashionable not only in computer and data science, but also in economics, sociology and social sciences in general. The reasons for the success of web scraping need hardly to be told at length: the usefulness of web scrapping, extracting data from whatever website we visit, is self-evident. Go to any Wikipedia page, and you can extract whatever data table in contains into your favorite statistical software. The potential of web scraping is huge and will be increasingly used in social sciences.\nThe main goal of this post is to show the basics of web scraping using R and the package rvest. To do so, I draw extensively on Paul C. Bauer’s Computational Social Sciences manual, which explains in depth how to do web scraping in R.\n\n\nI will here use web scraping to get video-game prices in Switzerland. The first step of web scraping is to find a webpage from which you want to collect data. To collect the prices of video-games, I need a web page from for instance a chain stores. In Switzerland, MediaMarkt is a famous chain stores selling consumer electronics and most Swiss people who still buy or order physical copy of their favorite video-games go to or order from MediaMarkt.\nFor the anecdote, at the time and for many years (at least until I was 18 in the 2010s), MediaMarkt used to sell video-games slightly below market prices and some stores in Geneva even sometimes sold copies before the release date. The situation has much changed since and the stores are now barely able to sell and distribute physical copies day one (it was the case this year at least for Wo Long and Dead Space 2 remake).\nI search then, on MediaMarkt website, the webpage dedicated to video-games. The webpages are separated by video-games platforms/consoles, so I choose to focus on Playstation 5 games. I store the link in an object called url. Then, the web page can be read in R using read_html():\n\nurl &lt;- \"https://www.mediamarkt.ch/fr/category/_jeux-ps5-772119.html?searchParams=&sort=&view=PRODUCTGRID&page=1\"\nmediamarkt &lt;- read_html(url)\n\nThe next step is to search and select each videogame price as well as the videogame’s titles using the chrome extension selectorgadget. This step is a bit tricky and is a pain because selectorgadget is a chrome-only extension. So if you don’t have chrome, you have to install it and then add selector gadget on it. To use selectorgadget, just click on what you want to collect on the webpage and then copy the css selector which is on the left of the selectorgadget bar.\n\n\n\nUsing selectorgadget\n\n\nThen, you just have to paste it into the html_nodes function (here .info-wrapper for the price, then .product-link for the title).\n\nprice &lt;- html_nodes(mediamarkt, \".info-wrapper\")\ngame &lt;- html_nodes(mediamarkt, \".product-link\")\n\nThen, html_text() will extract the information we want:\n\nprice &lt;- html_text(price, trim = TRUE)\ngame &lt;- html_text(game, trim = TRUE)\nhead(price)\n\n[1] \"274.95\\r\\n\\t\\r\\n\\t\\t\\tTVA compris, plus frais d’expédition 0.-\"\n[2] \"72.95\\r\\n\\t\\r\\n\\t\\t\\tTVA compris, plus frais d’expédition 0.-\" \n[3] \"67.95\\r\\n\\t\\r\\n\\t\\t\\tTVA compris, plus frais d’expédition 0.-\" \n[4] \"69.95\\r\\n\\t\\r\\n\\t\\t\\tTVA compris, plus frais d’expédition 0.-\" \n[5] \"73.95\\r\\n\\t\\r\\n\\t\\t\\tTVA compris, plus frais d’expédition 0.-\" \n[6] \"54.95\\r\\n\\t\\r\\n\\t\\t\\tTVA compris, plus frais d’expédition 0.-\" \n\nhead(game)\n\n[1] \"Marvel's Spider-Man 2: Édition Collector (CiaB) - PlayStation 5 - Allemand, Français, Italien\"\n[2] \"Sports FC 24 : Édition Standard - PlayStation 5 - Allemand, Français, Italien\"                \n[3] \"Marvel’s Spider-Man: Miles Morales - PlayStation 5 - Allemand, Français, Italien\"             \n[4] \"STAR WARS Jedi : Survivor - PlayStation 5 - Allemand, Français, Italien\"                      \n[5] \"F1 23 - PlayStation 5 - Allemand, Français, Italien\"                                          \n[6] \"Ratchet & Clank : Rift Apart - PlayStation 5 - Allemand, Français, Italien\"                   \n\n\nIt worked well for the game titles (even if they could be simplified), but not so much for prices. We need to extract the latter from the text in the price object. This can be done using function gsub() and some Regex manipulation:\n\nprice_final &lt;- as.numeric(gsub(\"([0.001-9]+).*$\", \"\\\\1\", price))\n#\"([0.001-9]+).*$\" means: \"keep only characters from 0.001 to 9 multiple times\" it will thus extract the number from the character list we got with html_text()\nprice_final\n\n [1] 274.95  72.95  67.95  69.95  73.95  54.95  44.95  69.95  62.95  59.95\n[11]  69.95  74.95  54.95  40.95  72.95  29.95  69.95  14.00  72.95  26.95\n[21]  22.00  24.00  74.95  89.95\n\n\nSimilarly, the video-game titles can be simplified using some Regex:\n\ngame_final &lt;- gsub(\"- PlayStation 5 - Allemand.*\", \"\", game)\n\nAnd finally, prices and titles can be combined in a dataframe\n\ndata &lt;- data.frame(game_final, price_final)\ndata %&gt;% gt()\n\n\n\n\n\n\n\ngame_final\nprice_final\n\n\n\n\nMarvel's Spider-Man 2: Édition Collector (CiaB)\n274.95\n\n\nSports FC 24 : Édition Standard\n72.95\n\n\nMarvel’s Spider-Man: Miles Morales\n67.95\n\n\nSTAR WARS Jedi : Survivor\n69.95\n\n\nF1 23\n73.95\n\n\nRatchet & Clank : Rift Apart\n54.95\n\n\nElden Ring : Édition Standard\n44.95\n\n\nStreet Fighter 6\n69.95\n\n\nHogwarts Legacy\n62.95\n\n\nNeed for Speed Unbound\n59.95\n\n\nDiablo IV\n69.95\n\n\nMarvel's Spider-Man 2\n74.95\n\n\nThe Last of Us Part I\n54.95\n\n\nMotoGP 23 : Édition Day One\n40.95\n\n\nGran Turismo 7\n72.95\n\n\nDer Herr der Ringe: Gollum\n29.95\n\n\nDiablo IV - PlayStation 5 - Français\n69.95\n\n\nNew Tales from the Borderlands: Deluxe Edition\n14.00\n\n\nMadden NFL 24 - PlayStation 5 - English\n72.95\n\n\nFar Cry 6\n26.95\n\n\nAssassin's Creed Valhalla\n22.00\n\n\nAssassin's Creed : Valhalla - Édition Ragnarök\n24.00\n\n\nWild Hearts\n74.95\n\n\nMortal Kombat 1: Premium Edition\n89.95\n\n\n\n\n\n\n\nWe have thus collected every game’s title and price on the first page of MediaMarkt Playstation 5 games.\n\ndata %&gt;% \n  ggplot()+\n  aes(y = reorder(game_final, -price_final), x = price_final)+\n  geom_col()+\n  geom_text(aes(label = price_final), size = 3, hjust = 1.4, color = \"white\")+\n  theme_minimal(base_size = 11)+\n  labs(title = \"Playstation 5 games\",\n    y = \"\", x = \"price in swiss francs\")\n\n\n\n\n\n\n\n\n\n\n\nWe have until now collected data only the first page of the website, but ideally we want to be able to collect data on all pages automatically. This can be done as follow:\nFirst, create a function which will automatically most of the steps: reading the url (read_html), the css nodes found with the chrome extension SelectorGadget and extracting the information of interest:\n\nscrape_website &lt;- function(url){\n  \n  website &lt;- read_html(url)\n  \n  title &lt;- html_nodes(website, \".product-link\")\n  price &lt;- html_nodes(website, \".info-wrapper\")\n  \n  \n  df &lt;- data.frame(game = gsub(\"- PlayStation 5 - Allemand.*\", \"\", html_text(title, trim = TRUE)),\n                   price = as.numeric(gsub(\"([0.005-9]+).*$\", \"\\\\1\", html_text(price, trim = TRUE))))\n  \n  return(df)\n}\n\nBut what about the url? In our example, there are 21 pages in total, resulting in 21 different url. Fortunately, since those pages are in fact a list of products, their patterns are very similar. The only difference between the pages are the “page=#” at the end, with # the number of the page. We can thus define base_url as the recurring url pattern common to all the 21 pages and then number of each particular page (pages object):\n\nbase_url &lt;- \"https://www.mediamarkt.ch/fr/category/_jeux-ps5-772119.html?searchParams=&sort=&view=PRODUCTGRID&page=\"\n\npages &lt;- seq(1, 21, by = 1)\n\nThis is an example on how to extract data using the function for the first page:\n\nurl = paste(base_url, pages[1], sep = \"\")\ndatasets &lt;- list()\ndatasets[[1]] &lt;- scrape_website(url = url)\n\nFinally, to perform web scrapping on all the pages, Paul Bauer proposes the following for loop code\n\ndatasets &lt;- list()\nfor (i in 1:length(pages)){\n  \n    # informative message about progress of loop\n      message(i, '/', length(pages))\n  \n    # prepare URL\n      url &lt;- paste(base_url, pages[i], sep=\"\")\n      \n    # scrape website\n      datasets[[i+1]] &lt;- scrape_website(url)\n      \n    # wait a couple of seconds between URL calls\n      Sys.sleep(0.2)\n}\n\nHowever, I propose to simplify the code by using the map() function from the purrr package:\n\nurl &lt;- paste(base_url, pages[1:21], sep = \"\") #create a vector with all the pages url\n\ndata &lt;- url %&gt;% \n  map_df(scrape_website) #apply the scrape_website function for each element of the url vector and store into a dataframe\n\nWe thus have all the data we wanted! Let’s now have a quick look at what we collected.\n\ndata %&gt;% \n  summarise(mean = mean(price, na.rm = TRUE),\n            median = median(price, na.rm = TRUE),\n            sd = sd(price, na.rm = TRUE),\n            min = min(price, na.rm = TRUE),\n            max = max(price, na.rm = TRUE),\n            ) %&gt;% \n  gt(caption = \"Video-games summary statistics (CHF)\")\n\n\n\n\n\nVideo-games summary statistics (CHF)\n\n\nmean\nmedian\nsd\nmin\nmax\n\n\n\n\n37.23819\n34.95\n23.70447\n5\n300.95\n\n\n\n\n\n\n\n\ndata %&gt;% \n  filter(price &lt;= 90) %&gt;% \n  ggplot()+\n  aes(x = price) %&gt;% \n  geom_histogram(binwidth = 9, color = \"white\", fill = \"darkblue\", alpha = 0.8)+\n  scale_x_continuous(breaks = seq(0, 90, by = 5))+\n  theme_minimal(base_size = 13)+\n  labs(title = \"Video-game price\",\n       subtitle = \"Switzerland, Playstation 5 games, July 2023\",\n       y = \"\", x = \"Swiss Francs (CHF)\")\n\n\n\n\n\n\n\n\n\n\n\nWhen browsing the internet, it often happens that we come across tables but without any possibility to export this table. Web scraping allows to easily read those tables into R.\nI will here import data from a table on the results of the famous 1992 Swiss voting on the adhesion to the European Economic Area. The Swiss government website provides a table here. I am almost certain that the Swiss government provides an excel or csv file on this but since I am too lazy to search for it (for my defense, the website page should allow to export directly the table into csv or excel), I will use web scraping to read this table in R and then extract the data into a dataframe.\nThe first step is the same as above: read the url to import the html code into R:\n\nurl &lt;- \"https://www.bk.admin.ch/ch/f/pore/va/19921206/can388.html\"\npage &lt;- read_html(url)\n\nThen, html_table() can directly import the table information\n\ntable &lt;- html_table(page, header = TRUE)\n\ndata &lt;- table[[1]] #extract the table dataframe from the list\nhead(data) %&gt;% \n  gt()\n\n\n\n\n\n\n\nCanton\nElecteurs\nVotants\n% Particip.\nOui\nNon\n% Oui\n% Non\n\n\n\n\nZurich\n768'126\n618'209\n80.48%\n297'503\n316'154\n48.5%\n51.5%\n\n\nBerne\n686'459\n540'179\n78.69%\n255'224\n281'026\n47.6%\n52.4%\n\n\nLucerne\n224'458\n181'614\n80.91%\n70'878\n109'447\n39.3%\n60.7%\n\n\nUri\n25'290\n19'816\n78.36%\n4'943\n14'728\n25.1%\n74.9%\n\n\nSchwyz\n77'278\n64'315\n83.23%\n17'094\n46'962\n26.7%\n73.3%\n\n\nObwald\n20'713\n16'940\n81.78%\n4'737\n12'062\n28.2%\n71.8%\n\n\n\n\n\n\n\nThere is one problem here: the numbers either contain “‘” or “%”. As a result, R read them as characters and it will not be possible to convert those number into numeric format unless all the’ and % are deleted.\n\ndata2 &lt;- as_tibble(lapply(data, function(x) {gsub(\"'|%\", \"\", x)})) # collapse all ' and % in the values to transform them into numeric\n\ndata2 &lt;- data %&gt;% \n  map_df(function(x) {gsub(\"'|%\", \"\", x)}) # same using map_df from dplyr\n\nAll the ’ and % symbols are now removed. The last step is to convert the values into numeric values:\n\ndata2[,-1] &lt;- data2[,-1] %&gt;% \n            map_df(as.numeric) ## convert all columns into numeric except for canton (first column)\n\nstr(data2)\n\ntibble [27 × 8] (S3: tbl_df/tbl/data.frame)\n $ Canton     : chr [1:27] \"Zurich\" \"Berne\" \"Lucerne\" \"Uri\" ...\n $ Electeurs  : num [1:27] 768126 686459 224458 25290 77278 ...\n $ Votants    : num [1:27] 618209 540179 181614 19816 64315 ...\n $ % Particip.: num [1:27] 80.5 78.7 80.9 78.4 83.2 ...\n $ Oui        : num [1:27] 297503 255224 70878 4943 17094 ...\n $ Non        : num [1:27] 316154 281026 109447 14728 46962 ...\n $ % Oui      : num [1:27] 48.5 47.6 39.3 25.1 26.7 28.2 33.9 31.9 43.8 64.9 ...\n $ % Non      : num [1:27] 51.5 52.4 60.7 74.9 73.3 71.8 66.1 68.1 56.2 35.1 ..."
  },
  {
    "objectID": "posts/web scraping/web_scraping.html#an-introduction-to-web-scraping-in-r",
    "href": "posts/web scraping/web_scraping.html#an-introduction-to-web-scraping-in-r",
    "title": "Web Scraping with R",
    "section": "",
    "text": "Web scraping is nowadays rife in statistics and data science. This data collection method has become fashionable not only in computer and data science, but also in economics, sociology and social sciences in general. The reasons for the success of web scraping need hardly to be told at length: the usefulness of web scrapping, extracting data from whatever website we visit, is self-evident. Go to any Wikipedia page, and you can extract whatever data table in contains into your favorite statistical software. The potential of web scraping is huge and will be increasingly used in social sciences.\nThe main goal of this post is to show the basics of web scraping using R and the package rvest. To do so, I draw extensively on Paul C. Bauer’s Computational Social Sciences manual, which explains in depth how to do web scraping in R.\n\n\nI will here use web scraping to get video-game prices in Switzerland. The first step of web scraping is to find a webpage from which you want to collect data. To collect the prices of video-games, I need a web page from for instance a chain stores. In Switzerland, MediaMarkt is a famous chain stores selling consumer electronics and most Swiss people who still buy or order physical copy of their favorite video-games go to or order from MediaMarkt.\nFor the anecdote, at the time and for many years (at least until I was 18 in the 2010s), MediaMarkt used to sell video-games slightly below market prices and some stores in Geneva even sometimes sold copies before the release date. The situation has much changed since and the stores are now barely able to sell and distribute physical copies day one (it was the case this year at least for Wo Long and Dead Space 2 remake).\nI search then, on MediaMarkt website, the webpage dedicated to video-games. The webpages are separated by video-games platforms/consoles, so I choose to focus on Playstation 5 games. I store the link in an object called url. Then, the web page can be read in R using read_html():\n\nurl &lt;- \"https://www.mediamarkt.ch/fr/category/_jeux-ps5-772119.html?searchParams=&sort=&view=PRODUCTGRID&page=1\"\nmediamarkt &lt;- read_html(url)\n\nThe next step is to search and select each videogame price as well as the videogame’s titles using the chrome extension selectorgadget. This step is a bit tricky and is a pain because selectorgadget is a chrome-only extension. So if you don’t have chrome, you have to install it and then add selector gadget on it. To use selectorgadget, just click on what you want to collect on the webpage and then copy the css selector which is on the left of the selectorgadget bar.\n\n\n\nUsing selectorgadget\n\n\nThen, you just have to paste it into the html_nodes function (here .info-wrapper for the price, then .product-link for the title).\n\nprice &lt;- html_nodes(mediamarkt, \".info-wrapper\")\ngame &lt;- html_nodes(mediamarkt, \".product-link\")\n\nThen, html_text() will extract the information we want:\n\nprice &lt;- html_text(price, trim = TRUE)\ngame &lt;- html_text(game, trim = TRUE)\nhead(price)\n\n[1] \"274.95\\r\\n\\t\\r\\n\\t\\t\\tTVA compris, plus frais d’expédition 0.-\"\n[2] \"72.95\\r\\n\\t\\r\\n\\t\\t\\tTVA compris, plus frais d’expédition 0.-\" \n[3] \"67.95\\r\\n\\t\\r\\n\\t\\t\\tTVA compris, plus frais d’expédition 0.-\" \n[4] \"69.95\\r\\n\\t\\r\\n\\t\\t\\tTVA compris, plus frais d’expédition 0.-\" \n[5] \"73.95\\r\\n\\t\\r\\n\\t\\t\\tTVA compris, plus frais d’expédition 0.-\" \n[6] \"54.95\\r\\n\\t\\r\\n\\t\\t\\tTVA compris, plus frais d’expédition 0.-\" \n\nhead(game)\n\n[1] \"Marvel's Spider-Man 2: Édition Collector (CiaB) - PlayStation 5 - Allemand, Français, Italien\"\n[2] \"Sports FC 24 : Édition Standard - PlayStation 5 - Allemand, Français, Italien\"                \n[3] \"Marvel’s Spider-Man: Miles Morales - PlayStation 5 - Allemand, Français, Italien\"             \n[4] \"STAR WARS Jedi : Survivor - PlayStation 5 - Allemand, Français, Italien\"                      \n[5] \"F1 23 - PlayStation 5 - Allemand, Français, Italien\"                                          \n[6] \"Ratchet & Clank : Rift Apart - PlayStation 5 - Allemand, Français, Italien\"                   \n\n\nIt worked well for the game titles (even if they could be simplified), but not so much for prices. We need to extract the latter from the text in the price object. This can be done using function gsub() and some Regex manipulation:\n\nprice_final &lt;- as.numeric(gsub(\"([0.001-9]+).*$\", \"\\\\1\", price))\n#\"([0.001-9]+).*$\" means: \"keep only characters from 0.001 to 9 multiple times\" it will thus extract the number from the character list we got with html_text()\nprice_final\n\n [1] 274.95  72.95  67.95  69.95  73.95  54.95  44.95  69.95  62.95  59.95\n[11]  69.95  74.95  54.95  40.95  72.95  29.95  69.95  14.00  72.95  26.95\n[21]  22.00  24.00  74.95  89.95\n\n\nSimilarly, the video-game titles can be simplified using some Regex:\n\ngame_final &lt;- gsub(\"- PlayStation 5 - Allemand.*\", \"\", game)\n\nAnd finally, prices and titles can be combined in a dataframe\n\ndata &lt;- data.frame(game_final, price_final)\ndata %&gt;% gt()\n\n\n\n\n\n\n\ngame_final\nprice_final\n\n\n\n\nMarvel's Spider-Man 2: Édition Collector (CiaB)\n274.95\n\n\nSports FC 24 : Édition Standard\n72.95\n\n\nMarvel’s Spider-Man: Miles Morales\n67.95\n\n\nSTAR WARS Jedi : Survivor\n69.95\n\n\nF1 23\n73.95\n\n\nRatchet & Clank : Rift Apart\n54.95\n\n\nElden Ring : Édition Standard\n44.95\n\n\nStreet Fighter 6\n69.95\n\n\nHogwarts Legacy\n62.95\n\n\nNeed for Speed Unbound\n59.95\n\n\nDiablo IV\n69.95\n\n\nMarvel's Spider-Man 2\n74.95\n\n\nThe Last of Us Part I\n54.95\n\n\nMotoGP 23 : Édition Day One\n40.95\n\n\nGran Turismo 7\n72.95\n\n\nDer Herr der Ringe: Gollum\n29.95\n\n\nDiablo IV - PlayStation 5 - Français\n69.95\n\n\nNew Tales from the Borderlands: Deluxe Edition\n14.00\n\n\nMadden NFL 24 - PlayStation 5 - English\n72.95\n\n\nFar Cry 6\n26.95\n\n\nAssassin's Creed Valhalla\n22.00\n\n\nAssassin's Creed : Valhalla - Édition Ragnarök\n24.00\n\n\nWild Hearts\n74.95\n\n\nMortal Kombat 1: Premium Edition\n89.95\n\n\n\n\n\n\n\nWe have thus collected every game’s title and price on the first page of MediaMarkt Playstation 5 games.\n\ndata %&gt;% \n  ggplot()+\n  aes(y = reorder(game_final, -price_final), x = price_final)+\n  geom_col()+\n  geom_text(aes(label = price_final), size = 3, hjust = 1.4, color = \"white\")+\n  theme_minimal(base_size = 11)+\n  labs(title = \"Playstation 5 games\",\n    y = \"\", x = \"price in swiss francs\")\n\n\n\n\n\n\n\n\n\n\n\nWe have until now collected data only the first page of the website, but ideally we want to be able to collect data on all pages automatically. This can be done as follow:\nFirst, create a function which will automatically most of the steps: reading the url (read_html), the css nodes found with the chrome extension SelectorGadget and extracting the information of interest:\n\nscrape_website &lt;- function(url){\n  \n  website &lt;- read_html(url)\n  \n  title &lt;- html_nodes(website, \".product-link\")\n  price &lt;- html_nodes(website, \".info-wrapper\")\n  \n  \n  df &lt;- data.frame(game = gsub(\"- PlayStation 5 - Allemand.*\", \"\", html_text(title, trim = TRUE)),\n                   price = as.numeric(gsub(\"([0.005-9]+).*$\", \"\\\\1\", html_text(price, trim = TRUE))))\n  \n  return(df)\n}\n\nBut what about the url? In our example, there are 21 pages in total, resulting in 21 different url. Fortunately, since those pages are in fact a list of products, their patterns are very similar. The only difference between the pages are the “page=#” at the end, with # the number of the page. We can thus define base_url as the recurring url pattern common to all the 21 pages and then number of each particular page (pages object):\n\nbase_url &lt;- \"https://www.mediamarkt.ch/fr/category/_jeux-ps5-772119.html?searchParams=&sort=&view=PRODUCTGRID&page=\"\n\npages &lt;- seq(1, 21, by = 1)\n\nThis is an example on how to extract data using the function for the first page:\n\nurl = paste(base_url, pages[1], sep = \"\")\ndatasets &lt;- list()\ndatasets[[1]] &lt;- scrape_website(url = url)\n\nFinally, to perform web scrapping on all the pages, Paul Bauer proposes the following for loop code\n\ndatasets &lt;- list()\nfor (i in 1:length(pages)){\n  \n    # informative message about progress of loop\n      message(i, '/', length(pages))\n  \n    # prepare URL\n      url &lt;- paste(base_url, pages[i], sep=\"\")\n      \n    # scrape website\n      datasets[[i+1]] &lt;- scrape_website(url)\n      \n    # wait a couple of seconds between URL calls\n      Sys.sleep(0.2)\n}\n\nHowever, I propose to simplify the code by using the map() function from the purrr package:\n\nurl &lt;- paste(base_url, pages[1:21], sep = \"\") #create a vector with all the pages url\n\ndata &lt;- url %&gt;% \n  map_df(scrape_website) #apply the scrape_website function for each element of the url vector and store into a dataframe\n\nWe thus have all the data we wanted! Let’s now have a quick look at what we collected.\n\ndata %&gt;% \n  summarise(mean = mean(price, na.rm = TRUE),\n            median = median(price, na.rm = TRUE),\n            sd = sd(price, na.rm = TRUE),\n            min = min(price, na.rm = TRUE),\n            max = max(price, na.rm = TRUE),\n            ) %&gt;% \n  gt(caption = \"Video-games summary statistics (CHF)\")\n\n\n\n\n\nVideo-games summary statistics (CHF)\n\n\nmean\nmedian\nsd\nmin\nmax\n\n\n\n\n37.23819\n34.95\n23.70447\n5\n300.95\n\n\n\n\n\n\n\n\ndata %&gt;% \n  filter(price &lt;= 90) %&gt;% \n  ggplot()+\n  aes(x = price) %&gt;% \n  geom_histogram(binwidth = 9, color = \"white\", fill = \"darkblue\", alpha = 0.8)+\n  scale_x_continuous(breaks = seq(0, 90, by = 5))+\n  theme_minimal(base_size = 13)+\n  labs(title = \"Video-game price\",\n       subtitle = \"Switzerland, Playstation 5 games, July 2023\",\n       y = \"\", x = \"Swiss Francs (CHF)\")\n\n\n\n\n\n\n\n\n\n\n\nWhen browsing the internet, it often happens that we come across tables but without any possibility to export this table. Web scraping allows to easily read those tables into R.\nI will here import data from a table on the results of the famous 1992 Swiss voting on the adhesion to the European Economic Area. The Swiss government website provides a table here. I am almost certain that the Swiss government provides an excel or csv file on this but since I am too lazy to search for it (for my defense, the website page should allow to export directly the table into csv or excel), I will use web scraping to read this table in R and then extract the data into a dataframe.\nThe first step is the same as above: read the url to import the html code into R:\n\nurl &lt;- \"https://www.bk.admin.ch/ch/f/pore/va/19921206/can388.html\"\npage &lt;- read_html(url)\n\nThen, html_table() can directly import the table information\n\ntable &lt;- html_table(page, header = TRUE)\n\ndata &lt;- table[[1]] #extract the table dataframe from the list\nhead(data) %&gt;% \n  gt()\n\n\n\n\n\n\n\nCanton\nElecteurs\nVotants\n% Particip.\nOui\nNon\n% Oui\n% Non\n\n\n\n\nZurich\n768'126\n618'209\n80.48%\n297'503\n316'154\n48.5%\n51.5%\n\n\nBerne\n686'459\n540'179\n78.69%\n255'224\n281'026\n47.6%\n52.4%\n\n\nLucerne\n224'458\n181'614\n80.91%\n70'878\n109'447\n39.3%\n60.7%\n\n\nUri\n25'290\n19'816\n78.36%\n4'943\n14'728\n25.1%\n74.9%\n\n\nSchwyz\n77'278\n64'315\n83.23%\n17'094\n46'962\n26.7%\n73.3%\n\n\nObwald\n20'713\n16'940\n81.78%\n4'737\n12'062\n28.2%\n71.8%\n\n\n\n\n\n\n\nThere is one problem here: the numbers either contain “‘” or “%”. As a result, R read them as characters and it will not be possible to convert those number into numeric format unless all the’ and % are deleted.\n\ndata2 &lt;- as_tibble(lapply(data, function(x) {gsub(\"'|%\", \"\", x)})) # collapse all ' and % in the values to transform them into numeric\n\ndata2 &lt;- data %&gt;% \n  map_df(function(x) {gsub(\"'|%\", \"\", x)}) # same using map_df from dplyr\n\nAll the ’ and % symbols are now removed. The last step is to convert the values into numeric values:\n\ndata2[,-1] &lt;- data2[,-1] %&gt;% \n            map_df(as.numeric) ## convert all columns into numeric except for canton (first column)\n\nstr(data2)\n\ntibble [27 × 8] (S3: tbl_df/tbl/data.frame)\n $ Canton     : chr [1:27] \"Zurich\" \"Berne\" \"Lucerne\" \"Uri\" ...\n $ Electeurs  : num [1:27] 768126 686459 224458 25290 77278 ...\n $ Votants    : num [1:27] 618209 540179 181614 19816 64315 ...\n $ % Particip.: num [1:27] 80.5 78.7 80.9 78.4 83.2 ...\n $ Oui        : num [1:27] 297503 255224 70878 4943 17094 ...\n $ Non        : num [1:27] 316154 281026 109447 14728 46962 ...\n $ % Oui      : num [1:27] 48.5 47.6 39.3 25.1 26.7 28.2 33.9 31.9 43.8 64.9 ...\n $ % Non      : num [1:27] 51.5 52.4 60.7 74.9 73.3 71.8 66.1 68.1 56.2 35.1 ..."
  },
  {
    "objectID": "posts/Vote for the left in Switzerland a simple analysis/select2019analysis.html",
    "href": "posts/Vote for the left in Switzerland a simple analysis/select2019analysis.html",
    "title": "Income, education and voting outcome: a cross-sectional analysis of voting outcome in Switzerland",
    "section": "",
    "text": "Two years ago, Piketty and his team published a fascinating book about the evolution of political cleavages in almost all democratic countries in the world and even beyond. The publication of this book and of its online database got me very interested in the study of political cleavages from a political economy approach. This book contains huge amount of findings and observations, but there is one which is really emphasized by the authors: from roughly the 1980s until, the class-based political divide has become a multidimensional one incorporating a “educational” or, in Inglehart terms, a “postmaterial” or “cultural” cleavage (Gethin, Martinez-Toledano, and Piketty 2021) (Inglehart 1971). Very shortly, They found that the electoral support for the left shifted from the low income and low education classes to the highly educated ones. Regarding the electoral support for the right, the latter remains positively correlated with income.\nThis post is going to simply test this finding for Switzerland using the post-electoral survey for the National Council election in 2019. The following analysis is based on data analysis of the Swiss Election Study (Selects) of 2019. The dataset can be found here. I will test if the support for the left is linked positively with the education level and negatively with income.\nTo analyze the link between vote for the left (dependent variable) and income and education, I create a dummy variable from the variable “W1_f1085_90” which asked for which party the respondent is going to vote for in the 2019 national council election. The indicator variable is equal to 1 if the respondent declared voting for the socialist party, the greens, the christian socialist party or for far left parties (swiss labour party, solidarités, ensemble à gauche…)\nRegarding the independ variables, the variable “f28910” asks the gross monthly houshold income of the individual and the variable has 15 income brackets (we thus do not have directly the income of the respondant). For education, “f21310” asks the highest level of achieved education. Here is below descriptive statistics for these variables:\n\n\n\n\n\nEducation level\n\n\n\n\n\n\n\n\n\nGross monthly houshold income"
  },
  {
    "objectID": "posts/Vote for the left in Switzerland a simple analysis/select2019analysis.html#note-this-post-is-not-finished-and-still-in-progress",
    "href": "posts/Vote for the left in Switzerland a simple analysis/select2019analysis.html#note-this-post-is-not-finished-and-still-in-progress",
    "title": "Income, education and voting outcome: a cross-sectional analysis of voting outcome in Switzerland",
    "section": "",
    "text": "Two years ago, Piketty and his team published a fascinating book about the evolution of political cleavages in almost all democratic countries in the world and even beyond. The publication of this book and of its online database got me very interested in the study of political cleavages from a political economy approach. This book contains huge amount of findings and observations, but there is one which is really emphasized by the authors: from roughly the 1980s until, the class-based political divide has become a multidimensional one incorporating a “educational” or, in Inglehart terms, a “postmaterial” or “cultural” cleavage (Gethin, Martinez-Toledano, and Piketty 2021) (Inglehart 1971). Very shortly, They found that the electoral support for the left shifted from the low income and low education classes to the highly educated ones. Regarding the electoral support for the right, the latter remains positively correlated with income.\nThis post is going to simply test this finding for Switzerland using the post-electoral survey for the National Council election in 2019. The following analysis is based on data analysis of the Swiss Election Study (Selects) of 2019. The dataset can be found here. I will test if the support for the left is linked positively with the education level and negatively with income.\nTo analyze the link between vote for the left (dependent variable) and income and education, I create a dummy variable from the variable “W1_f1085_90” which asked for which party the respondent is going to vote for in the 2019 national council election. The indicator variable is equal to 1 if the respondent declared voting for the socialist party, the greens, the christian socialist party or for far left parties (swiss labour party, solidarités, ensemble à gauche…)\nRegarding the independ variables, the variable “f28910” asks the gross monthly houshold income of the individual and the variable has 15 income brackets (we thus do not have directly the income of the respondant). For education, “f21310” asks the highest level of achieved education. Here is below descriptive statistics for these variables:\n\n\n\n\n\nEducation level\n\n\n\n\n\n\n\n\n\nGross monthly houshold income"
  },
  {
    "objectID": "posts/Vote for the left in Switzerland a simple analysis/select2019analysis.html#first-model-binary-logistic-regression",
    "href": "posts/Vote for the left in Switzerland a simple analysis/select2019analysis.html#first-model-binary-logistic-regression",
    "title": "Income, education and voting outcome: a cross-sectional analysis of voting outcome in Switzerland",
    "section": "First model: binary logistic regression",
    "text": "First model: binary logistic regression\nLet’s first start with a logistic regression. I simply regress the vote for the left with income and education. I leave education and income coded as numerical variables for now, since they have enough categories this is not big problem. Of course, that would have been better if I had directly the income of each individual and not brackets. Moreover, I could still do a Pareto interpolation, but I can’t due to lack of information: I don’t have the average income (total and per bracket) of the sample.\nThe model is thus:\n\\[\nLog(\\frac{P(left)}{1 - P(left)}) = \\beta_0 + \\beta_1income_i + \\beta_2educ_i + \\epsilon_i\n\\]\nNote that this is a very first step, I will step by step complexify this model.\nHere is the regression table:\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nvote_left\n\n\n\n\n\n\n\n\nincome\n\n\n-0.075***\n\n\n\n\n\n\n(0.007)\n\n\n\n\n\n\n\n\n\n\neducation\n\n\n0.111***\n\n\n\n\n\n\n(0.008)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-1.033***\n\n\n\n\n\n\n(0.075)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n6,866\n\n\n\n\nLog Likelihood\n\n\n-4,342.283\n\n\n\n\nAkaike Inf. Crit.\n\n\n8,690.566\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\nHere is an odd ratios plot to have a better visualization of the coefficients:\n\n\n\n\n\n\n\nHere is what this coefficient plot tells: the odds that a Swiss voter vote for a left-wing party for the National Council election in 2019 are linked negatively with income (odd ratio below 1) and positively with education (odd ratio &gt; 1). The coefficients are statistically significant at the one percent level, which is not a surprise because the sample is rather large.\nOdd ratios are known to be rather difficult to interpret. In effect, odd ratios are not what the literature calls “quantity of interest”, that is to say, the quantity of the dependent variable which is the most easy to interpret. I this model, the quantity of interest is the probability to vote for a left wing party and not the odds. A lot of economists and social scientists prefer to have a look directly at the marginal effects and predicted probability to have a better view of the relationships between the variables and of the quantity of interest.\nI first plot simple graphs of the estimated curves. To do so, I use the function Invlogit from the plot3logit package and put the estimated coefficient into this function. To do such graphs, one has to make the explanatory variable on the x axis to vary while the other explanatory variables are held constant. A choice has thus to be made about which fixed value of the other factors (of Income for the education level plot and conversely), I decided to choose the median value.\n\n\n\n\n\nProbability of voting for the left - curves from estimated coefficients\n\n\n\n\nWe can see that the slope of the education level curve is steeper than the one for income: this means that the positive link between the level of education and the probability to vote for the left is greater than the negative one for income. But let’s have a look directly at the marginal effects.\nThere are a lot of different ways to compute marginal effects, which make the latter sometimes confusing because we don’t know which type of marginal effects we are talking about. I will here consider one type of marginal effects:\n\nGroup-average marginal effects: slope estimates are produced for each row of the dataset used in computing the model. Then, the estimates can be grouped by the values of one of the regressor and the average for each group is computed.\n\nA first step in group-average marginal effects in R is to use the function “slopes” which calculate estimates of the slopes (marginal effects) for each observation used to compute the model in the first place. The term “variables” is for the variable for which the slopes are estimated and “by” the argument for\n\nmarginaleffectseduc &lt;- slopes(reg, variables = \"education\")\nhead(marginaleffectseduc)\n\n\n      Term Estimate Std. Error    z Pr(&gt;|z|)  2.5 % 97.5 %\n education   0.0216    0.00156 13.9   &lt;0.001 0.0186 0.0247\n education   0.0277    0.00212 13.1   &lt;0.001 0.0236 0.0319\n education   0.0253    0.00204 12.4   &lt;0.001 0.0213 0.0293\n education   0.0261    0.00212 12.3   &lt;0.001 0.0219 0.0303\n education   0.0266    0.00202 13.2   &lt;0.001 0.0226 0.0306\n education   0.0220    0.00165 13.3   &lt;0.001 0.0188 0.0252\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, vote_left, income, education \n\n\n\ndim(marginaleffectseduc)\n\n[1] 6866   14\n\n\nThe dataframe has 5607 rows which is the same number of observation used in the model. We can then used the different values of income level (from 1 to 15) as grouped within which estimates are averaged:\n\nmarginaleffectseduc %&gt;% \n  group_by(income) %&gt;% \n  summarise(mean.slopes.educ = mean(estimate),\n            conf.high = mean(conf.high), ## this is the same for the confidence interval\n            conf.low = mean(conf.low)) %&gt;% \n  ungroup() -&gt; game.educ\nhead(game.educ)\n\n# A tibble: 6 × 4\n  income                  mean.slopes.educ conf.high conf.low\n  &lt;dbl+lbl&gt;                          &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 1 [Less than 2'000 CHF]           0.0262    0.0299   0.0225\n2 2 [2'001-3'000 CHF]               0.0258    0.0294   0.0221\n3 3 [3'001-4'000 CHF]               0.0255    0.0290   0.0219\n4 4 [4'001-5'000 CHF]               0.0251    0.0286   0.0216\n5 5 [5'001-6'000 CHF]               0.0250    0.0285   0.0215\n6 6 [6'001-7'000 CHF]               0.0250    0.0285   0.0214\n\n\nA plot can then be made to have a better view of the average marginal effects/slopes of education for each group of income:\n\ngame.educ %&gt;% \n  ggplot()+\n  aes(x = income, y = mean.slopes.educ)+\n  geom_point()+\n  geom_line()+\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.5)+\n  theme_bw()+\n  labs(title = \"Group-average marginal effect of education for each level of income group\")+\n  ylab(\"Slopes education\")\n\n\n\n\n\n\n\n\nThe slope of education level decreases on average with higher values of income group. This means that even if the probability to vote for the left is linked positively with education level, this link is weaker for higher income groups. However, it is not so much weaker because even though th line is downward slopping, it remains rather flat.\nNormally, the function plot_slope should produce the same graph:\n\nplot_slopes(reg, variables = \"education\", by = \"income\")\n\n\n\n\n\n\n\n\nLet’s do the same for income:\n\nmarginaleffectsinc &lt;- slopes(reg, variables = \"income\")\ngame.inc &lt;- marginaleffectsinc %&gt;% \n  group_by(education) %&gt;% \n  summarise(mean.slopes.inc = mean(estimate),\n            conf.high = mean(conf.high),\n            conf.low = mean(conf.low)) %&gt;% \n  ungroup() -&gt; game.inc\n\n\ngame.inc %&gt;% \n  ggplot()+\n  aes(x = education, y = mean.slopes.inc)+\n  geom_point()+\n  geom_line()+\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.5)+\n  theme_bw()+\n  labs(title = \"Group-average marginal effects of income for each level of education\")+\n  ylab(\"Slopes income\")\n\n\n\n\n\n\n\n\nHere the result is more interesting: the average marginal slope of income is negative for each education level but this average decreases with higher level of education. This implies that the probability to vote for the left is linked negatively with income group and that this negative link is strengthened by higher level of education. Rich and highly educated people have thus a very low probability to vote for the left.\nAnother way to look at the effect of the two independent variable on the probability to vote for the left is to look at the predictions.\n\npredictionseduc &lt;- predictions(reg, variables = c(\"education\", \"income\"))\npredictionseduc &lt;- predictions(reg, by = c(\"education\", \"income\"))\n\n\nplot_predictions(reg, condition = c(\"education\", \"income\"))+\n  scale_color_brewer(palette = \"Set1\")+\n  scale_fill_brewer(palette = \"Set1\")+\n  theme_bw()+\n  theme(legend.position = c(0.5, 0.7),\n        legend.background = element_blank())+\n  ylab(\"probability vote left\") -&gt; plotpredicteduc\n\nplot_predictions(reg, condition = c(\"income\", \"education\"))+\n  scale_color_brewer(palette = \"Set1\")+\n  scale_fill_brewer(palette = \"Set1\")+\n  theme_bw()+\n  theme(legend.position = c(0.8, 0.8),\n        legend.background = element_blank())+\n  ylab(\"\") -&gt; plotpredictincome\n\ncowplot::plot_grid(plotpredicteduc, plotpredictincome)\n\n\n\n\n\n\n\n\nThose plots are essentially the same the first one, but with the confidence interval and for different values of the regressor considered fixed for certain values.\nAnother way is to compute directly the average marginal effects without grouping:\n\nmarginaleffectsinc &lt;- slopes(reg, variables = \"income\")\nmarginaleffectseduc &lt;- slopes(reg, variables = \"education\")\navg_effect_summary_reg &lt;- rbind(summary(marginaleffectsinc), summary(marginaleffectseduc))\n\navg_effect_summary_reg\n\n\n      Term    Contrast Estimate Std. Error     z Pr(&gt;|z|)   2.5 %  97.5 %\n income    mean(dY/dX)  -0.0167    0.00150 -11.1   &lt;0.001 -0.0196 -0.0137\n education mean(dY/dX)   0.0245    0.00177  13.9   &lt;0.001  0.0210  0.0279\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high \n\nmargins_summary(reg)\n\n    factor     AME     SE        z      p   lower   upper\n education  0.0245 0.0018  13.8684 0.0000  0.0210  0.0279\n    income -0.0167 0.0015 -11.0920 0.0000 -0.0196 -0.0137\n\n\nHere is a nice way to visualize the table above:\n\navg_effect_summary_reg %&gt;% \n  ggplot()+\n  aes(x = estimate*100, y = term)+\n  geom_vline(xintercept = 0, color = \"red\")+\n  geom_pointrange(aes(xmin = conf.low*100, xmax = conf.high*100))+\n  theme_bw()+\n  xlab(\"Average marginal effects (percentage points)\")+\n  ylab(\"\")+\n  geom_label(aes(label = round(estimate*100, 3)), nudge_y = 0.15)\n\n\n\n\n\n\n\n\nThe advantage of average marginal effects is the fact that they give information on the quantity of interest (here the probability to vote for the left) instead of odd ratio or log odds. The probability to vote for the left decreases on average by -1.6 percentage points if we compare two units which only differs by one income group level. Conversely, the probability increases on average by 2.5 percentage points if we compare two units which only differs by one level of education."
  },
  {
    "objectID": "posts/Vote for the left in Switzerland a simple analysis/select2019analysis.html#complexifying-the-model",
    "href": "posts/Vote for the left in Switzerland a simple analysis/select2019analysis.html#complexifying-the-model",
    "title": "Income, education and voting outcome: a cross-sectional analysis of voting outcome in Switzerland",
    "section": "Complexifying the model",
    "text": "Complexifying the model\n\nHierarchical model\nOne possible and interesting way to complexify the model is to include the different Swiss canton into the regression. In fact, the previous regression model can be considered as a “complete pooling” model in which I made the asumption that the slopes of the coefficients do not vary by cantons. However, cantons represents an important level of analysis in Switzerland, because elections and politics are a lot structures at the cantonal level. Cantons can thus be considered as levels in which the observed individuals in our sample are grouped.\nAccording to Gelman and Hill (2007) there are two main differents ways to consider these groups in regression analysis:\n\nNo pooling models: the slopes and/or the intercepts are allowed to vary across the groups freely.\nPartial pooling models: the slopes and/or the intercepts are allowed to vary, but they are modeled (we consider that they follow a normal distribution)\n\nIn the present analysis, I will consider a partial pooling model in which the slopes and intercepts for income and education can vary. In R, partial pooling models can be estimated with the function lmer() from the lme4 package.\nThe results of this model can be then represented through a table with tab_model()\n\ntab_model(glmer1, transform = NULL, title = \"Partial Pooling model\", digits = 3)\n\n\nPartial Pooling model\n\n \nvote_left\n\n\nPredictors\nLog-Odds\nCI\np\n\n\n(Intercept)\n-1.088\n-1.382 – -0.793\n&lt;0.001\n\n\nGross monthly householdincome, W1 updated withW4\n-0.076\n-0.095 – -0.058\n&lt;0.001\n\n\neducation\n0.106\n0.087 – 0.126\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 region\n0.33\n\n\nτ11 region.education\n0.00\n\n\nτ11 region.income\n0.00\n\n\nρ01\n-0.82\n\n\n\n-0.61\n\n\nN region\n26\n\nObservations\n6866\n\n\nMarginal R2 / Conditional R2\n0.041 / NA\n\n\n\n\n\n\nThe random effects coefficients at the cantons level for income and education are 0, which implies that there is very low variation between cantons. Furthermore, the fixed effect coefficients are almost the same than the previous model: this new model is thus not a big improvement and shows that including cantons as a group does not change the model a lot.\nHowever, it is still interesting to plot predicted probabilities to have a better overview:\n\nplot_predictions(glmer1, condition = c(\"education\", \"income\", \"region\"))+facet_wrap(~region)+\n  aes(linetype = income)+\n  scale_colour_brewer(palette = \"Set1\")+\n  labs(title = \"Probabilités prédites selon le niveau d'éducation, pour chaque canton et pour différent niveau de revenu\",\n       subtitle = \"Election Conseil National 2015\")+\n  geom_hline(yintercept = 0.5, alpha = 0.8)\n\n\n\n\n\n\n\n\nThe relationship between the predicted probabilities to vote for the left and education is positive for every cantons. There are a lot of cantons that show similar relationships and curves and whose predicted probabilities, despite the positive link with education, do not go above 0.5 (with some exception such as Aargau, Thurgau, Tessin, Valais, Glarus…). The cantons which show different patterns are Basel-Stadt, Geneva, Jura, Vaud and Neuchâtel with higher predicted probabilities.\n\n\nAdding variables to the model\nLet’s now add some control variables to the model. The models above are flawed by the fact that there are only two regressors. One important assumption of regression models are the mean independence of the covariates with the error term. The model is assumed to include all the important explanatory variables in the model. If not, the estimates will be biased, so one can suspect that the coefficients estimates for this first model are heavily biased.\nFor the second model, I include gender (dummy with female = 1), age (in categories), religiosity (the higher, the less religious is the respondent), a variable indicating whether the respondent has a full-time job (full_time), whether the respondent works in the public sector (public) and marital status. I also include variables on issue position related to socio-economic issues such as EU integration, taxation and attitude towards foreigners (each one of these variables are ordinal on a 1-5 scale). Finally, I run the same model for the other main parties in Switzerland: the liberal radical party (center-right, plr), the Swiss people’s party (far-right, udc) and the two greens party (the greens, the liberal greens).\n\nmarginsfull %&gt;% \n  ggplot()+\n  aes(x = AME*100, y = factor, color = party, group = party)+\n  geom_vline(xintercept = 0, color = \"red\")+\n  geom_pointrange(aes(xmin = lower*100, xmax = upper*100))+\n  theme_bw()+\n  xlab(\"Average marginal effects (percentage points)\")+\n  ylab(\"\")+\n  labs(title = \"2019 Swiss national council election\") + \n  facet_wrap(~party)+\n  theme(legend.position = \"none\",\n        )"
  },
  {
    "objectID": "posts/Sugar, Cochineal and Power/sugar cochineal power.html",
    "href": "posts/Sugar, Cochineal and Power/sugar cochineal power.html",
    "title": "Sugar, Cochineal and power",
    "section": "",
    "text": "“The wealth of societies in which the capitalist mode of production prevails appears as an ‘immense collection of commodities’ […] Our investigation therefore begins with the analysis of the commodity” (Marx 1990, 124). So is the incipit of one of the most famous and important books in political economy. If Marx saw in the commodity the most crucial and elementary unit of abstraction in the capitalist mode of production, so much that he spent the first and most tedious pages of his magnus opus dedicated to its analysis and decortication, the commodity per se, that is, as a focus of historical and economic analysis, is rather rare in economics and economic history. For instance, mainstream economics reduces commodities to any objects or services bringing utility to consumers, abstracting from its historical and political dimensions.\nNonetheless, the two texts I will review in this critical summary are attempts to fill this gap and put the commodity back into the heart of not only economics, but also of social and historical analysis. These two texts share indeed a common characteristic: they both focus on a single commodity. Mintz’ book chapter Power (from Sweetness and Power, The Place of Sugar in Modern History (1985)) focuses on sugar from an anthropological perspective whereas Marichal’s book chapter Mexican Cochineal and the European Demand for American Dyes, 1550-1850 (2006) concentrates on cochineal from an economic history point of view. Another peculiar common characteristic of the two papers is that sugar and cochineal, a dyestuff product which is used for its crimson color, are both important inputs for a great number of final products: they are both illustrative examples of intermediary goods constituting a great part of global value chains.\nIn this critical summary, I will first summarize those two texts by outlining their main hypotheses and arguments in a comparative approach. I will then make some critical remarks and comments on what I believe are the positive aspects and chief contributions as well as the flaws and weaknesses of those two papers.\n\n\nMarichal’s analysis of the rise and decline of cochineal production and trade rests upon three main hypotheses. The first one is that the prosperity of cochineal production and trade in Oaxaca, the main farming region of cochineal in Mexico, from the 16th century on was demand led. He advances two reasons for this strong demand, mostly driven by the upper classes of Europe, namely the Church, the nobility, and other elites. One reason is cultural and symbolic: cochineals produce a particularly magnificent crimson color that the nobility and the church associated with symbols of prestige and power. The other reason is physical: cochineal has physical properties which make it materially more advantageous compared to its competitors (kermes for example). This strong demand thus made cochineal highly profitable. Trade, production, and prices thus boomed from the 16th until the late 18th century which marked the start of a long run decline. Mintz’ analysis of how sugar became such an essential and highly demanded commodity is more nuanced. In fact, Mintz’s text is an attempt to answer the following question: how and why did sugar become one of the most widespread and consumed commodities in British society? An obvious and straightforward answer would be, as Marichal argues in the cochineal’s case, that humans are naturally inclined to accept sugar in their consumption habits because of its physical properties. As an anthropologist, Mintz knows that there is nothing natural in the growth of sugar demand. More elaborated answers would be for instance that sugar’s consumption habits spread from the upper classes to the lower classes through imitation (what Mintz calls “intensification”), or the constant tendency of sugar prices to fall. None of these answers are convincing enough for Mintz, whose argument is that the structural transformations of the working conditions and schedules of the British workers, during a transition process from an agricultural economy to an industrial one from circa the 16th century to the 19th century, were the real reason behind sugar’s tremendous prosperity as an essential commodity with strong meaning and signification for its consumers. Mintz explores the dual “meaning” of sugar. On the one hand, sugar replaced honey as a signification of sweetness, happiness, endearment, and tenderness (the “inside meaning” of sugar in anthropological terms) for the common British people. On the other hand, sugar became a symbol of power and prosperity for the imperialistic British ruling classes which had an interest in sugar production and trading made possible by the exploitation of African slaves in the Caribbean plantations. This represents the “outside meaning” of sugar.\n\n\n\nColonial sugar cane manufacturing\n\n\n\n\n\nSecondly, Marichal argues that the stability and success of cochineal’s mode of production were the results of the repartimiento system. The latter was the product of active Spanish colonial policy and merchants’ activity: local Oaxaca bureaucrats obtained funds from Mexican merchants and lent those funds to indigenous cochineal farmers. Indigenous farmers had then to produce cochineals and pay back the functionaries in kind. Marichal argues that the end of the repartimiento system was one of the main reasons for the decline of cochineal production after the late 18th century, which as further exacerbated by the invention of synthetic dyes during the second industrial revolution during the 19th century. This is the third and last hypothesis of Marichal. For Mintz, the availability of sugar was also the product of deliberate British imperialistic policy and is linked to the “outside meaning” explained above. However, the anthropologist argues that sugar demand itself was a product of British government policy: demand on the side of the British masses had to be planned, constructed and stimulated as well. The introduction of rum in the British army, of sugar in almshouses and the abolition of tariffs and duties are example of this active policy so that both “outside” and “inside” meanings of sugar could become unified. Critics and comments\nThese two texts are both instructive and captivating. They offer an enlightening history of sugar and cochineal, giving a lot of data and information on the economic history, modes of production and demand formation processes of those two commodities. The heuristic approach of taking a commodity as a unit of analysis is certainly the most interesting aspect of the two papers.\nFor instance, by focusing on cochineal production in Oaxaca, Marichal deconstructs some prevalent commonplaces of Spanish colonial history and mercantilist policy. One learns that Spaniards were not only interested in silver and gold, which is a widespread cliché in economic history , and that they in fact also sought to capture and exploit other profitable businesses as the market for dyes. The description of the repartimiento system and of its functional role in the stability of cochineal production are convincing and show to what extent Spanish colonization, traditionally considered as mercantilist, was capitalist regarding some sectors (here cochineal production and exchange). However, his argument has some blind spots. First, he totally neglects the importance of class struggle within the repartimiento system and seems to consider the Mexican Revolution of 1820 only as an exogenous shock to the system, without considering the possibility that the fall of the repartimiento system could have been the result of its own contradictions and could have been (this is only a hypothetical possibility) somehow linked to internal class struggle, social movements and uprisings or even the Mexican Revolution. Second, the history of cochineal production and use in Latin America prior to colonialism and how it became monopolized by the Spaniards are overlooked. Finally, it would have been interesting to grant some attention to the resilience of cochineal production despite its decline after the second industrial revolution, since the product is still used today and somehow managed to escape its demise to some extent.\nMintz, however, offers an even more insightful analysis and it is not by chance that the book from which this chapter is taken from has become a classic, giving rise to an entire new field in anthropology, namely the anthropology of food. The argument is explained through a wide variety of anthropological concepts (intensification, extensification, outside and inside meanings…) which are interesting when applied to an economic subject such as the sugar commodity. The most powerful aspect of this text is its confrontation to some classical evidence that are omnipresent and taken for granted in economics and economic history: the naturality of sugar demand which is generally considered as given and self-evident, the important role given to price fluctuation and even to free choice and individual agency. The role of prices, meanings, demand, individual choices and liberty are carefully examined and put into question.\nUnfortunately, Mintz does not support his own thesis. He even admits at the end that his hypothesis, the success of sugar in British society as a result of structural changes in schedule, working conditions and daily life of the British workers, is “difficult or impossible to prove” (p. 186) and he regresses his argument stating that the nature of sugar must also have played a role in its success. Such contradictions are present throughout the text and tend to make difficult any attempt to clearly grasp Mintz’ arguments and positions. Another example of such contradictions is the distinctive place of sugar between “intensification” (explained above) and “extensification”, when meanings and habits are indigenous to a specific group or class and spread without imitation. Mintz states that sugar was particular in its high degree of extensification among the British masses and that the latter developed meanings towards sugar independently from the upper classes. However, Mintz does not develop further this interesting idea and his description of sugar’s meanings through literature is not sufficient and convincing enough to explain what sugar meant to the working class."
  },
  {
    "objectID": "posts/Sugar, Cochineal and Power/sugar cochineal power.html#comments-on-sidney-mintzs-sweetness-and-power-and-carlos-marichals-mexican-cochineal-and-the-european-demand-for-american-dyes-1550-1850",
    "href": "posts/Sugar, Cochineal and Power/sugar cochineal power.html#comments-on-sidney-mintzs-sweetness-and-power-and-carlos-marichals-mexican-cochineal-and-the-european-demand-for-american-dyes-1550-1850",
    "title": "Sugar, Cochineal and power",
    "section": "",
    "text": "“The wealth of societies in which the capitalist mode of production prevails appears as an ‘immense collection of commodities’ […] Our investigation therefore begins with the analysis of the commodity” (Marx 1990, 124). So is the incipit of one of the most famous and important books in political economy. If Marx saw in the commodity the most crucial and elementary unit of abstraction in the capitalist mode of production, so much that he spent the first and most tedious pages of his magnus opus dedicated to its analysis and decortication, the commodity per se, that is, as a focus of historical and economic analysis, is rather rare in economics and economic history. For instance, mainstream economics reduces commodities to any objects or services bringing utility to consumers, abstracting from its historical and political dimensions.\nNonetheless, the two texts I will review in this critical summary are attempts to fill this gap and put the commodity back into the heart of not only economics, but also of social and historical analysis. These two texts share indeed a common characteristic: they both focus on a single commodity. Mintz’ book chapter Power (from Sweetness and Power, The Place of Sugar in Modern History (1985)) focuses on sugar from an anthropological perspective whereas Marichal’s book chapter Mexican Cochineal and the European Demand for American Dyes, 1550-1850 (2006) concentrates on cochineal from an economic history point of view. Another peculiar common characteristic of the two papers is that sugar and cochineal, a dyestuff product which is used for its crimson color, are both important inputs for a great number of final products: they are both illustrative examples of intermediary goods constituting a great part of global value chains.\nIn this critical summary, I will first summarize those two texts by outlining their main hypotheses and arguments in a comparative approach. I will then make some critical remarks and comments on what I believe are the positive aspects and chief contributions as well as the flaws and weaknesses of those two papers.\n\n\nMarichal’s analysis of the rise and decline of cochineal production and trade rests upon three main hypotheses. The first one is that the prosperity of cochineal production and trade in Oaxaca, the main farming region of cochineal in Mexico, from the 16th century on was demand led. He advances two reasons for this strong demand, mostly driven by the upper classes of Europe, namely the Church, the nobility, and other elites. One reason is cultural and symbolic: cochineals produce a particularly magnificent crimson color that the nobility and the church associated with symbols of prestige and power. The other reason is physical: cochineal has physical properties which make it materially more advantageous compared to its competitors (kermes for example). This strong demand thus made cochineal highly profitable. Trade, production, and prices thus boomed from the 16th until the late 18th century which marked the start of a long run decline. Mintz’ analysis of how sugar became such an essential and highly demanded commodity is more nuanced. In fact, Mintz’s text is an attempt to answer the following question: how and why did sugar become one of the most widespread and consumed commodities in British society? An obvious and straightforward answer would be, as Marichal argues in the cochineal’s case, that humans are naturally inclined to accept sugar in their consumption habits because of its physical properties. As an anthropologist, Mintz knows that there is nothing natural in the growth of sugar demand. More elaborated answers would be for instance that sugar’s consumption habits spread from the upper classes to the lower classes through imitation (what Mintz calls “intensification”), or the constant tendency of sugar prices to fall. None of these answers are convincing enough for Mintz, whose argument is that the structural transformations of the working conditions and schedules of the British workers, during a transition process from an agricultural economy to an industrial one from circa the 16th century to the 19th century, were the real reason behind sugar’s tremendous prosperity as an essential commodity with strong meaning and signification for its consumers. Mintz explores the dual “meaning” of sugar. On the one hand, sugar replaced honey as a signification of sweetness, happiness, endearment, and tenderness (the “inside meaning” of sugar in anthropological terms) for the common British people. On the other hand, sugar became a symbol of power and prosperity for the imperialistic British ruling classes which had an interest in sugar production and trading made possible by the exploitation of African slaves in the Caribbean plantations. This represents the “outside meaning” of sugar.\n\n\n\nColonial sugar cane manufacturing\n\n\n\n\n\nSecondly, Marichal argues that the stability and success of cochineal’s mode of production were the results of the repartimiento system. The latter was the product of active Spanish colonial policy and merchants’ activity: local Oaxaca bureaucrats obtained funds from Mexican merchants and lent those funds to indigenous cochineal farmers. Indigenous farmers had then to produce cochineals and pay back the functionaries in kind. Marichal argues that the end of the repartimiento system was one of the main reasons for the decline of cochineal production after the late 18th century, which as further exacerbated by the invention of synthetic dyes during the second industrial revolution during the 19th century. This is the third and last hypothesis of Marichal. For Mintz, the availability of sugar was also the product of deliberate British imperialistic policy and is linked to the “outside meaning” explained above. However, the anthropologist argues that sugar demand itself was a product of British government policy: demand on the side of the British masses had to be planned, constructed and stimulated as well. The introduction of rum in the British army, of sugar in almshouses and the abolition of tariffs and duties are example of this active policy so that both “outside” and “inside” meanings of sugar could become unified. Critics and comments\nThese two texts are both instructive and captivating. They offer an enlightening history of sugar and cochineal, giving a lot of data and information on the economic history, modes of production and demand formation processes of those two commodities. The heuristic approach of taking a commodity as a unit of analysis is certainly the most interesting aspect of the two papers.\nFor instance, by focusing on cochineal production in Oaxaca, Marichal deconstructs some prevalent commonplaces of Spanish colonial history and mercantilist policy. One learns that Spaniards were not only interested in silver and gold, which is a widespread cliché in economic history , and that they in fact also sought to capture and exploit other profitable businesses as the market for dyes. The description of the repartimiento system and of its functional role in the stability of cochineal production are convincing and show to what extent Spanish colonization, traditionally considered as mercantilist, was capitalist regarding some sectors (here cochineal production and exchange). However, his argument has some blind spots. First, he totally neglects the importance of class struggle within the repartimiento system and seems to consider the Mexican Revolution of 1820 only as an exogenous shock to the system, without considering the possibility that the fall of the repartimiento system could have been the result of its own contradictions and could have been (this is only a hypothetical possibility) somehow linked to internal class struggle, social movements and uprisings or even the Mexican Revolution. Second, the history of cochineal production and use in Latin America prior to colonialism and how it became monopolized by the Spaniards are overlooked. Finally, it would have been interesting to grant some attention to the resilience of cochineal production despite its decline after the second industrial revolution, since the product is still used today and somehow managed to escape its demise to some extent.\nMintz, however, offers an even more insightful analysis and it is not by chance that the book from which this chapter is taken from has become a classic, giving rise to an entire new field in anthropology, namely the anthropology of food. The argument is explained through a wide variety of anthropological concepts (intensification, extensification, outside and inside meanings…) which are interesting when applied to an economic subject such as the sugar commodity. The most powerful aspect of this text is its confrontation to some classical evidence that are omnipresent and taken for granted in economics and economic history: the naturality of sugar demand which is generally considered as given and self-evident, the important role given to price fluctuation and even to free choice and individual agency. The role of prices, meanings, demand, individual choices and liberty are carefully examined and put into question.\nUnfortunately, Mintz does not support his own thesis. He even admits at the end that his hypothesis, the success of sugar in British society as a result of structural changes in schedule, working conditions and daily life of the British workers, is “difficult or impossible to prove” (p. 186) and he regresses his argument stating that the nature of sugar must also have played a role in its success. Such contradictions are present throughout the text and tend to make difficult any attempt to clearly grasp Mintz’ arguments and positions. Another example of such contradictions is the distinctive place of sugar between “intensification” (explained above) and “extensification”, when meanings and habits are indigenous to a specific group or class and spread without imitation. Mintz states that sugar was particular in its high degree of extensification among the British masses and that the latter developed meanings towards sugar independently from the upper classes. However, Mintz does not develop further this interesting idea and his description of sugar’s meanings through literature is not sufficient and convincing enough to explain what sugar meant to the working class."
  },
  {
    "objectID": "posts/Sugar, Cochineal and Power/sugar cochineal power.html#summing-up",
    "href": "posts/Sugar, Cochineal and Power/sugar cochineal power.html#summing-up",
    "title": "Sugar, Cochineal and power",
    "section": "Summing up",
    "text": "Summing up\nTo sum up, Marichal’s book chapter on the cochineal is part of a wider project destined to put commodities back into the heart of economics and economic history. The author has three main hypothesis covering cochineal’s economic history. First, the tremendous growth in production and trade of this commodity was led by the demand of European upper classes. Second, the stability of the production process in Oaxaca was based on the repartimiento system. Finally, the decline of cochineal production and trade was the consequence of the second industrial revolution thanks to the invention of synthetic dies. Whereas cochineal is nowadays more of a relic from the past despite some relative resilience that should deserve some further attention (cochineal’s carminic acid is still used in some final products like syrup for example), sugar represents today the paroxysm of consumption under capitalism and is perhaps the most emblematic and embodiment of the commodity, as a crucial input in value chains as well as a final product. Mintz’ Sweetness and Power has since become a classic in anthropology, laying the foundation of a new field focused around the anthropology of food. I tried through this critic to show that Mintz’ argument is more than just a link between British demand for sugar and the exploitation of African slaves in the Caribbean plantations. Mintz’ analysis provides deeper investigations of the role of meanings attributed to sugar as well as for the role of both demand and supply, the political and power interests in the rise of one of nowadays’ most consumed, omnipresent, and controversial commodity."
  },
  {
    "objectID": "posts/political compass/pol_compass.html",
    "href": "posts/political compass/pol_compass.html",
    "title": "Political Compass: what lies behind political science’s most famous graphs",
    "section": "",
    "text": "People who get lost or waste their time on the internet have surely seen the now famous “political compass” graphs, which have become a popular meme on the web. The irony and paradox of political compass, and that is what makes the latter intriguing, are that it is actually an important visualization in political science. Very shortly, a political compass is a mapping of political leaning and preferences based on two axes: (1) an economic axis representing the position of an individual, party, association or group on global economic issues (pro and anti redistribution, for economic planning or free market…) and (2) a “cultural” or “post-material” (a lot of different names are given to this axis) axis representing position on cultural issues, such as: “in favor or against immigration”, “in favor or against gay marriage/lgbt rights/traditional values…”\nWhereas most people who use political compass do so to make memes and amuse their friends, political scientists are still seriously attached to their favorite toy and one often sees such mapping in political science articles. For instance, Oesch and Rennwald provide such graphics to map individuals political preferences for Switzerland and other countries using European Social Survey data (Oesch and Rennwald 2018, 795). Pascal Sciarini, in a manual on Swiss politics, also provides such a graph using Swiss Elections Studies data (Sciarini 2023, 341). The idea that political preferences can be separated between an economic and cultural cleavage comes back at least to Inglehart (1971) who explained the emerging salience of cultural issues over the traditional economic divide as a consequence of post-war tremendous economic growth which made the West so wealthy that people would get rid off of any material insecurity, hence shifting the focus on cultural matters rather than economic ones. But how do political scientists construct such graphs? The goal of this post is to explore how the economic and cultural axes are constructed, what are the data and variables used, and what kind of statistical manipulation is done with those data."
  },
  {
    "objectID": "posts/political compass/pol_compass.html#intro",
    "href": "posts/political compass/pol_compass.html#intro",
    "title": "Political Compass: what lies behind political science’s most famous graphs",
    "section": "",
    "text": "People who get lost or waste their time on the internet have surely seen the now famous “political compass” graphs, which have become a popular meme on the web. The irony and paradox of political compass, and that is what makes the latter intriguing, are that it is actually an important visualization in political science. Very shortly, a political compass is a mapping of political leaning and preferences based on two axes: (1) an economic axis representing the position of an individual, party, association or group on global economic issues (pro and anti redistribution, for economic planning or free market…) and (2) a “cultural” or “post-material” (a lot of different names are given to this axis) axis representing position on cultural issues, such as: “in favor or against immigration”, “in favor or against gay marriage/lgbt rights/traditional values…”\nWhereas most people who use political compass do so to make memes and amuse their friends, political scientists are still seriously attached to their favorite toy and one often sees such mapping in political science articles. For instance, Oesch and Rennwald provide such graphics to map individuals political preferences for Switzerland and other countries using European Social Survey data (Oesch and Rennwald 2018, 795). Pascal Sciarini, in a manual on Swiss politics, also provides such a graph using Swiss Elections Studies data (Sciarini 2023, 341). The idea that political preferences can be separated between an economic and cultural cleavage comes back at least to Inglehart (1971) who explained the emerging salience of cultural issues over the traditional economic divide as a consequence of post-war tremendous economic growth which made the West so wealthy that people would get rid off of any material insecurity, hence shifting the focus on cultural matters rather than economic ones. But how do political scientists construct such graphs? The goal of this post is to explore how the economic and cultural axes are constructed, what are the data and variables used, and what kind of statistical manipulation is done with those data."
  },
  {
    "objectID": "posts/political compass/pol_compass.html#constructing-a-political-compass",
    "href": "posts/political compass/pol_compass.html#constructing-a-political-compass",
    "title": "Political Compass: what lies behind political science’s most famous graphs",
    "section": "Constructing a Political Compass",
    "text": "Constructing a Political Compass\nPolitical compass is typically constructed from ordinal variables related to economic and cultural issues and the individual self-placement of survey respondents to these issues. For instance, Oesch and Rennwald (2018) construct the preferences of respondents on the economic axis from an ordinal 5-points scale variable about preferences on redistribution (1 = strongly agree to reduce differences in income levels, 5 = strongly disagree). For the cultural cleavage, they took three variables: (1) the impact of immigrants on cultural life, (2) European integration and (3) freedom of gay people. One can directly see the problem with the construction of such a “cultural” axis: are here the European integration and immigration issues purely cultural issues? In my opinion, European integration cannot be considered only as a cultural “openness vs closeness values” issue and should not have been included to construct the cultural axis. However, the next step after choosing the variables is to standardize them: each observed value of the dataset is subtracted by the mean of the variable and divided by its standard deviation. If more than one variable are taken into account, the variables are standardized and the average values on these three standardized variables are computed to create the final index.\n\nExample and Variables\nTo work with an example, I will work here with a dataset which is not used by the literature cited above: the Voxit studies. The latter offer a cumulative dataset on Swiss referendum voting from 1981 to 2016. The dataset contains a list of “issue positions” variables:\n\nIn favor of state intervention or free markets\nIn favor of or against income equality\nIn favor of an open or closed Switzerland\nIn favor of active equality policy between men and women\nIn favor of equal chances between Swiss people and foreigners\nIn favor of environnment protection or economic growth\nIn favor of a strong or weak Swiss army\n\nAll these variables are ordinal on a 7-points scale. Another variable, “p02” collected political party support (which party the respondent feels the most closed to). I will use this variable as a group to plot Swiss political parties on the economic and cultural axes depending on the average value of their supporters on the standardized economic and cultural issues variables.\n\n\nConstructing the axes\nTo construct the economic axis, I will use the “state vs market” variable. “Open vs closed Switzerland” will be used for the cultural axis.\nHere are all the steps to transform the variable, compute the average values for each year for each political party support and how to plot the final political compass.\n\nstrdz &lt;- function(x){\n  (x - mean(x, na.rm = TRUE))/(sd(x, na.rm = TRUE))\n} # create a function which transfrom x into standardize variable by subtracting the mean and dividing by standard deviation\n\n\nvox &lt;- \nvox %&gt;% \n  group_by(annee) %&gt;% # group by year\n  mutate(state_market = strdz(val11), # compute the standardization transformation and store into new variable\n         open_closed = strdz(val14),\n         envi_vs_growth = strdz(val10),\n         inegalite_hf = strdz(val16),\n         weak_army = strdz(val3),\n         inc_equality = strdz(val5),\n         no_chance_foreign = strdz(val9)) %&gt;% \n  ungroup()\n\n#### create a table: each row (observation) is a political party repeated for each year in which data were available (1996 to 2016)\nvalues &lt;- \nvox %&gt;% \n  group_by(annee, as_character(p02)) %&gt;% # average computed across both year and parties\n  summarise(state_market = mean(state_market, na.rm = TRUE), # compute the average of standardized state vs market variable in each year for each party support group.\n            open_closed = mean(open_closed, na.rm = TRUE),\n            envi_vs_growth = mean(envi_vs_growth, na.rm = TRUE),\n            inegalite_hf = mean(inegalite_hf, na.rm = TRUE),\n            weak_army = mean(weak_army, na.rm = TRUE),\n            inc_equality = mean(inc_equality, na.rm = TRUE),\n            no_chance_foreign = mean(no_chance_foreign, na.rm = TRUE)) %&gt;% \n  ungroup() %&gt;% \n  drop_na() %&gt;% \n  rename(\"party\" = `as_character(p02)`)\n\nBefore plotting the political compass, it could be interesting to see the correlation between the average values through the years. If two variables are positively correlated, that means there are chances that, given any year, when one has high value (average of the standardized issue position variable for each respondent having declared to support the Swiss’s People party for instance) the other will have a high value as well:\n\nvalues %&gt;% \n  dplyr::select(-annee, -party) %&gt;% \n  cor() %&gt;% \n  corrplot(order = \"hclust\")\n\n\n\n\n\n\n\n\nThe correlation plots give coherent results overall. For example, the average of party supporters who declared not being in favor of equal chance for foreigners are also supportive of a isolated Switzerland (see “no_chance-foreign” - “open_closed”).\n\n\nPlotting the Graph\nLet’s now plot a political compass for a given year. I choose the first year available: 1996. I also filter political parties to select only the most important ones: the Swiss Social Democratic Party (PSS/SPS), the Radicals (PRD), The Swiss Peoples Party (UDC/SVP), the Liberals, The Catholic-Conservatives (PDC), the Greens (PES/GPS), the Evangelical party (PEP/EVP).\n\nvalues %&gt;% \n  filter(annee %in% c(1996) & party %in% c(\"UDC/SVP\", \"PDC/CVP\", \"PLS/LPS\", \"PSS/SPS\", \"PRD/FDP\", \"PES/GPS\", \"PEP/EVP\")) %&gt;% \n  ggplot()+\n  aes(x = state_market, y = open_closed, label = party, color = party)+\n  geom_hline(yintercept = 0)+\n  geom_vline(xintercept = 0)+\n  geom_label()+\n  labs(x = \"State intervention - Free market\",\n       y = \"Open vs Closed Switzerland\",\n       title = \"Swiss Political Compass\",\n       subtitle = \"1996\",\n       caption = \"Data source: Voxit\")+\n    xlim(-1,1)+\n    ylim(-1,1)+\n  theme(legend.position = \"none\")+\n  scale_color_manual(values = c(\"PSS/SPS\" = \"darkred\", \"PES/GPS\" = \"green\", \"UDC/SVP\" = \"darkgreen\", \"PEP/EVP\" = \"gold\", \"PLS/LPS\" = \"blue\",\n                                \"PLR\" = \"darkblue\", \"PDC/CVP\" = \"darkorange\", \"PRD/FDP\" = \"darkblue\"))\n\n\n\n\n\n\n\n\nSwitzerland’s political space is often considered as constituted by three political blocs. The first plot above shows that it was indeed the case in 1996 if we look only at the main parties. The greens and social-democratic supporters are for an open Switzerland and rather for state intervention. Piketty (2019) would call this bloc “Social-egalitarian” because social economically and egalitarian in terms of cultural/openess values. A second bloc is constituted by free-market and open Switzerland Swiss citizen, who support the Radical-Democratic party (PRD/FDP) and the Liberal party (PLS/PRD). Note that these two parties, constituting a “Social-inegalitarian bloc” (Piketty 2019), merged in 2009 and gave birth to the Liberal-Radical party (PLR). The graph above shows that base the two parties was indeed very close. Third and finally, what Piketty calls negatively the “Social-nativist trap” is represented here on the upper-right cadran. However, the graph clearly shows that the SVP cannot be considered as “social-nativist” because its supporters are on average clearly in favor of pro-market policies. The party is thus more “nativist-inegalitarian” than “social-nativist”. Social-nativism is not represented in Switzerland: there is no political support for any party which is on the left economically and on the right in terms of cultural values (empty upper-left cadran).\n\n\nTime Comparisons\nNext, let’s take advantage of what can be done with such data and visualization. It would be interesting to compare 1996 with 2016 (the last year of the dataset) to see the evolution: is the tripolar political space described above stable?\n\nvalues %&gt;% \n  filter(annee %in% c(1996, 2016) & party %in% c(\"UDC/SVP\", \"PDC/CVP\", \"PLS/LPS\", \"PSS/SPS\", \"PRD/FDP\", \"PES/GPS\", \"PEP/EVP\", \"PLR\")) %&gt;% \n  ggplot()+\n  aes(x = state_market, y = open_closed, label = party, color = party, group = party)+\n  geom_hline(yintercept = 0)+\n  geom_vline(xintercept = 0)+\n  geom_label()+\n  geom_line(arrow = arrow(length=unit(0.30,\"cm\"), type = \"closed\"))+\n  labs(x = \"State intervention - Free market\",\n       y = \"Open vs Closed Switzerland\",\n       title = \"Swiss Political Compass\",\n       subtitle = \"1996-2016\",\n       caption = \"Data source: Voxit\")+\n  theme(legend.position = \"none\")+\n  scale_color_manual(values = c(\"PSS/SPS\" = \"darkred\", \"PES/GPS\" = \"green\", \"UDC/SVP\" = \"darkgreen\", \"PEP/EVP\" = \"gold\", \"PLS/LPS\" = \"blue\",\n                                \"PLR\" = \"darkblue\", \"PDC/CVP\" = \"darkorange\", \"PRD/FDP\" = \"darkblue\"))\n\n\n\n\n\n\n\n\nWe can see that there is no great change overall: Switzerland’s tripolar division is still valid in 2016. The greens supporters haven’t really change their position. PSS supporter have become more right-wing economically. The PEP supporters have totally changed their position in terms of cultural values. The PLS and PRD have merged to create the PLR and the SVP supporters have become slighlty more right-wing economically.\n\n\nMaking a Video\nHowever, a great deal of information is hidden behind this graph. In effect, we also want to know the transition every year. This would allow to check whether the tripolar division was stable or not. To do so, we have few choices but to create an animated graph that would display the graphs every year from 1996 to 2016. Fortunaly, the package gganimate allows to rather easily make such animations in R. In my case, I will simply add the function transition_time():\n\nanim &lt;- \nvalues %&gt;% \n  filter(party %in% c(\"UDC/SVP\", \"PDC/CVP\", \"PLS/LPS\", \"PSS/SPS\", \"PRD/FDP\", \"PES/GPS\", \"PEP/EVP\", \"PLR\")) %&gt;% \n  ggplot()+\n  aes(x = state_market, y = open_closed, label = party, color = party)+\n  geom_hline(yintercept = 0)+\n  geom_vline(xintercept = 0)+\n  geom_label()+\n  theme(legend.position = \"none\")+\n  scale_color_manual(values = c(\"PSS/SPS\" = \"darkred\", \"PES/GPS\" = \"green\", \"UDC/SVP\" = \"darkgreen\", \"PEP/EVP\" = \"gold\", \"PLS/LPS\" = \"blue\",\n                                \"PLR\" = \"darkblue\", \"PDC/CVP\" = \"darkorange\", \"PRD/FDP\" = \"darkblue\"))+\n  transition_time(time = round(annee))+\n  labs(title = 'Year: {frame_time}')+\n  ease_aes('linear', interval = 1)\n\nThen use animate() to improve the fps, add a pause at the end and slow down the animation\n\nanimate(anim, duration = 30, fps = 30, end_pause = 50)\n\n\n\n\n\n\n\n\nSince gganimate provides gifs, which are sometimes not really convenient (if we want to pause the animation for example). Here is thus the same animation in video format:\n\n\n\nReplacing party support by income group\nWhat if we average variables across income group instead of party support? We would then see how each income group situates on average on the economic and cultural axes. Have the lowest income groups become more conservative in terms of cultural values? How do the rich situate on the graph? Those are all interesting questions that can be answered by repeating the same steps above, but changing the group (part ==&gt; income group).\nThe income variable that I will use is a variable I created from two income variables of the dataset (look here if you want to see the details). The income variable has four groups.\n\nvox$income_f &lt;- factor(vox$income, levels = c(-1, 0, 1, 2), labels = c(\"low\", \"lower middle\", \"upper middle\", \"high\"))\n\nvalues_inc &lt;- \nvox %&gt;% \n  group_by(annee, income_f) %&gt;% # average computed across both year and parties\n  summarise(state_market = mean(state_market, na.rm = TRUE), # compute the average of standardized state vs market variable in each year for each party support group.\n            open_closed = mean(open_closed, na.rm = TRUE),\n            envi_vs_growth = mean(envi_vs_growth, na.rm = TRUE),\n            inegalite_hf = mean(inegalite_hf, na.rm = TRUE),\n            weak_army = mean(weak_army, na.rm = TRUE),\n            inc_equality = mean(inc_equality, na.rm = TRUE),\n            no_chance_foreign = mean(no_chance_foreign, na.rm = TRUE)) %&gt;% \n  ungroup() %&gt;% \n  drop_na() %&gt;% \n  rename(\"income_group\" = income_f)\n\n\nvalues_inc %&gt;% \n  dplyr::select(-annee, -income_group) %&gt;% \n  cor() %&gt;% \n  corrplot(order = \"hclust\")\n\n\n\n\n\n\n\n\n\nvalues_inc %&gt;% \n  filter(annee %in% c(1996, 2016)) %&gt;% \n  ggplot()+\n  aes(x = state_market, y = open_closed, label = income_group, fill = income_group, group = income_group)+\n  geom_hline(yintercept = 0)+\n  geom_vline(xintercept = 0)+\n  geom_label()+\n  geom_line(arrow = arrow(length=unit(0.30,\"cm\"), type = \"closed\"))+\n  labs(x = \"State intervention - Free market\",\n       y = \"Open vs Closed Switzerland\",\n       title = \"Swiss Political Compass\",\n       subtitle = \"Average by income group, 1996\",\n       caption = \"Data source: Voxit\")+\n  theme(legend.position = \"none\")+\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\nanim2 &lt;- \nvalues_inc %&gt;% \n  ggplot()+\n  aes(x = state_market, y = open_closed, label = income_group, fill = income_group)+\n  geom_hline(yintercept = 0)+\n  geom_vline(xintercept = 0)+\n  geom_label()+\n  theme(legend.position = \"none\")+\n  scale_fill_brewer(palette = \"Set1\")+\n  transition_time(time = annee)+\n  labs(title = 'Year: {frame_time}',\n       x = \"State intervention - Free market\",\n       y = \"Open vs Closed Switzerland\",\n       subtitle = \"Average by income group, 1996-2016\",\n       caption = \"Data source: Voxit\")\n\nanimate(anim2, duration = 30, fps = 30, end_pause = 50)"
  },
  {
    "objectID": "posts/growth debate switzerland/growth debate switzerland.html",
    "href": "posts/growth debate switzerland/growth debate switzerland.html",
    "title": "Economic stagnation in Switzerland: discussion of a growing debate",
    "section": "",
    "text": "In the last years, recurring debates are emerging in Switzerland regarding the future of the country’s socio-economic model, showing a rising concern by the Swiss elites for their country’s economic performance. One of the most growing debates is about economic growth: does Switzerland, as in other Western countries, suffer from economic slow-down, or even stagnation ? This was suggested by an article published in 2022 in the renown Swiss-German newspaper Neue Zürcher Zeitung. This article, written by Thomas Fuster and entitled “Immigration: The Swiss economy is primarily growing extensively” argued that the Swiss economy is solely growing “extensively”, that is, only thanks to demographic growth and hence immigration. While economic growth, measured by the famous gross domestic product (GDP), is still growing due to immigration, GDP per capita growth remains near-stagnant because of the decline in productivity growth. This article provoked heated reactions among the neoliberal think thank Avenir Suisse and the business interest association EconomieSuisse, which responded through a series of articles offering an overview and a discussion of the performance of the Swiss economy in the last decades. Both EconomieSuisse and Avenir Suisse were eager to prove that Switzerland’s economy is resilient, that intensive growth based on productivity gains is still a fact, and that the country still holds its privileged position as the so-called Swiss miracle.\nEconomieSuisse’s argument and empirical evidence (Minsch 2023) are first based on a graph showing the evolution of Switzerland’s real gdp per capita growth in international comparison. The graph shows that Swiss gdp per capita is well advanced compared to other Western countries such as the US, Germany or France, and continues to grow despite its advanced level. However there are several issues with EconomieSuisse’s figure. First, the choice of the countries which are compared to Switzerland biases overstates the advance of the Swiss real gdp per capita. Second, the choice of the time interval (from 1970-2022) also overstates the advance of the Swiss economy. Using data offering a longer time frame (going back to 1950) and including other countries, one can arrive at totally different conclusions from what is argued by EconomieSuisse\nFigure 1: Real GDP per capita growth in Switzerland in international comparison. Constant 2022 euros at ppp. Data source: World Inequality Database (WID).\nFigure 1 shows the evolution of the Swiss real gdp per capita at purchasing power parities (ppp) compared to a sample of Western countries, using high-quality data provided by the World Inequality Database (WID). One can see that Switzerland’ gdp per capita was indeed well in advanced over other Western countries in the post-war period. However, starting from the 1970s, gdp per capita growth has slowed down in Switzerland, and the difference with other countries is gradually getting smaller. Contrary to what is argued by EconomieSuisse, Switzerland has even been caught up and surpassed by Norway. It is interesting to note that Norway is not taken into account in EconomieSuisse’s empirical evidence, and this shows that EconomieSuisse has carefully chosen the countries to compare with Switzerland. Norway’s growth has been impressive since the 90s, and the country would have surpassed Switzerland already shortly after 2008 if the Great Recession did not happen. Moreover, The United States are now very close to Switzerland, at least when we take a longer historical perspective.\nAnother issue of the debate about Switzerland’s economic growth is the excessive focus on GDP as an indicator of a country’s economic wealth and performance. In that matter, other indicators such as national income1 also deserves close attention.\nFigure 2: Real net national income per capita in Switzerland and other Western countries. Constant 2022 euros at ppp. Data source: WID.\nFigure 2 puts even more into question the discourse elaborated by EconomieSuisse. In terms of average national income, Switzerland has been caught up and surpassed Norway and the United States since the 2008 crisis. Other countries such as Denmark are also getting closer to Switzerland.\nHow can we better grasp the relative economic slow-down of Switzerland ? An important issue is that of the so-called “secular stagnation” phenomena that is observed in developed countries since the Great Recession. When we look at the average rate of growth of real GDP and national income per capita over 10 years with a moving window, we can have a better overview of the evolution of the Switzerland’s economic growth.\nFigure 3: Swiss GDP and national income per capita, estimated growth trend with a rolling ten-years window. 95% confidence intervals. The horizontal lines represent the average rate of growth over the whole period. Data source: WID."
  },
  {
    "objectID": "posts/growth debate switzerland/growth debate switzerland.html#footnotes",
    "href": "posts/growth debate switzerland/growth debate switzerland.html#footnotes",
    "title": "Economic stagnation in Switzerland: discussion of a growing debate",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNational income is defined as GDP minus depreciation of capital and plus net foreign income earned by residents in the rest of the world.↩︎"
  },
  {
    "objectID": "posts/About cycles and crises/cycles_crises.html",
    "href": "posts/About cycles and crises/cycles_crises.html",
    "title": "Crises & cycles",
    "section": "",
    "text": "One of the most striking phenomena in capitalist mode of production is certainly the recurring and systematic outbreak of financial and economic crises. Since the emergence of capitalism as a mode production in England during the 18th century and its progressive spread in Europe in the 19th and then in the world in the 20th and 21th centuries, the economy of capitalist countries must endure what economists call “business cycles”, that is to say, cyclical appearances of economic booms in production, employment and prices and busts (decline in production, employment and prices).\nHowever, the acknowledgement of business cycles as such took a long and tedious path. The first main broad examination of economic crisis can be found at the beginning of the 19th century during the famous “General Glut Controversy” which involved famous economists such as Say, Ricardo and Malthus. Say, for example, argued that long-term deep economic crisis could not occur because of what will be afterwards called “Say’s law”, that is, the fact that there can be no overall overproduction crisis since production creates its own demand. It creates its own demand mainly because it generates products which can be directly exchanged for other ones. For example, If I produce coffee beans and sell it, all the value I got through selling will be in return my demand for other commodities by the same amount of value. Marx, who can be considered as a pioneer in crisis and business cycles theory, made his critic of Say’s law a point of departure of his bold thoughts on crisis.\n\n\nAs Foley’ Understanding Capital (chapter 9) argues, there is no precise, consistent and modelled theory of crisis in Marx’ writings. In fact, he had diverse and dispersed analysis of different types of economic crisis that Marxists reconstructed and developed afterwards. Nonetheless, Marx’ main point on crisis is the following paradox he underlined: the capitalist system is characterized by an abundance of products (use values) due to tremendous developed productive forces and, at the same time, unfulfilled desires on the part of the working class. According to Marx, the main reason lies in the fact that production is carried under capitalism not to fulfill needs (produce use-value to satisfy needs and desires), but to accumulate profits (or, in Marxist terms, surplus value). Marxists reconstructed then three main theories of crisis: disproportionality crisis, underconsumption crisis and crisis due to the falling rate of profit.\nFirstly, in Capital Volume 2, Marx sets up a model of capital circulation through two department: department I (production of means of production) and II (production of means of consumption). For those two sectors to have balanced growth, there should be a balanced investment level between the two, such that there can be no overproduction in one department and/or not a lack of production in the other. The problem is that it is rarely the case, since capitalist allocate their capital not in order to balance the two department, but to accumulate surplus value. This type of crisis is called “disproportionality crisis”.\nSecondly, the underconsumption theory underlines the striking feature of the inability, under capitalism, to sell all the produced commodities. Since workers only receive a part of the value they produce in the form of wages, aggregate demand has a tendency to be always lower than the supply. Hence, their purchasing power will always be lower than the value of the produced commodities which need to be absorbed. This leaves an excess supply in the market and crisis follows. Foley argues that this version of the underconsumption theory is simplistic, because that part of the value that the worker does not get goes as income to the capitalist, who can also support additional demand for output (through consumption, or through investment, a point whom which Marx was aware of). At this precise point, Rosa Luxemburg had an important and rather unorthodox analysis. She argued that capitalist economy is structurally incapable of generating enough aggregate demand. Her argument is that even if capitalists have enough money and the willingness to invest (which would normally support aggregate demand), they cannot invest ad infinitum. In effect, the purpose of investment is in fine also to produce consumption goods (it is also its justification), one cannot invest only to accumulate means of production per se. Foley underlines the fact that this analysis by Luxemburg is strikingly un-Marxist, since Marx himself argued that the ultimate goal of capitalist production is accumulation of surplus value.\nThirdly Foley explains two versions of the falling rate of profit theory, the first being the “profit squeeze” theory and the second the tendency of falling profit rate: 1) The expansion phase of the business cycle leads to higher employment, the workers’ bargaining power rises and thus wages rise. That leads to a fall of the profit rate and this leads to a crisis since capitalists do not invest further until production and employment fall enough to decrease workers’ bargaining power so that wages decrease and profit rate can go up. 2) Pressured by competition and the search for profit to innovate, the capitalists tend to replace more and more labor by capital, that is, mainly machinery and equipment goods. In Marxist terms, there is a rise of the organic composition of capital (c/v with c=constant capital: machinery, equipment; and v the variable capital: wages) which leads to a fall of the profit rate (defined as p=s/(c+v) = (s/v)/(c/v+1) with s the surplus value).\n\n\n\nApart from Marx and Marxists scholars, another trend emerged in the end of the 19th and in the first half the of 20th century: economists who became increasingly interested in the measurement and analysis of business cycles. The biggest impetus in that matter can surely be accredited to Wesley Clair Mitchell (1874-1948) who was the first director of the National Bureau of Economic Research (founded in 1920) and conducted tremendous research on the measurement and analysis of business cycles. Influenced by Thorstein Veblen and his critic of neoclassical economics, Mitchell sought a new way of conducting economic analysis and research, mainly through historical and empirical data analysis. He was convinced that his empirical and inductive approach (constructing new theories from empirical evidence) would make older economic theories obsolete: “quantitative analysis” would produce more robust economic reasonings than “qualitative analysis”, that is, neoclassical deductive approach. Conclusions Mitchell drew from his work are remarkably similar to Marx’: business cycles are chiefly driven by the incessant pursuit for profit. Assuredly, Mitchell’s work paved the way to a golden age of statistical and quantitative analysis of business cycles in the 1920s, so much that even economists hitherto not interested in that domain sought to bring their own contribution to business cycle analysis. A striking example is surely Irving Fisher (1867-1947) and his debt-deflation theory. Fisher, who recognized that he was a newcomer on the subject, made a distinction between “forced cycles”, cycles determined and driven by external factors (Fisher gives the example of the sun spot theory which sought to explain economic cycles through astronomical cycles) and “free cycles”, namely, cycles whose prime movers are not external, but inherent and intrinsic to the cycle. It is to this latter case, the free cycles, that Fisher wants to explain taking what he calls a “economic science” point of departure as opposed to economic history which only describe business cycles. Fisher wants to identify relations and tendencies that drive the cycles. His main idea is that the main two factors are (1) over-indebtedness and (2) deflation, the former being more important than the latter. The state of over-indebtedness leads to worries on the part of either debtors and/or creditors. As debtors try to sell their assets to pay back their debts, the quantity of money in circulation and the velocity of money decline (agents consume less to save). This leads to a fall in price level and a paradoxical situation where the more economic agents try to get rid of their debts, the more its value in real terms rise: “the more the debtors pay, the more they owe” (Fisher 1933: 54). The fall in general price level leads to a fall of profit rate and hence to a decline of production and employment. This leads to general pessimism, loss of confidence and hoarding, the value of money rises even more, prices decline further and the real interest rate rises. Fisher argues it would be insane not to intervene through economic policy and that the economy should not be left to natural market forces: reflation policy becomes necessary. However, it would not be sufficient, since the main problem of the tendency towards over-indebtedness remains. This leads Fisher to explore some of the reasons which drive the economy towards this state of over-borrowing and over-indebtedness. He argues, finally, that the mains factors are the new investments opportunities associated with great profit expectations (because it will encourage agents to borrow to invest). Those new investments opportunities are, in return, created by new inventions and technological breakthrough and progress.\n\n\n\nBabson presenting one of his famous babsoncharts on business cycles\n\n\n\n\n\nTo sum up, analysis of business cycles began first with inquiries and reflections around market gluts and economic crises. As we saw, Marx and Marxists scholars can be considered as pioneers in the analysis of crises and business cycles. Outside Marxism, it was W.C. Mitchell who gave the greater impetus into business cycles analysis. As stated above, Mitchell and Marx analysis of business cycles as profit driven are similar. Nonetheless, I think Marx’ investigation goes one step further, as he also sought to explain the origins of profit through exploitation and the extraction of surplus value. Notwithstanding, Mitchell’s contribution improved greatly the quantitative measurement and analysis of business cycles in such a way that it even drew a mainstream neoclassical economist such as Irving Fisher to elaborate a bold economic theory regarding business cycles. However, if Fisher’s debt-deflation theory provides insights into the economic mechanisms of economic crises and cycles, it is limited by the fact that it neglects the political economy/ political aspects which have an important role in business cycles (one could also argue that Mitchell also omits this fact, as many business cycles economists did). In conclusion, I would like to acknowledge the important works of Kalecki, who, in that matter, successfully incorporated a political economy dimension into business cycle analysis, an analysis I find particularly valuable (this theory is briefly explained in Foley’s book chapter). In his political business cycle theory, Kalecki argues that cycles are characterized by shifting class alliances between the working class, the business class and the rentiers: during a crisis, the business class pressures the state to revive the economy, aligning thus its class interests with those of the working class. As production and employment rise thanks to state intervention, the business side becomes progressively more reluctant to it, because it loses bargaining power (jobs are secured and workers do not fear unemployment and sacks). As employment and inflation rise, the capitalists have incentive to pressure the state to conduct more orthodox economic policy: this puts an end to the expansion phase of the political business cycle until the economy slumbers once again in a crisis in which state intervention is once again required and so on and so forth. Foley argues that Kalecki’s political business cycle model is a recent underconsumptionist theory, because it stresses the incapacity/unwillingness of a capitalist-dominated society to tolerate a high level of aggregate demand for too long."
  },
  {
    "objectID": "posts/About cycles and crises/cycles_crises.html#marxists-theories-of-crisis",
    "href": "posts/About cycles and crises/cycles_crises.html#marxists-theories-of-crisis",
    "title": "Crises & cycles",
    "section": "",
    "text": "As Foley’ Understanding Capital (chapter 9) argues, there is no precise, consistent and modelled theory of crisis in Marx’ writings. In fact, he had diverse and dispersed analysis of different types of economic crisis that Marxists reconstructed and developed afterwards. Nonetheless, Marx’ main point on crisis is the following paradox he underlined: the capitalist system is characterized by an abundance of products (use values) due to tremendous developed productive forces and, at the same time, unfulfilled desires on the part of the working class. According to Marx, the main reason lies in the fact that production is carried under capitalism not to fulfill needs (produce use-value to satisfy needs and desires), but to accumulate profits (or, in Marxist terms, surplus value). Marxists reconstructed then three main theories of crisis: disproportionality crisis, underconsumption crisis and crisis due to the falling rate of profit.\nFirstly, in Capital Volume 2, Marx sets up a model of capital circulation through two department: department I (production of means of production) and II (production of means of consumption). For those two sectors to have balanced growth, there should be a balanced investment level between the two, such that there can be no overproduction in one department and/or not a lack of production in the other. The problem is that it is rarely the case, since capitalist allocate their capital not in order to balance the two department, but to accumulate surplus value. This type of crisis is called “disproportionality crisis”.\nSecondly, the underconsumption theory underlines the striking feature of the inability, under capitalism, to sell all the produced commodities. Since workers only receive a part of the value they produce in the form of wages, aggregate demand has a tendency to be always lower than the supply. Hence, their purchasing power will always be lower than the value of the produced commodities which need to be absorbed. This leaves an excess supply in the market and crisis follows. Foley argues that this version of the underconsumption theory is simplistic, because that part of the value that the worker does not get goes as income to the capitalist, who can also support additional demand for output (through consumption, or through investment, a point whom which Marx was aware of). At this precise point, Rosa Luxemburg had an important and rather unorthodox analysis. She argued that capitalist economy is structurally incapable of generating enough aggregate demand. Her argument is that even if capitalists have enough money and the willingness to invest (which would normally support aggregate demand), they cannot invest ad infinitum. In effect, the purpose of investment is in fine also to produce consumption goods (it is also its justification), one cannot invest only to accumulate means of production per se. Foley underlines the fact that this analysis by Luxemburg is strikingly un-Marxist, since Marx himself argued that the ultimate goal of capitalist production is accumulation of surplus value.\nThirdly Foley explains two versions of the falling rate of profit theory, the first being the “profit squeeze” theory and the second the tendency of falling profit rate: 1) The expansion phase of the business cycle leads to higher employment, the workers’ bargaining power rises and thus wages rise. That leads to a fall of the profit rate and this leads to a crisis since capitalists do not invest further until production and employment fall enough to decrease workers’ bargaining power so that wages decrease and profit rate can go up. 2) Pressured by competition and the search for profit to innovate, the capitalists tend to replace more and more labor by capital, that is, mainly machinery and equipment goods. In Marxist terms, there is a rise of the organic composition of capital (c/v with c=constant capital: machinery, equipment; and v the variable capital: wages) which leads to a fall of the profit rate (defined as p=s/(c+v) = (s/v)/(c/v+1) with s the surplus value)."
  },
  {
    "objectID": "posts/About cycles and crises/cycles_crises.html#from-crises-to-business-cycles",
    "href": "posts/About cycles and crises/cycles_crises.html#from-crises-to-business-cycles",
    "title": "Crises & cycles",
    "section": "",
    "text": "Apart from Marx and Marxists scholars, another trend emerged in the end of the 19th and in the first half the of 20th century: economists who became increasingly interested in the measurement and analysis of business cycles. The biggest impetus in that matter can surely be accredited to Wesley Clair Mitchell (1874-1948) who was the first director of the National Bureau of Economic Research (founded in 1920) and conducted tremendous research on the measurement and analysis of business cycles. Influenced by Thorstein Veblen and his critic of neoclassical economics, Mitchell sought a new way of conducting economic analysis and research, mainly through historical and empirical data analysis. He was convinced that his empirical and inductive approach (constructing new theories from empirical evidence) would make older economic theories obsolete: “quantitative analysis” would produce more robust economic reasonings than “qualitative analysis”, that is, neoclassical deductive approach. Conclusions Mitchell drew from his work are remarkably similar to Marx’: business cycles are chiefly driven by the incessant pursuit for profit. Assuredly, Mitchell’s work paved the way to a golden age of statistical and quantitative analysis of business cycles in the 1920s, so much that even economists hitherto not interested in that domain sought to bring their own contribution to business cycle analysis. A striking example is surely Irving Fisher (1867-1947) and his debt-deflation theory. Fisher, who recognized that he was a newcomer on the subject, made a distinction between “forced cycles”, cycles determined and driven by external factors (Fisher gives the example of the sun spot theory which sought to explain economic cycles through astronomical cycles) and “free cycles”, namely, cycles whose prime movers are not external, but inherent and intrinsic to the cycle. It is to this latter case, the free cycles, that Fisher wants to explain taking what he calls a “economic science” point of departure as opposed to economic history which only describe business cycles. Fisher wants to identify relations and tendencies that drive the cycles. His main idea is that the main two factors are (1) over-indebtedness and (2) deflation, the former being more important than the latter. The state of over-indebtedness leads to worries on the part of either debtors and/or creditors. As debtors try to sell their assets to pay back their debts, the quantity of money in circulation and the velocity of money decline (agents consume less to save). This leads to a fall in price level and a paradoxical situation where the more economic agents try to get rid of their debts, the more its value in real terms rise: “the more the debtors pay, the more they owe” (Fisher 1933: 54). The fall in general price level leads to a fall of profit rate and hence to a decline of production and employment. This leads to general pessimism, loss of confidence and hoarding, the value of money rises even more, prices decline further and the real interest rate rises. Fisher argues it would be insane not to intervene through economic policy and that the economy should not be left to natural market forces: reflation policy becomes necessary. However, it would not be sufficient, since the main problem of the tendency towards over-indebtedness remains. This leads Fisher to explore some of the reasons which drive the economy towards this state of over-borrowing and over-indebtedness. He argues, finally, that the mains factors are the new investments opportunities associated with great profit expectations (because it will encourage agents to borrow to invest). Those new investments opportunities are, in return, created by new inventions and technological breakthrough and progress.\n\n\n\nBabson presenting one of his famous babsoncharts on business cycles"
  },
  {
    "objectID": "posts/About cycles and crises/cycles_crises.html#conclusion",
    "href": "posts/About cycles and crises/cycles_crises.html#conclusion",
    "title": "Crises & cycles",
    "section": "",
    "text": "To sum up, analysis of business cycles began first with inquiries and reflections around market gluts and economic crises. As we saw, Marx and Marxists scholars can be considered as pioneers in the analysis of crises and business cycles. Outside Marxism, it was W.C. Mitchell who gave the greater impetus into business cycles analysis. As stated above, Mitchell and Marx analysis of business cycles as profit driven are similar. Nonetheless, I think Marx’ investigation goes one step further, as he also sought to explain the origins of profit through exploitation and the extraction of surplus value. Notwithstanding, Mitchell’s contribution improved greatly the quantitative measurement and analysis of business cycles in such a way that it even drew a mainstream neoclassical economist such as Irving Fisher to elaborate a bold economic theory regarding business cycles. However, if Fisher’s debt-deflation theory provides insights into the economic mechanisms of economic crises and cycles, it is limited by the fact that it neglects the political economy/ political aspects which have an important role in business cycles (one could also argue that Mitchell also omits this fact, as many business cycles economists did). In conclusion, I would like to acknowledge the important works of Kalecki, who, in that matter, successfully incorporated a political economy dimension into business cycle analysis, an analysis I find particularly valuable (this theory is briefly explained in Foley’s book chapter). In his political business cycle theory, Kalecki argues that cycles are characterized by shifting class alliances between the working class, the business class and the rentiers: during a crisis, the business class pressures the state to revive the economy, aligning thus its class interests with those of the working class. As production and employment rise thanks to state intervention, the business side becomes progressively more reluctant to it, because it loses bargaining power (jobs are secured and workers do not fear unemployment and sacks). As employment and inflation rise, the capitalists have incentive to pressure the state to conduct more orthodox economic policy: this puts an end to the expansion phase of the political business cycle until the economy slumbers once again in a crisis in which state intervention is once again required and so on and so forth. Foley argues that Kalecki’s political business cycle model is a recent underconsumptionist theory, because it stresses the incapacity/unwillingness of a capitalist-dominated society to tolerate a high level of aggregate demand for too long."
  },
  {
    "objectID": "economics_series/production theory/mainstream_production_theory.html",
    "href": "economics_series/production theory/mainstream_production_theory.html",
    "title": "Mainstream Production Theory",
    "section": "",
    "text": "As I explained in the consumer theory post, the most important element of mainstream neoclassical theory is the concept of market equilibrium under perfect competition. In the consumer theory post, I explained how mainstream economists derive the market equilibrium downwards slopping demand curve. Here, the objective is to understand how the other curve, the upward slopping supply curve, is derived. To understand why mainstream economics arrived to this result, one needs to understand mainstream production theory, which is the goal of this post."
  },
  {
    "objectID": "economics_series/production theory/mainstream_production_theory.html#intro",
    "href": "economics_series/production theory/mainstream_production_theory.html#intro",
    "title": "Mainstream Production Theory",
    "section": "",
    "text": "As I explained in the consumer theory post, the most important element of mainstream neoclassical theory is the concept of market equilibrium under perfect competition. In the consumer theory post, I explained how mainstream economists derive the market equilibrium downwards slopping demand curve. Here, the objective is to understand how the other curve, the upward slopping supply curve, is derived. To understand why mainstream economics arrived to this result, one needs to understand mainstream production theory, which is the goal of this post."
  },
  {
    "objectID": "economics_series/production theory/mainstream_production_theory.html#how-firms-produce-in-neoclassical-mainstream-theory",
    "href": "economics_series/production theory/mainstream_production_theory.html#how-firms-produce-in-neoclassical-mainstream-theory",
    "title": "Mainstream Production Theory",
    "section": "How firms produce in neoclassical-mainstream theory",
    "text": "How firms produce in neoclassical-mainstream theory\nNeoclassical production theory has a very specific way to conceive production. In fact, the theory considers that production, how firms or productive units transform inputs (of labour, raw materials or other intermediary inputs/materials/services) into output (final goods or services produced), can be modeled as what is now famously known as the neoclassical production function. The latter is written as:\n\\(Y = F(K,L)\\)\nWith Y the output, the quantity produced, K the quantity of capital used and L the quantity of labour employed. From now on, we will work with the following neoclassical production function as an example:\n\\(Y = K^{1/2}L^{1/2}\\)\nHere is how to write this function in R:\n\nneo_prod_fun &lt;- function(l, k) (l^(1/2)) * (k^(1/2))\n\nThe neoclassical production function rests on important assumptions which illustrate how neoclassical theory conceive the production process:\n\nConstant returns to scale\nConstant returns to scale means that quantities produced increase proportionally with the quantity of inputs. For instance, if the quantities of inputs (capital and labour) are doubled, output will double.\nPositive but diminishing marginal returns\nAlso called diminishing marginal products, or decreasing marginal productivity. This means that all factors of production (capital and labor) show positive but diminishing marginal returns: when we increase one of the inputs by one, with all the other inputs fixed, output will increase, but by less than the previous additional input. For example, the quantity of capital is often considered fixed in the short run, making labor the only variable input in the short run. Labor is assumed to have a decreasing marginal productivity (or marginal returns): with capital fixed, each time a firm employs an additional laborer, output will increase, but by less than when the previous additional laborer was employed. In our example, marginal product of labour is the first derivative with respect to labour:\n\n\\[\n\\begin{aligned}\n\\frac{\\partial{Y}}{\\partial{L}} = 0.5K^{0.5}L^{0.5-1}\n\\\\\n= 0.5K^{0.5}L^{-0.5}\n\\end{aligned}\n\\] Here is how to find the marginal productivity of labor function of our example in R:\n\nMPL &lt;- Deriv(neo_prod_fun, \"l\")\nMPL\n\nfunction (l, k) \n0.5 * (sqrt(k)/sqrt(l))\n\n\nIf we fix the amount of capital to any value, 10 and 25 for instance, we can plot the marginal productivity of labour to better illustrate the principle of diminishing marginal returns:\n\n\n\n\n\n\n\n\n\n\nSubstitutability of factors/inputs\nThis third assumption implies that there is an infinite choice bewteen capital and labour for each level of output. For example, to produce 20 units of output, firms have an infinite choice to combine labour and capital. This is a strong assumption and as we will see the “rival” of the neoclassical production function is the Leontief production function, which is also called the “fixed-proportion” production function, because it assumes that for each level of output, there is only one possible combination of capital and labour.\n\nNote that in the standard neoclassical production model makes the assumption that firm operate under perfect competition: perfect competition has three important characteristics:\n\nPrice taking\nSince there are a lot of consumers and firms in the market, firms have not impact on the price, they cannot manipulate the latter. Price is thus exogenous and given.\nProduct homogeneity\nThe good produced in the market by the firms is the same, it is homogenous.\nFree entry and exit\nThere are no special costs associated with the entry in the market for any potential firm nor costs associated with exit (a firm that would want to exit the market)."
  },
  {
    "objectID": "economics_series/production theory/mainstream_production_theory.html#short-run-and-long-run",
    "href": "economics_series/production theory/mainstream_production_theory.html#short-run-and-long-run",
    "title": "Mainstream Production Theory",
    "section": "Short run and long run",
    "text": "Short run and long run\nNeoclassical production theory makes an important distinction between the short run and the long run, with direct implication on how to manipulate the production function:\n\nShort run\nWe talk about short run when not all factors of production can be changed. Capital is typically considered as a fixed factor in the short run, whereas labour can still be changed. Thus, in the short run, capital is fixed and labour is variable.\nLong run\nWe talk about long run when all factors can be changed. This is the amount of time needed to make all inputs variable."
  },
  {
    "objectID": "economics_series/production theory/mainstream_production_theory.html#optimal-choice-of-output-capital-and-labour",
    "href": "economics_series/production theory/mainstream_production_theory.html#optimal-choice-of-output-capital-and-labour",
    "title": "Mainstream Production Theory",
    "section": "Optimal choice of output, capital and labour",
    "text": "Optimal choice of output, capital and labour\nIn neoclassical production theory, there is an optimal choice of output, capital and labour, which implies that there is an ideal size for a firm. This is a strong assessment, because this implies that each firm has an optimal size at which they grow and then stop growing once they reach this optimal size."
  },
  {
    "objectID": "economics_series/production theory/mainstream_production_theory.html#isoquant-and-isocost",
    "href": "economics_series/production theory/mainstream_production_theory.html#isoquant-and-isocost",
    "title": "Mainstream Production Theory",
    "section": "Isoquant and Isocost",
    "text": "Isoquant and Isocost\nIsoquants are a way to represent graphically any combination of labour and capital for any level of ouput. Capital is typically plotted on the y axis and labour on the x axis and output is fixed along each curve. If we go back to our production function curve \\(Y = k^{0.5}L^{0.5}\\), we have to isolate k to draw isoquants for this function, and then choose any value of output (Y):\n\\(K = \\frac{Y^{2}}{L}\\)\n\n\n\n\n\n\n\n\n\nAs in consumer theory, firms cannot choose any combination of capital and labour they want because, as consumers face a budget constraint, firms also face a constraint: their total cost. The isocost line, which shows all possible combinations of labour and capital that the firm can purchase with its current budget, hence total cost. Isocost is for the firm what the budget constraint line is to consumers.\nThe isocost function can be written as:\n\\(TC = wl +rK\\)\nWith TC the total cost being equal to the wage (rate) w times labour L and r the rental cost of capital K. r includes the depreciation cost of capital and the lost interest rate (if the capital was invested somewhere else).\nIsocost can then be rearranged to:\n\\(K = TC/r - (w/r)/L\\)\nFor example, let’s say that the total cost over rental cost of capital (TC/r) is equal to 40 and the wage rate - rental rate is equal to 2 (w/r = 2). The function becomes \\(K = 10 - 2L\\)\n\nisocost &lt;- function(l) 40 - 2*l\n\n\n\n\n\n\n\n\n\n\nThe steps to derive the optimal choice of output, capital and labor is the same than for consumer theory: here the slope of the isoquant is called the marginal rate of technical substitution (MRTS). Setting the latter equal to the slope of the isocost, which is w/r (1 in our example) and solving for K, Y and L. In our example, the equilibrium level of output is 14.14, with K = 20 and L = 10."
  },
  {
    "objectID": "economics_series/production theory/mainstream_production_theory.html#profit-equation-and-profit-maximisation",
    "href": "economics_series/production theory/mainstream_production_theory.html#profit-equation-and-profit-maximisation",
    "title": "Mainstream Production Theory",
    "section": "Profit equation and profit maximisation",
    "text": "Profit equation and profit maximisation\nThe neoclassical production function we saw above is not only important because it models how neoclassical firms produce, but also because it is part of the profit equation that firms want to maximize (in neoclassical theory, firms maximize their profits as consumers maximize their utility). The profit equation is written as:\n\\(\\pi = PQ - TC\\)\nor more generally\n\\(\\pi(q) = R(q) - TC(q)\\)\nWith \\(\\pi\\) the profits of the firm, \\(PQ\\) the price P multiplied by the quantity produced Q, minus the total cost TC. We saw above that total cost was equal to \\(TC = wL -rK\\), but micro manuals sometimes simplify this by just stating that total cost is equal to variable cost and fixed cost (in the short run, because in the long run all factors are variable).\nTwo important concepts are linked to this profit equation: marginal revenue and marginal cost.\n\nMarginal revenue is the change in revenue resulting from from a one-unit increase in output.\nMarginal cost is the change in cost associated with a one-unit increase in output\n\nIn perfect competition, marginal revenue is determined outside the firm, the latter having no influence on it because it has no power to manipulate the price. Marginal revenue is in fact the market price of the good produced by the firm and it is exogenous in a perfectly competitive market. On the other hand, marginal cost is inherent to the firm because it depends on the level of output chosen by the firm.\nMathematically, marginal revenue is the derivative of the total revenue of the firm \\(P*Q\\) and is equal to the market price \\(P\\). Marginal cost is the derivative of total cost, but marginal cost is not the same whether we are in the short of the long run. In the short run, only labour is variable, so the marginal cost is the additional increase in cost associated with a one-unit labour increase (an additional labourer)\n\\(MC = \\frac{\\Delta{VC}}{\\Delta{}{}Q} = \\frac{w\\Delta{}{}L}{\\Delta{Q}}\\)\nBut in the long run, marginal cost is the additional cost resulting from an additional increase of any input \\(MC = \\frac{\\Delta{TC}}{\\Delta{Q}}\\). Algebraically, marginal cost is the derivative of total cost (long run) or variable cost (short run).\nThen, neoclassical production theory explains that the profit equation \\(\\pi = PQ - TC\\) has a concave form. The logic behind this is the fact that the theory considers that firms have increasing economies of scale at low level of production and, as output increases, economies of scale will gradually decrease and become negative (economies of scale at the beginning and the diseconomies of scale at some point).\n\nEconomies of scales\nWhen output can be increased for less than proportionnaly increasing the cost. For instance if output is doubled, cost less than double. Neoclassical theory typically considers that for low level of output (small firms), economies of scale are more likely because of productivity gains due to\n\nspecialization,\nreorganization of the production process and\nbargaining power for some intermediary inputs (advantage of buying in bulk).\n\nDiseconomies of scales\nWhen output is increased, cost more than proportionnaly increase. For instance, if output is doubled, cost more than double. Neoclassical theory considers that diseconomies of scale arrive at some point for high level of ouput because of:\n\nLack of space in the factory/working place ==&gt; more difficult to do the job\nIncreasing number of tasks ==&gt; management becomes more difficult due to increasing complexity of tasks\nThe advantage of buying in bulk disappears when certain quantity level is reached.\n\n\nThose factors explain why the profit equation is concave. This has strong implications, because that means there is an optimal size for the firm associated with an optimal level of output which maximise profits.\nThen, we can show that profit maximization under all of those conditions lead to the marginal cost being equal to marginal revenue\n\\((\\pi)' = 0\\) (()’ meaning derivative)\n\\(0 =(PQ)' - (TC)'\\) the derivative of total revenue is the marginal revenue, and the derivative of the total cost is the marginal cost\n\\(0 =MR - MC\\)\n\\(MR = MC\\)\nGraphically, the profit equation has a negative-concave shape, and optimal level of output is set where profits are at their maximum:"
  },
  {
    "objectID": "economics_series/production theory/mainstream_production_theory.html#deriving-the-supply-curve",
    "href": "economics_series/production theory/mainstream_production_theory.html#deriving-the-supply-curve",
    "title": "Mainstream Production Theory",
    "section": "Deriving the supply curve",
    "text": "Deriving the supply curve\nWhen will firms decide to supply a good at a given price? And why the quantities supplied increase with the price of the good?\nMicro theory tells that the supply curve represents all the combinations of price and output (quantity of the good supplied) in which the price is higher than the average variable cost. If the market price is below the average variable cost (AVC), supplying the good would not be profitable. If the price is higher than AVC, it is profitable for the firm to produce (and the firm will supply the quantity at which its marginal revenue, the price, is equal to the marginal cost). Then, if price increases, marginal revenue increases, and firms can increase production and quantity supplied until marginal revenue is again equal to the marginal cost."
  },
  {
    "objectID": "economics_series/neo-kaleckian models/neo-kaleckian models.html",
    "href": "economics_series/neo-kaleckian models/neo-kaleckian models.html",
    "title": "Neo-Kaleckian Models",
    "section": "",
    "text": "What follows is extensively based on the fourth chapter of Heterodox Macroeconomics: Models of of Demand, Distribution and Growth (2019) by Robert A. Blecker and Mark Setterfield."
  },
  {
    "objectID": "economics_series/neo-kaleckian models/neo-kaleckian models.html#intro",
    "href": "economics_series/neo-kaleckian models/neo-kaleckian models.html#intro",
    "title": "Neo-Kaleckian Models",
    "section": "Intro",
    "text": "Intro\nMichał Kalecki (1899-1970) was a Polish economist and major figure of heterodox economics. He made important contributions in macroeconomics and political economy, and his work represents a crucial legacy for a lot of contemporary heterodox economics.\nThe models that will be presented here are related to the macroeconomics of Kalecki. However, it is important to keep in mind that Kalecki also developed a theory of political business cycles (I talk shortly about this theory here). What will be presented here (based on the manual) is just one part of the many important contributions of Kalecki to this discipline.\nThe Neo-Kaleckian model is actually posterior to Kalecki (hence “Neo”), but heavily based on his work. It was developed by Kaleckian economists in the 70s and 80s based on Kalecki work, but also on the work of Josef Steindl (1912-1993), an austrian post-keynesian economist who was a close collegue of Kalecki.\nUntil now, almost all the models I summarized in my heterodox economics series, the classical-marxian models (CMMs) and the Neo-Keynesian models (NKMs), explain that more rapid growth can only be achieve at the expense of a more unequal distribution of income. In other words, the CMM and NKM explain that increasing the rate of growth implies more inequality through either lower real wage or lower wage share.\nOne major flaw of the CMM and NKM is that they either don’t incorporate the important role of aggregate demand in the analysis (in the case of the CMMs) or only partially incorporate it (for the NKMs). Why is it important to take aggregate demand into account when one wants to analyze the determinants of economic growth? Before answering this question, let’s first define aggregate demand. The latte can be defined as the sum of all demand sources for goods and services in an economy. Aggregate demand is often summarized with the following formula:\n\\[Y^d \\equiv C+I+G+(X-M)\\]\nWith \\(Y^d\\) aggregate demand, which depends positively:\n\n\\(C\\), private consumption demand (by individuals, not firms nor the government, nor foreigners). \\(C\\) is the sum of all private consumption in a given economy.\n\\(I\\), investment demand. The latter represents all purchases of capital goods by firms and all new residential investment. It includes mainly the purchase by firms of machinery, tools, raw materials and other equipment. However, residential investment (purchase of new houses) is also considered as an investment in national accounts.\n\\(G\\), the demand coming from government purchases of goods and services.\n\\(X\\), the demand coming from purchase by foreigners of national goods and services. It represents all exportation made by the economy. Subtracted by \\(M\\), which is imported goods and services, we get the trade balance.\n\nWhy is aggregate demand important? Because any increase in demand gives impetus to economic expansion: the higher the demand for goods and services, the more firms will want to respond by producing more.\nThe (neo)-kaleckian models are thus very important among heterdox models of economic growth, because it the former incorporates the role of aggregate demand and its impact on growth, even in the long run."
  },
  {
    "objectID": "economics_series/neo-kaleckian models/neo-kaleckian models.html#main-characteristics-of-the-model",
    "href": "economics_series/neo-kaleckian models/neo-kaleckian models.html#main-characteristics-of-the-model",
    "title": "Neo-Kaleckian Models",
    "section": "Main characteristics of the model",
    "text": "Main characteristics of the model\n\nCapacity rate of utilization\nThe main difference of the Neo-Kaleckian model with the CMMs and NKMs is that the Neo-Kaleckian model does not make the assumption that the capacity rate of utilization is fixed, constant, or constant at full level \\(u = 1\\). But what is the rate of capacity utilization already?\nThe rate of capacity utilization \\(u\\) is the ratio of actual output \\(Y\\) to the full-capacity output \\(Y_k\\), which is the output level when all capital is used in the economy. The capacity rate can be thus written as\n\\[u = \\frac{Y}{Y_k}\\] If we consider, as the CMMs and NKMs, that \\(u = 1\\), we simply say that \\(Y = Y_k\\). In other words, \\(u = 1\\) means that all capital is used in the production process in the economy. If, for instance, \\(u = 0.5\\), that means that actual output is only half of what it could be if all capital was used in production.\nA major characteristics of the Neo-Kaleckian model is that the model considers that the capacity rate \\(u\\) is flexible and not equal to one: the level of output is thus never at its full capacity level, contrary to the previous models presented in the manual.\n\n3 Reasons why u&lt;1\nWhy does the Neo-Kaleckian model has such a conception of the capacity rate? There are three main reasons for which firms do not operate at full capacity (full utilization of capital).\n\nIndivisibilities\nFirms tend to purchase capital goods (machinery, raw materials, tools…) which can be obtained only in large and discontinuous units, and thus can be operated at less than 100 per cent of their potential.\nBuilding ahead of demand\nSince firms do not know and cannot know what the demand for their products will be in the future (fundamental uncertainty of the future), they will hold excess capacity (excess capital not used in production) in case if demand rise in the future. If firms did not hold excess capacity and if demand suddenly happened to rise, they would not be able to respond by increasing production.\nEntry deterrence\nMost large firms operate in a non-competitive (great number of competitive firms) framework, in which only a few large oligopolistic firms compete in the market. In this situation, these large firms will hold excess capacity as a weapon in case if new firms (new entrants) would want to enter the market as producers and compete with the established firms. Having excess capacity allows established firms to be able to rise production immediately and thus push price downwards to crush any potential new competitors.\n\n\n\n\nPrices as markups over costs\nIn most models described until now, none of them had a precise model/equation describing how prices are set by firms. The Neo-Kaleckian as an explicit and precise definition of how prices are set. Recall that in the context of perfect competition, standard economic theory tells that prices are the result of the equilibrium between demand and supply. In the Neo-Kaleckian framework, markets are not perfectly competitive and firms have what economics call “market power”, that is, the power to influence prices and thus set the latter.\nHow will firms then set prices in the Neo-Kaleckian model? The manual presents the following price equation:\n\\[P = (1 + \\tau)Wa_0\\]\nWhich means that a firm will increase price \\(P\\) if it decides to rise its markup rate \\(\\tau\\), or if nominal wage increase \\(W\\) (since nominal wage is a cost for the firm, note that here other costs are ignored for simplicity and only labor costs are taken into account), or if \\(a_0 = \\frac{L}{Y}\\), the labor/output ratio increases. Here, \\(Y\\) refers to the output produced by the firm and \\(L\\) the amount of labor employed by the firm.\nWhat will influence change in the markup rate \\(\\tau\\)? Kalecki and the manual give five main factors which will influence the markup (a + or - sign is added next to each factor to indicate whether it rises the markup or decreases it):\n\nIndustrial concentration (+)\nThe fewer the firms are in competition with each other, the less the market is competitive and the more firms have market power to increase prices through an increase in their markup.\n‘Overheads’ or fixed costs (+)\nBy these overhead and fixed costs, we mean all the costs associated with machinery and equipment, management, maintenance (overhead labor), Research & Development expenses, intellectual property rights costs or debt service expenses. All these costs are taken into account by firms, which want to set price with a markup over these costs. If these costs increase, firms will also want to increase the markup to set a prices bringing enough revenues.\nSales effort (+)\nWhen firms operate under an oligopolistic framework (few firms, low competition between firms), adverstising is important to increase the number of consumers. Moreover, adverstising is also useful for ’product differentiation”, that is, convincing consumers that the firm’s products are different than the other products offered in the market. Consumers would then be more disposed to consume the differentiated product at a higher price. Advertising is thus both a way to increase the number of consumers and a way to increase oligopolistic market power.\nStrength of labour unions (-)\nIf workers within a firm have enough bargaining power, they may are able to capture parts of the firms’s potential profits. The degree of power workers can have depend on the existence and strength of labor unions: the more labor unions there are and the stronger and more well-organized and powerful they are, they more firms will have to share their value added, and thus decrease their markups.\nExternal competition (-)\nThe more national firms are in competition with the rest of the world, the more competitive pressure they have to lower their price, and thus their markup.\n\n\n\nNew wage-profit relationship\nAs with the CMMs, we start with the definition of national income under the income approach: national income is the sum of capitalists and workers’ income (profits and wages, ignoring depreciation, no government, only one good):\n\\[PY = WL + rPK\\] Dividing by \\(PY\\), we get a new inverse wage profit relationship:\n\\[w = \\frac{1}{a_0} - \\frac{a_1}{a_0} \\frac{r}{u}\\]\nThis equation is almost the same as the one we derived in the classical-marxian models. Since the CMMs assumed a full and constant capacity rate of utilization \\(u = 1\\), it does not appear in their wage-profit equation. However, \\(u\\) appears here in the denominator of the \\(r/u\\) fraction because it is assumed to be inferior to one (below full capacity) and flexible. In the equation above, wage appears to be a positive function of \\(u\\), but this is misleading since \\(u\\) and profit rate \\(r\\) are not independent of each other, meaning that if one of the two variables increases, the other will change as well, leaving an ambiguous impact on real wage \\(w\\)."
  },
  {
    "objectID": "economics_series/consumer theory/consumer theory.html",
    "href": "economics_series/consumer theory/consumer theory.html",
    "title": "Mainstream Consumer Theory",
    "section": "",
    "text": "What follows is a summary of the neoclassical market equilibrium model under perfect competition. This summary was inspired and constructed from my notes of my microeconomics classes."
  },
  {
    "objectID": "economics_series/consumer theory/consumer theory.html#market-equilibrium",
    "href": "economics_series/consumer theory/consumer theory.html#market-equilibrium",
    "title": "Mainstream Consumer Theory",
    "section": "Market equilibrium",
    "text": "Market equilibrium\nMarket equilibrium is perhaps the most important element of neoclassical theory. Stated simply, market equilibrium tells what will be the price of any object or service, as long as the latters are commodified. Market equilibrium explains not only the equilirbium level of prices and commodities of any good or service, but also the change in prices resulting from exogenous shocks (change in income, confidence, technology…)\nLet’s directly take an example: suppose that the demand for grain follows a negative linear function.\n\\(Q_{demand} = 7-0.5p\\)\nThat means that the quantity demanded for grains decreases if the price for grain increases and vice and versa.\nMoreover, let’s say that the quantity supplied for grains is a positive linear function of prices for grain: the higher the price, the more are firms willing to supply grains.\n\\(Q_{supply} = 1+0.6p\\)\n\ndemand &lt;- function(p) 7 - (0.5*p)\nsupply &lt;- function(p) 1 + 0.6*p\n\n\n\n\n\n\n\n\n\n\nTo find the equilibrium price and quantity, we equate the demand and supply functions and solve for q:\n\\[\n\\begin{aligned}\nq_{demand} = 7- 0.5*p\n\\\\\nq_{supply} = 1 + 0.6*p\n\\\\\nq_{demand} = q_{supply}\n\\\\\n7 - 0.5*p = 1 + 0.6*p\n\\\\\n6-0.11p = 0\n\\\\\np^* = 6/1.1 = 5.45\n\\end{aligned}\n\\]\nThe equilibrium price level is thus \\(p^* = 5.45\\). To find the equilibrium quantity, we simple put the value of the equilibrium price (5.45) into either the supply or demand function: \\(7-0.5*5.45 = 4.275 = p^*\\)\nIt is easy to check directly if the computation is correct in r:\n\nequilibrium &lt;- curve_intersect(demand, supply, empirical = FALSE, domain = c(1,10))\nequilibrium\n\n$x\n[1] 5.454545\n\n$y\n[1] 4.272727\n\n\n\n\n\n\n\n\n\n\n\nBut how did neoclassical theory arrive to this kind of model of equilibrium price and quantity determination? To understand better this model, we need to know why we have this positive supply curve and this negative demand curve. We will first investigate consumer choice theory, which is behind the negative demand curve, and then production-side theory, which is behind the positive supply curve"
  },
  {
    "objectID": "economics_series/consumer theory/consumer theory.html#neoclassical-mainstream-consumer-choice-theory",
    "href": "economics_series/consumer theory/consumer theory.html#neoclassical-mainstream-consumer-choice-theory",
    "title": "Mainstream Consumer Theory",
    "section": "Neoclassical-mainstream consumer choice theory",
    "text": "Neoclassical-mainstream consumer choice theory\nMainstream consumer choice theory has for ambition to explain consumers’ decision, that is to say, the choices consumers make between consuming one good or another.\n\nThe three assumptions\nConsumer theory makes important assumptions, which are the foundation of the theory:\n\nCompleteness:\nconsumers have complete knowledge about the goods and services they can potentially consume, they have clear preferences about these goods and services and can rank all of them (like a descending list where we would have the most preferred goods and services at the top and utility associated with goods and services would decrease as we go down in the list)\nTransitivity:\nPreferences regarding goods and services are transitive. That means that if a consumer prefers A to B and B to C, A is better than C.\nMore is better than less\n(non satiety assumption): Goods and services are always desirable. For example, if someone gives you one apple, then two, then three, then twenty, and then one thousands, you would always accept those apples, because you are still better off even if one gives you too many apples.\n\n\n\nUtility function, marginal utility\nConsumer theory then illustrates any choice between two goods with the help of the famous indifference curves , which show the relation between the demand for one good against the demand for another good (for example food and clothes, cars and bikes…). Indifference curves are based on utility functions whose really important property and assumption is the decreasing marginal utility principle. Decreasing marginal utility means that for every one additional unit of a given good a consumer get, the utility for this consumer increases less than the previous additional unit. Let’s say, for instance, that you don’t have food at the moment and you are hungry: if i give you one apple, you will be a lot better off and your utility will increase a lot when I give you this one apple. Then, if I give you another apple, your utility will still increase, but by less than when I gave you the first apple. Finally, after I give you an additional apple for the fifteenth time, your additional utility will still be positive, but by far more less than when I gave you the first apple.\n\n\n\n\n\n\n\n\n\nDecreasing marginal utility is an important assumption which explains the shape of the indifference curves. The latter, if the two goods are substitutes (but not perfect substitute) and not perfect complements, are convex-shaped. If, for instance, we consider an indifference curve for the choice between units of apples and bikes, the line of the indifference curve represents all the possible combination of the two goods which give the same utility for the consumer.\n\n\nIndifference curve\nIndifference curves are based on utility functions. An utility function can be for example:\n\\[\n\\begin{aligned}\nU(x,y)=x^20.18y\n\\end{aligned}\n\\] With x and y two different goods, apples and watches for example. To get the indifference curve function, we fix utility U at any positive value, and rearrange the function above to get y as a function of x and U:\n\\[\n\\begin{aligned}\nU = x^20.18y\n\\\\\ny = \\frac{U}{0.18*x^2}\n\\end{aligned}\n\\]\n\n\nYacas vector:\n[1] y == U/(0.18 * x^2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBudget constraint\nIn the indifference curve graph above, the consumer can choose any combination of apples and watches on the line, and those combinations would bring the same utility U = 10. However, one important element was not taken into account yet. This element is the fact that consumers are limited in their consumption decisions by their income. Neoclassical theory calls this budget constraint. For instance, let’s say that our consumer has an income of 600 francs. The price of one apple is 4 francs (one bag of apples to be more realistic) whereas the price of a watch is 200 francs. The budget constraint can be written as:\n\\[\n\\begin{aligned}\nIncome = P_{apple}*Q_{apple} + P_{watch}*Q_{watch}\n\\\\\n600 = 4*apple + 200*watch\n\\end{aligned}\n\\]\nTo include this budget constraint into the previous graph, we have to rearrange this equation to have the quantity of apples as a function of the quantity of books:\n$$\n\\[\\begin{aligned}\n600 = 4*apple + 200*watch\n\\\\\n4apple = 600 - 200watch\n\\\\\napple = 600/4 - 200/4*watch\n\\\\\napple = 150 - 50*watch\n\n\\end{aligned}\\]\n$$ More generally, the budget function can also be written as\n\\[\nQ_1 = \\frac{Income}{P_1} - \\frac{P_2}{P_1}Q_2\n\\]\n\nbudget &lt;- function(x) 150 - 50*x\n\nNow we can plot both the indifference curves and the budget constraint:\n\n\n\n\n\n\n\n\n\nThe budget line represents all the combination of apples and watches that the consumer can afford with his income. This implies that is final choice has to be on this line. The consumer cannot afford to be on the U = 90 indifference curve because its income is not large enough. Also, he will not choose any point on U = 10 because the latter’s majority of point are on the left of the budget line (the assumption of more is better than less would not be respected if the consumer for instance chooses 1 watch and 50 apples, because he can afford more of the two goods).\nSo what will be the consumer’s final choice? He will choose the point at which the budget constraint line is tangent to one of his indifference curve.\nMathematically, this means that the quantity of apples and watches the consumer will choose is the point at which the slope of the budget line \\(\\frac{P_{watch}}{P_{apples}}\\) is equal to the slope of the indifference curve, which microeconomics call the marginal rate of substitution. Marginal rate of substitution shows how much of a good (here apples) the consumer can give up in exchange of one unit of the other good (here a watch). Using algebra, we can find the slope of the budget line and of the indifference curve by computing their derivatives.\n\\[\n\\begin{aligned}\napple = 150 - 50*watch\n\\\\\n\\frac{\\partial{apple}}{\\partial{watch}} = -50\n\\\\\n\\end{aligned}\n\\]\nNote that finding the marginal rate of substitution is a bit trickier than for the budget line. To find the MRS, we have to compute the derivative with respect to the first good (x which are apples here) and then for the second one (y the watches). Then, the MRS is the ratio between the two marginal utilities \\(\\frac{\\frac{\\partial{U}}{\\partial{x}}}{\\frac{\\partial{U}}{\\partial{y}}}\\)\n\\[\n\\begin{aligned}\nU = x^20.18y\n\\\\\n\\frac{\\partial{U}}{\\partial{x}} = 0.36xy\n\\\\\n\\frac{\\partial{U}}{\\partial{y}} = 0.18*x^2\n\\end{aligned}\n\\] Thus\n\\[\n\\begin{aligned}\nMRS = \\frac{0.36xy}{0.18x^2} = 2\\frac{y}{x}\n\\end{aligned}\n\\] Here is how to compute this in R\nThen, we set the marginal rate of substitution equal to the slope of the budget line:\n\\[\n\\begin{aligned}\n2\\frac{y}{x} = 50\n\\\\\ny = 25x\n\\end{aligned}\n\\] We can then substitute y with 25x in the budget equation\n$$\n\\[\\begin{aligned}\ny = 150 - 50*x\n\\\\\n25x = 150 -50x\n\\\\\nx = 150/75 = 2\n\\\\\nx = 2\n\n\\end{aligned}\\]\n$$\nThe optimal solution for x (quantity of watch) is thus 2, the consumer will choose 2 watches. Now that know the quantity of watches, we can obtain the quantity of apples as well as how much utility this combination of apples and watches will bring to the consumer.\nTo get the number of apples, we simply replace x by 6 in the budget constraint equation \\(y = 150-50*2 = 50\\), \\(U = 2^2*50*0.18 = 36\\)\nR can check the results\n\nprice_x = 200\nprice_y = 4\nSolve(paste(Simplify(mu_x / mu_y), \"==\", price_x, \"/\", price_y), y)\n\nYacas vector:\n[1] y == -(-9 * x/0.36)\n\n\n\noptimal_x &lt;- uniroot(function(x) budget(x) - marginal_utility(x), c(0, 100))$root\noptimal_y &lt;- budget(optimal_x)\noptimal_u &lt;- utility_u(optimal_x, optimal_y)\n\n\noptimal_x\n\n[1] 2\n\noptimal_y\n\n[1] 50\n\noptimal_u\n\n[1] 36\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeriving the downward slopping demand curve from indifference curve and budget constrain\nThe final step to grasp why micro theory draws downwards slopping demand curve is to see how a change in relative prices \\(\\frac{P_2}{P_1}\\) changes the consumer’s optimal choice for a good (the equilibrium point in the last graph). In our example, the optimal quantity choice of watch for the consumer was 2 (2 watches costing 200 francs each). What happends in the graph above if the price of watch increases?\nThe budget constraint will change. Initially we have\n\\(apple = 150 - 50watch\\)\nAnd now, the price of watch increases up to 400, income is unchanged (Income = 600) as well as the price of apples (4):\n\\[\n\\begin{aligned}\n600 = 4*apple + 400watch\n\\\\\n4apple = 600-400watch\n\\\\\napple = 150 - 100watch\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nAt the new equilibrium level, utility has decreased to 9, apple consumption remains unchanged and the quantity of watches has decreased to one. Note that there are two important mechanisms behind the graph above:\n\nIncome effect Income effect relates to how the demand for a good change when income changes. If the demand increases when income increases, micro talks about positive income effect (and conversely negative income effect). In our example, the consumer’s income has not changed (600 francs), but the real income has decreased, because the price of one the good has increased. The consumer has thus a lower (real) income, which led to a decrease in the quantity demanded for the good whose price increased.\nSubstitution effect Substitution effect refers to how the demand for a good change when the relative price of the good changes.\n\nIncome and substitution effects depend on the type of the good. There are indeed different types of goods, depending on how demand changes when price changes:\n\nNormal good\nA good is normal when demand increases when its price decreases vice and versa\nInferior good\nA good is inferior when the demand decreases as income increases or the price increases\nGiffen good\nA Giffen good refers to goods whose demand increases when price increases. The logic behind this are the goods which are very essential to every day living (for example staple food): an increase in the price of very essential food can lead to the consumer’s decision of reducing the consumption of other goods to still afford consuming the essential good.\n\nThe table below is a good summary of how income and substitution effect:\n\n\n\n\n\nThis is how microeconomics derive the demand curve. We will see below that the supply curve is also derived with the same logic, the steps being almost the same as we saw here, but by replacing the utility function with a production function and replacing the two goods by capital and labour, which are the factors of production of any firm."
  },
  {
    "objectID": "economics.html",
    "href": "economics.html",
    "title": "Economics series",
    "section": "",
    "text": "Neo-Kaleckian Models\n\n\n\nHeterodox Economics\n\n\nKalecki\n\n\nMacroeconomics\n\n\n\n\n\n\n\nCelâl Güney\n\n\nJul 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeo-Keynesian Models\n\n\n\nHeterodox Economics\n\n\nNeo-Keynesian Economics\n\n\nMacroeconomics\n\n\n\n\n\n\n\nCelâl Güney\n\n\nJul 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassical-Marxian Model\n\n\n\nHeterodox Economics\n\n\nMacroeconomics\n\n\n\n\n\n\n\nCelâl Güney\n\n\nJul 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIS-LM Model\n\n\n\nEconomics\n\n\nMacroeconomics\n\n\n\n\n\n\n\nCelâl Güney\n\n\nJun 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMainstream Production Theory\n\n\n\nEconomics\n\n\nMicroeconomics\n\n\n\n\n\n\n\nCelâl Güney\n\n\nJun 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMainstream Consumer Theory\n\n\n\nEconomics\n\n\nMicroeconomics\n\n\n\n\n\n\n\nCelâl Güney\n\n\nJun 26, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Celâl and I am a master’s student in the Political Economy of Capitalism at the University of Geneva. I specialize in macroeconomics, post-Keynesian economics and in data science. I am also interested in econometrics, economic history, the history of economic ideas and theories and in programming, especially with R. My master’s studies are coming to an end this June, when I will defend my master thesis, which is about institutional change in Switzerland. I am currently looking for an internship, a Phd position or a job linked to my studies and interests, so don’t hesitate to have a look to my CV or to my blog posts where I publish stuff related to my studies, readings and research interests.\n\n\n Bachelor in History-Economics-Society (now BA in Political Economy and Economic History) | University of Geneva | 2017-2020\n Complementary Certificate in Applied Statistics | University of Geneva (GSEM) | 2021-2022\n Master in The Political Economy of Capitalism | University of Geneva | 2022 - 2024\n\n\n\nInequality and political cleavages\nInstitutional economics | Evolutionary economics\nPolitical economy and comparative capitalism\nEconomic history | history of economic thought\nData science | statistics & econometrics | history of economic methods and measures\nDemography\n\n\n\n\n\nFrench - Native\nEnglish - Very proficient\nSpanish - Very proficient\nGerman - Good knowledge"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "Bachelor in History-Economics-Society (now BA in Political Economy and Economic History) | University of Geneva | 2017-2020\n Complementary Certificate in Applied Statistics | University of Geneva (GSEM) | 2021-2022\n Master in The Political Economy of Capitalism | University of Geneva | 2022 - 2024\n\n\n\nInequality and political cleavages\nInstitutional economics | Evolutionary economics\nPolitical economy and comparative capitalism\nEconomic history | history of economic thought\nData science | statistics & econometrics | history of economic methods and measures\nDemography\n\n\n\n\n\nFrench - Native\nEnglish - Very proficient\nSpanish - Very proficient\nGerman - Good knowledge"
  },
  {
    "objectID": "economics_series/classical-marxian model/classical-marxian model.html",
    "href": "economics_series/classical-marxian model/classical-marxian model.html",
    "title": "Classical-Marxian Model",
    "section": "",
    "text": "What follows is extensively based on the second chapter of Heterodox Macroeconomics: Models of of Demand, Distribution and Growth (2019) by Robert A. Blecker and Mark Setterfield\n\nAssumptions of the model\nThe classical-marxian model, named after Smith, Ricardo (who are considered as classics) and marx and based on their important work, comes its specific assumptions:\n\nConstant or full capacity rate of utilization.\n\nThe latter, \\(u = \\frac{Y}{Y_{K}}\\), is ratio of the actual output (GDP) divided by the full capacity output \\(Y_K\\), which is the level if output if all capital in the economy was used at its full capacity (no unused machines, raw materials…). If this ratio is equal to one, that means that the economy is at full capacity, the actual output is equal to its capacity level. If the ratio is for example 0.5, that means that actual output is half its level if all capital was fully used. But why does this model make such an assumption? This assumption stands in sharp contrast with the (neo)kaleckian model which typically reverses this assumption stating that capacity rate \\(u\\) is an important adjusting variable.\nBefore explaining the reason behind this assumption, it is necessary to understand the classical-marxian conception of the production process (how inputs, capital and labor, are combined to produce either goods or services), which rests on the Leontieff production function. The latter can be written as\n\\[\nY = min(\\frac{N}{a_0}, \\frac{K}{a_1})\n\\]\nWith N and K respectively labor and capital, \\(a_0 = L/Y\\) the labour-output ratio (the quantity of labour required to produce one unit of ouput) and \\(a_1 = K/Y\\) the quantity of capital required to produce one unit of output (capital-output ratio).\n\n# how to write the leontief production function in R\nleontief &lt;- function(l, k, a0, a1) (min(c(l/a0, k/a1)))\n\n\\(\\frac{N}{a_0}\\) is the maximum output that can be produced with available labour resources while \\(\\frac{K}{a_1}\\) represents maximum output when all capital resources are used.\nFor instance, if three units of labor are required to produce one unit of output and one unit of capital is required to produce one unit of output, what will be produced if we have N = 9 and K = 4 ?\nIn our example, \\(a_0 = 3/1\\) (three units of labor required to produce Y = 1) and \\(a_1 = 1\\) (one unit of capital to produce one unit). We compute \\(N/a_0 = 9/3\\) and \\(K/a_1 = 4/1\\). Since the former \\(9/3 = 3\\) is bigger than \\(4\\), all 9 units of labor will be used to produce 3 units of output. Note that at Y = 3, one unit of capital will be leftover and be unused. In this example, output is “labor-constrained” because it is the amount of labor which is fully used and capital which is not. Output at full employment is \\(Y_N = N/a_0\\).\n\nleontief(l = 9, k = 4, a0 = 3, a1 = 1)\n\n[1] 3\n\n\nConsidering labor as the constraint is what the (neo)kaleckian model does, while the classical-marxian model considers that capital is the binding constraint on potential output. In the classical-marxian model, all capital is used and thus output is output at full capital utilization \\(Y_K = K/a_1\\).\nWhile both classical-marxian and (neo)kaleckian models use Leontief production function (and are here different from neoclassical models which use cobb-douglas production function), they differ by what they consider as the constraining factor on output: labor for (neo)kaleckian model \\(Y_N\\), capital for classical-marxian model \\(Y_k\\).\nThus, considering that capital is the constraint implies the possibility to consider that the economy can reach a full capacity utilization \\(u = Y/Y_k\\), this is what the classical-marxian model does thus why \\(u\\) is considered constant at full capacity \\(u = 1\\).\n\nConstant and Given Technology\n\nThere is no technological change in the classical-marxian model. However, the effects of exogenous technological change can still be analyzed.\n\n\nBasis of the model\n\nWage-profit trade-off\nThe main equation is derived from the income approach to national income, which is an accounting identity showing that national income is the sum of all income sources in the economy. The model makes here other assumptions: no government, closed economy, only workers and capitalists, only one good produced, which are nessecary to write:\n\\[\nPY = WL + rPK\n\\] Which is the national income identity after those assumptions, with \\(P\\) the price, \\(Y\\) the output, \\(W\\) the nominal wage rate, \\(L\\) the amount of labor employed, \\(r\\) the profit rate and \\(K\\) the real stock of capital.\nFrom this equation above is derived one the main equation of the model, the inverse wage-profit relationship:\n\\[\nw = \\frac{1}{a_0}-\\frac{a_1}{a_0}r\n\\]\n\nw &lt;- function(r) 40 -1*r\n\nggplot(data = tibble(x = 0:10), aes(x = x))+\n  geom_function(fun = w)+\n  annotate(geom = \"label\", x = 7.5, y = 40, label = \"w = 1/a0 - (a1/a0)*r\", color = \"red\")+\n  theme_classic()+\n  labs(title = \"Inverse wage-profit relationship\",\n       x = \"profit rate (r)\", y = \"real wage\")\n\n\n\n\n\n\n\n\nThis equation also implies an inverse relationship between consumption and growth\n\\[\nc = \\frac{1}{a_0}-\\frac{a_1}{a_0}g\n\\] With \\(c\\) consumption of both workers and capitalist and \\(g\\) the rate of capital accumulation \\(g = \\Delta{K}/K = I/K\\)\nThese two relationships and hence trade-off between wage and profit, and consumption and growth are the implications of the two main assumptions explained above: constant rate of capacity utilization and given constant technology. Technological change or rise in \\(u\\) can improve the wage-profit trade-off and thus make both profits and wage rate rise (same for consumption-growth trade-off). Graphically, the slope of the curve above would either shift upward or one of the intercept increase.\n\n\nSaving function\nThe model makes here another important assumption: all saving is done by the capitalist class. All savings come out from of the profits received by the capitalists. Another important feature is the fact that the classics and Marx did not distinguish between savings and investment (“the purchase of newly produced capital goods, such as machinery, equipment or structures”(p.63)). In C-M terminology, “accumulation” means the mechanistic flow between savings, investment and thus growth. This leads to the “accumulation function”:\n\\[\ng \\equiv I/K \\equiv S/K = s_r(r-r_{min})\n\\]\n\\(g \\equiv I/K \\equiv S/K\\) (growth \\(g\\) is equal to the investment rate \\(I/K\\) which is the same as saving rate \\(S/K\\)) comes from this non-distinction between saving and investment. \\(s_r\\) is the proportion capitalists save out of their profits \\(r\\). This equation means that growth can increase only with an increase in the saving rate of the capitalists.\n\n\n\nThe three equations and the alternative closures\n\n3 Equations\nThe basic model is thus based on the three main equations\n\nProfit-wage trade-off\n\n\\[\nw = \\frac{1}{a_0}-\\frac{a_1}{a_0}r\n\\]\n\nConsumption-growth trade-off\n\n\\[\nc = \\frac{1}{a_0}-\\frac{a_1}{a_0}g\n\\]\n\nAccumulation function\n\n\\[\ng \\equiv I/K \\equiv S/K = s_r(r-r_{min})\n\\] However, the model is not “closed” in the sense that all the main variables (w, r, s_r, g) are endogenous (they all are a function of one another). The model needs thus to be “closed” by adding an exogenous variable. Since the classics and Marx did not give a precise and consistent closure (exogenous variable), the textbook gives four “alternative closures” which come from different interpretation the classics and Marx.\n\n\n4 alternative closures\n\nAn exogenously given real wage \\(\\bar{w}\\)\n\nWhich represents an ordinary standard of living for a working-class family. This should not be confounded with the infamous “iron law” of wages (that wages will always tend to go to the subsistence level, this was made popular by Ferdinand Lassalle and Malthus but has nothing to do with Marx and the classics). This exogenous wage rate \\(\\bar{w}\\) is given, but socially and historically determined and varies across countries and time. This is not a natural/physical minium subsistence wage.\n\nExogenous given wage share of national income\n\nThis assumes that wages are determined through a bargaining process in which workers can bargain with capitalists to get wages such as a given wage share of national income is achieved. Note that the wage share can be written as \\(\\psi = wa_0 = w*L/Y\\) and the profit share \\(\\pi = ra_1=rK/Y\\). The real wage \\(w\\) can be written as negatively related to the profit share \\(\\pi\\)\n\\[\nw = (1 - \\bar{\\pi})/a_0 = (1 - \\bar{\\pi})Q\n\\] With \\(Q=Y/L\\) labour productivity.\n\nFull employment or constant employment\n\nThis closures explains that wage depends on the balance between labor demand and supply. Increase in labor demand tend to increase wages whereas the growth in labor supply will tend to decrease wages. A rapid growth of labor demand increase the bargaining power of workers, who can bargain higher wages and conversely when population and labor supply increase rapidly.\nThis closure states that the change in real wage \\(\\hat{w}\\) is a negative function of growth in labour supply \\(n\\)\n\\(\\hat{w} = \\phi(g - n), \\phi'&gt;0\\)\nThus, wage is constant when \\(g = n\\)\nBut then, the manual (on page 71) considers another function, which describes the growth of labor force \\(n\\) as a positive function of real rage \\(w\\), the idea is that the higher wage leads to higher population growth and thus higher labor supply growth:\n\\(n = n_0 + n_1w\\)\n\nA given rate of profit, determined by financial market forces\n\nThe profit rate is determined by the interest rate on loans to firms \\(i\\) and by a risk premium \\(\\lambda\\):\n\\(r = i + \\lambda\\)\n\n\n\nEffects of exogenous change\n\nVizualization of the models\n\n\nFirst and second closures\n\n\n\nThe model under first and second closures (exogenous real wage or wage share),Blecker and Setterfield (2019, 69)\n\n\n\n\nThird closure\n\n\n\nThird closure: constant of full employment rate, Blecker and Stterfield (2019, 72)\n\n\nSince the fourth closure is in my opinion not really important, I will mostly focus now of those closures above.\n\n\nEffects of exogenous change in propensity to save\nWhat happens if the saving propensity out of profits, \\(s_r\\), rise or fall?\n\nClosure 1 and 2\nIf the propsensity to save increase, the accumulation function curve becomes flatter (attention: in the graph above, the y and x axis are inverted on the right quadrant, where the accumulation function is drawn. So a rise in the slope implies a flattening of the curve). Since the real wage (or wage change) is exgenously given, it does not change. What changes are the growth (accumulation) rate and consumption: accumulation and growth increase because more profits are saved and invested into new capital. Consumption decreases because since capitalists increase their saving propensity, less of their profits are dedicated to personal consumption. Conversely, if the saving propensity decreases, accumulations and growth decrease and consumption increase. The real wage and profit rate remain unchanged.\n\n\\(\\nearrow s_r \\Rightarrow \\nearrow I/K \\Rightarrow \\nearrow g, \\searrow c\\)\n\nClosure 3 (natural rate of growth closure)\nAn increase in the propensity to save will again make the accumulation curve rotate down to the right (the slope increase). The direct short run effect is a rise in growth rate g. Then, this increase in growth rate makes labor demand increase faster than labor supply, resulting in a rise in the real wage. The rise in real wage decreases the profit rate and thus the growth rate decreases until the growth of labor demand and labor supply are equal. Whereas the growth rate g ends up at the same level or higher depends if labor supply is considered as endogenous or exogenous. If it is endogenous, the increase in labor supply after the increase in real wage will be faster and the new equilibrium growth rate will be higher than the original level. If labor supply is exogenous (a vertical curve in the upper-right candrant), the equilibrium growth rate cannot rise in the long run and will return at its original level.\nEndogenous labor supply: \\(\\nearrow s_r \\Rightarrow \\nearrow \\nearrow g, \\Rightarrow \\nearrow w, \\searrow r \\Rightarrow \\nearrow n \\Rightarrow \\searrow g\\),\nbut with final g &gt; initial g. Note that under these conditions, both the growth rate and the real wage rise.\nExogenous labor supply: \\(\\nearrow s_r \\Rightarrow \\nearrow \\nearrow g \\Rightarrow \\nearrow w, \\searrow r \\Rightarrow \\searrow \\searrow g\\)\n\n\n\nEffects of redistribution of income\n\nClosure 1 and 2\nA rise in real wage or wage share would decrease the profit rate (recall the trade-off between wage and profit), which would decrease the growth rate (less is saved and invested since profits decrease). Consumption rises due to the increase in real wage/wage share\n\n\\(\\nearrow \\bar{w} \\Rightarrow \\searrow r \\Rightarrow \\searrow g \\Rightarrow \\nearrow c\\)\nA decrease in real wage or wage share would have the opposite effects.\n\nClosure 3 natural rate of growth\nIn this closure, a change in real wage or wage share would be the effect of an exognous change in population growth (and thus of labor supply). A rise in real wage would happen if there is a negative exogenous shock to population and labor supply (for instance a brutal epidemic like the black death, which decreased population a lot in the 14th century and made wages rise because of labor supply scarcity). The curve n = n0 + n1*w shifts to the left, real wages rise, profit rate decreases, growth rate decreases and concumption increases.\n\n\\(\\nearrow n_0 \\Rightarrow \\nearrow w, \\nearrow c \\Rightarrow \\searrow r \\Rightarrow \\searrow g\\)\n\n\n\nEffects of technological change\nThe manual presents four types of technological change:\n\nHarrod-neutral: pure labor saving technological change\nLabor productivity rises, but capital productivity remains unchanged\n\n\\(\\nearrow Q = Y/L = 1/a_0\\)\n\nHicks-neutral: factor-saving technological change\nBoth capital and labor productivity rise\n\n\\(\\nearrow Q = Y/L = 1/a_0, \\nearrow Y/K = 1/a_1\\)\n\nMarx-biased: labor saving, capital using\nLabor productivity increases, but capital productivity decreases\nSolow-neutral: pure capital saving technological change\nCapital productivity rises, with labor productivity unchanged\n\nAll those patterns of technological change, with the exception of the marx-biased one, will improve the wage-profit trade-off, the curve of the latter shifting outward. Wage and profit as well as growth rise. Under the natural rate of growth closure (3) with exogenous labor supply, the long-run growth rate does not increase, however.\n\nMarx-biased technological change the falling tendency of the rate of profit (FTRP)\nThe FTRP is perhaps one of the most famous claim/theory of Marx. In Capital Volume III, Marx exposes this theory, which claims that technological change, by increasing labor productivity while decreasing capital productivity, will lead to a fall in the profit rate. The manual claims that Marx makes one important but often forgotten assumption: a constant rate of exploitation \\(e = s/v\\) with s the surplus value and v the value of labor power. Blecker and Setterfield argue that assuming a constant rate of exploitation is the same as assuming a constant wage share and profit share.\nRecall that the profit share can be written as \\(r = \\pi / a_1\\), if \\(\\pi\\) the profit share is constant and marx-biased technological change happens, \\(a_1\\) will increase (\\(a_1\\) is the inverse of capital productivity \\(a_1 = K/Y_k\\)). and thus decrease profit rate \\(r\\). Thus, under the constant wage share closure, Marx-biased technological change does imply a fall in profit rate. However, it is unlikely that capitalists will let their profit rate fall without reacting and trying to suppress wages or slow down accumulation and growth, which would decrease labor demand, reduce workers bargaining power and thus lead to lower wages and to a recovery of the profit rate.\nWhat about the third closure? Under this closure, the fall in profit rate does not happen, mainly because of the effect of the increase in labor supply resulting from the increase in real wage. If labor supply is considered as exogenous, the long run profit and growth rate cannot change: \\(a_0\\) falls more than real wage increase, and thus \\(\\pi = 1-wa_0\\) increases. Thus, the FTRP is false under this closure, but another of Marx prediction is true: the relative immiseration of the proletariat, since the profit share increases (and wage share decreases)."
  },
  {
    "objectID": "economics_series/is-lm model/is-lm model.html",
    "href": "economics_series/is-lm model/is-lm model.html",
    "title": "IS-LM Model",
    "section": "",
    "text": "The main objective of macroeconomics is to describe and explain the relationship between important economic variables such as unemployment, inflation, production and growth or the interest rate. Questions macroeconomics typically tries to answer are “how can long run growth be sustained?” “how can unemployment be reduced?”. Macroeconomics also takes into account economic policy and the role of the government: how can the government reduce unemployment, increase output and maintain low level of inflation?\nThose are all questions most of macro models try to answer. In this blog post, I will explain one of the most famous and widespread macro model, which is the Investment-Savings - Liquidity-Money model (IS-LM model)."
  },
  {
    "objectID": "economics_series/is-lm model/is-lm model.html#intro",
    "href": "economics_series/is-lm model/is-lm model.html#intro",
    "title": "IS-LM Model",
    "section": "",
    "text": "The main objective of macroeconomics is to describe and explain the relationship between important economic variables such as unemployment, inflation, production and growth or the interest rate. Questions macroeconomics typically tries to answer are “how can long run growth be sustained?” “how can unemployment be reduced?”. Macroeconomics also takes into account economic policy and the role of the government: how can the government reduce unemployment, increase output and maintain low level of inflation?\nThose are all questions most of macro models try to answer. In this blog post, I will explain one of the most famous and widespread macro model, which is the Investment-Savings - Liquidity-Money model (IS-LM model)."
  },
  {
    "objectID": "economics_series/is-lm model/is-lm model.html#production-as-gross-national-product-gdp",
    "href": "economics_series/is-lm model/is-lm model.html#production-as-gross-national-product-gdp",
    "title": "IS-LM Model",
    "section": "Production as Gross National Product (GDP)",
    "text": "Production as Gross National Product (GDP)\nThe main feature of the model is a graph, which show the relationship between output and the interest rate. But first, we have to define what is actually output (production) in macro theory.\n\nThe three approach to GDP\nProduction is defined as the sum of value added within an economy (can be a nation, a region, or the world taken as a whole) during a period of time (a year, month, semester, quarter…). There are three equivalent ways to define and compute GDP:\n\nProduction approach\nGDP is equal to the value (measured in price) of all final goods and services sold minus the value of all intermediary inputs used in the production process\n\n\\(GDP \\equiv value\\: output\\: sold - cost\\: intermediary\\: inputs\\)\n\nIncome approach\nGDP is equal to the income of all agents in the economy\n\n\\(GDP \\equiv salaries\\:of\\:workers + profits\\:of\\:capital\\:owners\\)\n\nExpenditure approach\nFinally, and this is the most famous definition of GDP, GDP is equal to the total expenditure in the economy, which can be decomposed into the private expenditures of housholds on goods and services C, private expenditure of firms, investment I, and public expenditure G.\n\n\\(GDP \\equiv C + I + G\\)\nIf we take into account the fact that the economy is trading with the rest of the world, we must include the expenditure on imported goods M (imports) and expenditure from the rest of the world for national goods and services X (exports).\n\\(GDP \\equiv C + I + G + (X-I)\\)\nThose three approaches are equivalent. Take for example the income and expenditure methods: it makes sense that any expenditure someone makes has to be a revenue for someone else.\n\n\nInvestment-savings curve (IS)\nThe objective of the IS curve is to describe the relationship between the interest rate and output, which is determined by demand in the short run.\nThe first step is to model demand and the equilibrium on the goods and services market. This can be done from the expenditure method to GDP, since expenditure is kind of the same way to say demand.\nThe total demand in a (closed) economy is thus\n\\(Y \\equiv GDP \\equiv C + I + G\\)\n\nConsumption C\nBut what determines private consumption C? IS-LM model considers that consumption as a positive function of income \\(Y\\)\n\\(C = C_{0}+C_{1}Y\\)\nWith \\(C_{0}\\) the minimum level of consumption if income is zero, \\(C_{1}\\) the marginal propsentity to consume (the additional consumption resulting from one-unit increase in income Y) and \\(Y\\) the income. Note that income \\(Y\\) is the disposable income, that is, income after tax T: \\(Y_d = Y-T\\). Taxes T can be written as a proportion taken from income \\(T = tY\\) with \\(t\\) the tax rate on income.\nThe consumption function can thus be rewritten:\n\\[\n\\begin{aligned}\nC = C_{0}+C_{1}Y\n\\\\\nC = C_{0} + C_{1}(Y-tY)\n\\\\\nC = C_{0} + C_{1}Y(1-t)\n\\end{aligned}\n\\]\n\n\nInvestment I\nRegarding investment, the latter is considered as a decreasing function of the interest rate i:\n\\(I = I_{0} -iI_{1}\\)\nThe logic behind a negative relationship between investment and the interest rate is the following: the interest rate is the cost of borrowing. The higher the i, the higher it is to finance investment through borrowing. Conversely, at low i borrowing is cheaper, making firms more likely to borrow in order to invest.\n\n\nPublic spending G\nGovernment spending is considered as exogenous in the is-lm model. This means that G is determined by government decisions which is determined outside the model.\n\n\nFinal output equation\nThe formula \\(Y \\equiv GDP \\equiv C + I + G\\) can thus take its final IS form:\n\\(Y = [C_{0} + C_{1}Y(1-t)] + [I_{0} - iI_{1}] + \\bar{G}\\)\nWe can see in this equation that ouput Y is negatively related to the interest rate i: the IS curve as thus a negative linear shape.\nTo grasp the function in an easier way, we will just consider that the IS curve is a linear function of consumption (positive relation between output and consumption), Investment and government spending (also positive relation) and the interest rate (negative relation):\n\\(IS = Y = F[C(Y_{+}, T_{-}), I(i_{-}), G_{+}]\\)\n\n#example of a linear IS function\nIS &lt;- function(A, k, x) A - k*x #with A autonomous spending, k the mutliplier and x the interest rate\n\n\nisplot &lt;- ggplot(data = tibble(x = 0:15), aes(x = x)) +\n  stat_function(fun = IS, args = list(A = 20, k = 1))+\n  stat_function(fun = IS, args = list(A = 25, k = 1), linetype = \"dashed\")+\n  labs(title = \"Investment-Savings curve\",\n       subtitle = \"Effect of an increase in government spending, consumption or investment\",\n       y = \"interest rate\", x = \"Y, output, GDP\")+\n  annotate(geom = \"label\", label = \"IS\", x = 0, y = 18)+\n  annotate(geom = \"segment\", x = 6, y = 15, xend = 7.5, yend = 15,\n           arrow = arrow(ends = \"last\"))\n\nisplot"
  },
  {
    "objectID": "economics_series/neo-keynesian models/Neo-keynesian-models.html",
    "href": "economics_series/neo-keynesian models/Neo-keynesian-models.html",
    "title": "Neo-Keynesian Models",
    "section": "",
    "text": "What follows is extensively based on the third chapter of Heterodox Macroeconomics: Models of of Demand, Distribution and Growth (2019) by Robert A. Blecker and Mark Setterfield.\n“Neo-keynesianism” refers in our context to economic models conceived by neo-keynesian economists, that is, economists who built their model from the important contributions of Keynes. It here refers mainly to :\n\nRoy Forbes Harrod (1900-1978), was a British economist who did his education and then was professor in Oxford and Cambridge, where he met Keynes.\nNicholar Kaldor (1908-1986), born in Budapest, was a British economist (he completed his education in Britain). What will be presented here as the early Kaldorian model (EKM) is based on his early work, in which he attempted to derive the conditions at which the growth rate could grow at a rate consistent with full-employment in the long run.\nJoan Robinson (1903-1983), was also a British economist from Cambridge. She is known for her development of Keynesian theory.\n\n\n\nRoy Forbes Harrod (1900-1978) is an important figure in economics. The British economist is first and foremost known for his formal description of the mechanisms of economic growth, which one of the first attempt to do so in the discipline. As we will see, the conclusion of Harrod’s model is that growth under capitalism is fundamentally unstable.\n\n\nHarrod distiguished three growth rates:\n\nThe actual growth rate\nThe rate of growth which is actually observed in the economy \\(y = \\frac{Y_{t}-Y_{t-1}}{Y_{t}}\\)\nNatural growth rate\nWhich is defined as \\(y_N = q + n\\), with n the rate of growth of labour and q the rate of growth of labor productivity.\n\nThis rate of growth comes from the definition of ouput at the full-employment level: \\(Y_N = \\frac{N}{a_0}\\) with \\(a_0 = \\frac{N}{Y}\\) the labour-output ratio (how much labour is required per unit of output). The rate of change of \\(Y_N\\) is \\(\\widehat{Y_N} = n - \\widehat{a_0}\\). Since \\(a_0\\) is also the inverse of labour productivity \\(\\frac{1}{a_0} = Y/L\\), \\(-\\widehat{a}\\) is equivalent to labour productivity growth \\(\\widehat{Q} = q\\).\nThis natural rate of growth can be interpreted as an upper limit, the maximum rate of growth that can be achieved in the long run at full employment. The limit comes from the fact that production (output level) is limited by labour supply constraint (since \\(Y_N\\) is the maximum level of output at full-employment).\n\nWarranted rate of growth\nWhich is the rate of growth when investment is equal to savings \\(S = I\\).\n\nHow can we found this third rate of growth? We must first define the investment function (how the model thinks investment decisions are made):\n\\[\nI_t = a_1(Y^e_t - Y_{t-1})\n\\]\nWith \\(I_t\\) investment at period \\(t\\), \\(a1\\) the full-capacity capital–output ratio (quantity of capital required to produce any given level of output) \\(a_1 = \\frac{K}{Y_K}\\). This ratio can also be interpreted here as the additional quantity of capital needed to produce any additional output (the “at the margins” interpretation). This equation simply means that if agents at time \\(t-1\\) expect output a the next period \\(t\\) to be higher, \\(Y^e_t &gt; Y_{t-1}\\), then investment at the next period \\(t\\) will be equal this this positive difference multiplied by the full-capacity capital–output ratio.\nWhy multiply by \\(a_1\\) ? If we define the actual quantity of capital which is utilized in the economy as \\(K_u = uK\\), \\(u\\) being the full capacity utilization rate \\(u = Y/Y_k\\), we can rewrite the actual quantity of utilized capital as \\(K_u = uK = \\frac{Y}{Y_k}K = a_1Y\\)\nThe change of the amount of capital actually used in the production process is thus \\(\\Delta{K_u} = a_1\\Delta{Y}\\)\nInvestment basically means that new capital is bought and added to the stock of capital available for production: \\(I = \\Delta{K}\\), so that when investment occurs, capital stock increases by \\(\\Delta K\\). If we consider that expectations are realized, \\(\\Delta{Y} = \\Delta{Y^e}\\), we can write\n\\[\nI = \\Delta K = a_1\\Delta Y = \\Delta K_u = a_1\\Delta{Y}\n\\] Thus, \\(a_1\\) is not arbitrary, but conformed to the known quantity of capital required to expand production through investment.\nWe have thus the investment function:\n\\[\nI_t = a_1(Y^e_t - Y_{t-1})\n\\] The saving function, on the other hand, is equal to:\n\\[\nS_t = sY_t\n\\]\nWith \\(s\\) the propensity to save. This saving function simply means that the total saving is a fixed proportion of total income (production).\nThe last steps to find the warranted rate of growth is to equate the savings and investment function, after making the assumption that expectations are realized:\n\\[\n\\begin{aligned}\nY_t = Y^e_t\n\\\\\nS_t = I_t\n\\\\\ns_tY = a_1(Y_t - Y_{t-1})\n\\\\\n\\frac{s}{a_1} = \\frac{Y_t-Y_{t-1}}{Y_t}\n\\end{aligned}\n\\]\nThe warranted rate of growth is thus\n\\[y_w = \\frac{s}{a_1}\\]\nTo sum up, the three rates of growth are\n\nActual Growth Rate\n\n\\(y = \\frac{Y_t-Y_{t-1}}{Y_t}\\)\n\nNatural Growth Rate\n\n\\(y_n = q + n\\)\n\nWarranted Growth Rate\n\n\\(y_w = \\frac{s}{a_1}\\)\n\n\n\nThe first Harrod problem states that there is no mechanisms that would ensure a persistent or non-accidental equality between the three growth rates, thus the equality\n\\[\ny = \\frac{s}{a_1} = \\bar{q} + \\bar{n}\n\\] is possible, but as the manual puts it, not likely. The reason is that what influences the warranted rate of growth and the natural rate of growth are independent of each other.\n\n\n\nThe second Harrod problem states that the warranted rate of growth \\(y_w = s/a_1\\) is unstable. That means that any deviation of the rate of growth from the warranted rate will be self-reinforcing.\nTo see this, we start with the equality between savings and investment:\n\\[\n\\begin{aligned}\nI = S\n\\\\\nI_t = sY_t\n\\\\\nY_t = \\frac{I_t}{s}\n\\\\\n\\end{aligned}\n\\]\nThen by substituting \\(I_t\\) by the investment function \\(I_t = a_1(Y^e_t - Y_{t-1})\\):\n\\[\nY_t = \\frac{a_1(Y^e_t - Y_{t-1})}{s} = \\frac{Y^e_t - Y_{t-1}}{y_w}\n\\] Note that here \\(a_1/s\\) is the inverse of the warranted rate of growth \\(y_w\\), this is why \\(y_w\\) appears at the denominator. If we divide both sides of the equation above by the expected rate of growth \\(Y^e_t\\), we get:\n\\[\\frac{Y_t}{y^e_t} = \\frac{y^e}{y_w}\\]\n\\(y^e\\) comes from the fact that we divided \\(Y^e_t - Y_{t-1}\\) by \\(Y^e_t\\), which is the rate of growth of expected growth rate. This equation above simply shows that, if expectations are realized (\\(Y_t = Y^e_t\\), this is a condition for the warranted rate of growth), and \\(y^e = y_w\\), then we have an equilibrium between the actual growth rate and warranted growth rate\n\\[y^e = y = y_w\\]\nThe second Harrod problem explains that any deviation of \\(y\\) from \\(y_w\\) will be self-reinforcing. If \\(y&lt;y_w\\), there will be a downwards pressure on \\(y\\), thus a persistent recession. Conversely, if \\(y&gt;y_w\\), there will be an upwards pressure on \\(y\\), hence a self-reinforcing economic expansion.\nThese mechanisms are the results of how the model conceive the change of the expected growth rate \\(y^e_t\\). If at period \\(t\\) the growth rate \\(y_t\\) is greater than the expected growth rate \\(y_t&gt;y^e_t\\), then at period \\(t+1\\) agents will revise their expected growth rate upward \\(y^e_{t+1} &gt; y^e_t\\). This simple behavioral principle can be summarized as:\n\\[y_t &gt; y^e_t \\Rightarrow y^e_{t+1} &gt;y^e_t\\]\n\\[y_t &lt; y^e_t \\Rightarrow y^e_{t+1} &lt;y^e_t\\]\n\n\n\nInstability of the Warranted rate of Growth\n\n\nThe graph above shows that any deviation from the equilibrium point, at which \\(y_w = y = y^e\\) will lead either to permanent boom or bust. Note that the second Harrod problem depends on how expectations are revised: here we consider expectations as adaptive: expectations will be based on what happened at the previous period.\n\n\n\nTwo scenarios can be considered.\nFirst, when \\(y_w&lt;y_N\\), the economy will be prone to boom and bust behavior. If the actual growth rate is greater than the warranted growth rate, the former will be ever-increasing until the economy will “overheat” when the actual growth rate is superior to the natural rate (which is an upper limit above which there will be exhaustion of labor force and a likely wage-inflation spiral).\nSecond, if \\(y_w&gt;y_N\\), the economy is in a situation of chronic depression. Since the actual growth rate cannot be permanently equal to the warranted rate in this situation (because of the upper limit \\(y_N\\)), the actual rate of growth would tend to permanently fall until government policies would likely start to intervene.\nFinally, this model has consequence for economic policy. Increasing the saving propensity \\(s\\) to boost growth is not a good idea if it is not accompanied by a rise in aggregate demand, in which case increased saving means reduced consumption demand and investment demand is not guaranteed to offset the fall in consumption.\n\n\n\n\n\n\nThis model is very close to the Classical-Marxian model I described here. The main difference is the fact that the early Kaldorian model (EKM) includes the saving propensity out of wages \\(s_w\\) whereas the classical-marxian model only includes savings out of profits.\nRecall that the classical-marxian saving function was\n\\(S/K= s_r(r-r_min)\\)\nThe EKM introduces savings out of wages, with a propensity lower than savings out of profits. Total savings is thus a weighted average bewteen savings out of wages and profits:\n\\[S = [(1-\\pi)s_w+\\pi s_r]Y\\]\n\\(\\pi\\) is the profit share, \\((1-\\pi)\\) the wage share, with \\(0&lt;s_w &lt; s_r&lt;1\\).\n\n\n\nThe model makes some assumptions, which are globally similar to the C-M model:\n\nEKM assumes that labor productivity is constant \\(q=0\\), hence the natural rate of growth is simply \\(y_N=n\\).\nLike the Classical-Marxian model, growth rate is the rate of capital accumulation \\(g = \\Delta K/K\\).\nFull or constant capacity rate of utilization \\(u\\)\nConstancy and exogenous capital-output and labor-output ratios \\(a_0,a_1\\)\n\nTo maintain full employment, capital accumulate rate must grow the same as the investment rate and labor supply growth rate:\n\\(g = \\frac{I}{K} = n\\)\n\n\n\nAs usual, we equate \\(S = I\\), to find the equilibrium:\n\\[g = n = \\frac{I}{K} = \\frac{S}{K} = \\frac{[(1-\\pi)s_w+\\pi s_r]Y}{K} = \\frac{(s_r-s_w)\\pi + s_w}{a_1}\\]\nThe equilibrium profit share is thus\n\\[\\pi^* = \\frac{a_1 n - s_w}{s_r - s_w}\\]\nAnd the equilibrium growth rate is\n\\[g^* = n = \\frac{s^*}{a_1}\\]\nA major difference with the C-M model is that instead of considering an exogenous wage share or real wage and deriving the profit rate and growth rate from it, Kaldor set the growth rate equal to its natural rate and derive the profit/wage share and saving propensity necessary to reach this equilibrium. For the EKM to work, wage and profit shares must be flexible, there must be active government stabilization policies (fiscal & monetary).\nEKM has the implications that for the economy to grow faster only with higher inequality.\n\n\n\n\nThe major point of this model is to introduce a separation between investment and savings. All the models we saw until now did not distinguish between savings and investment (as the classical-marxian model).\n\n\nThe main equations of the model are thus a saving function out of profits and a desired investment as a positive function of expected profits.\n\nSaving function\n\n\\[\\sigma = s_rr\\] With \\(\\sigma\\) savings.\n\nDesired investment function\n\n\\[g = f(r^e)\\]\n\\(g\\) being investment. This function is assumed to have positive but diminishing marginal impact of expected profits on desired investment (\\(f'&gt;0, f''&lt;0\\)).\n\n\n\nRobinson “banana diagram”\n\n\nPoint A on the graph above is a stable equilibrium whereas point B is an unstable one. The graph has to be read as follows: at \\(r_0\\) the saving rate is smaller than the (desired) growth rate \\(g_0\\). At \\(g_0\\), the corresponding profit rate is \\(r_1\\), which is above the profit rate related to actual saving rate (\\(r_0\\)). Firms have thus invested in way that will make profits rise and they will thus revise their profits expectations upwards: at \\(r_1\\) they will invest at a level slighlty above \\(g_0\\), which will again push up the profit rate until point A, at which \\(g=\\sigma\\) is reached.\nOn the other hand, if we start at any profit rate below point B, the desired rate of investment will lead to a growth rate that will lead in return to a lower profit rate, firms will revise their profit rate expectations downwards and thus reduce their desired rate of investment, which will then lead to an even lower growth and profit rates and so on. Below point B, this process does not end automatically, there must be for example government stabilization. Above point A, however, the process is the same as described above in this paragraph, but it stabilizes at point A. Note that what happens below point B is strongly similar to the second Harrod problem.\n\n\n\nThe Neo-Robinsonian model combines the model above with the inverse wage-profit relationship we saw in the C-M model. The desired investment rate is simplified to\n\\(g = f_0+f_1r^e\\)\nWith the same saving function \\(\\sigma = s_rr\\) and the inverse profit-wage relationship \\(w =\\frac{1-a_1 r}{a_0}\\)\n\n\n\nNeo-robinsonian model\n\n\nTo find the equilibrium levels of growth, profit and wage rates, we set investment equals to savings \\(\\sigma = g\\) and solve for:\n\\[r^* = \\frac{f_0}{s_r-f_1}\\]\nand\n\\[g^*=\\frac{s_rf_0}{s_r-f_1}\\]\nand\n\\[w^* = \\frac{1}{a_0} - \\frac{a_1}{a_0}(\\frac{f_0}{s_r-f_1})\\]\nNote that in this model the real wage and wage share are flexible, but nominal wage is not.\nIn this model, an increase in the propensity to save leads to lower equilibrium level of growth and profit rates and a higher equilibrium real wage. The mechanism is that an increase in the propensity to save, with an unchanged desired investment curve (which means that saving propensity increases without changing firms’ willingness to invest), reduces aggregate demand through a reduction in consumption demand. With falling demand, profits realized from actual investment expenditures also fall.\n\n\n\nEffect of an increase in saving propensity\n\n\nGraphically, the saving curve rotates down to the right. Why does the real wage increase? Recall that in this model, nominal wage is fixed. A change in real wage must then come from a change in the price level. In the case of higher propensity to save, the latter has depressed aggregate demand and growth, which reduces the price level \\(\\searrow P\\).\nWhat if we consider a shift in the desired investment curve? An exemple would be a rise in \\(f_0\\), which can be interpreted as the level of confidence of firms: the higher their confidence in the economy, the higher they will invest for every level of expected profit rates. A rise in \\(f_0\\) would shift the desired investment curve \\(g = f_0+f_1r\\) to the right, profit rate would rise, growth rate would rise, but real wage would decrease. Real wage decreases because inflation increases as a result of higher aggregate demand and thus excess demand in the goods market. Note that inflation in the Neo-robinsonian model is always “demand-pushed”, that is, led by excess or lower demand in the market for goods and services.\n\n\n\n\nSince the assumption of fixed real wage was considered not very realistic, since it is very likely that workers would bargain for higher wages in the case of higher inflation as described above, Marglin has constructed model which is a mixed between the classical-marxian model and the neo-robinsonian model. Marglin’s contribution was to consider that workers bargain for nominal wages rather than the real wage or wage share.\nMarglin’s synthesis add two functions, which model the change in nominal wage and price level.\n\n\n\\(\\hat{W} = \\Omega (\\bar{w} - w)\\)\nWith \\(\\bar{w}\\) the target real wage for workers and \\(\\Omega\\) a constant describing how fast change in nominal wage adjusts to the difference between the target real wage and the real wage \\((\\bar{w} - w)\\). If the target real wage is greater than the actual real wage, workers are willing and are able to bargain for higher wages.\n\n\n\n\\(\\hat{P} = \\Phi (g^d-g)\\)\nWith\n\\(g^d = f_0 + f_1r\\) (assuming \\(r^e = r\\))\nThe desired investment function, which means that the desired investment rate (\\(g^d\\)) is a positive function of the profit rate (assumed to be equal to the expected rate).\nAnd\n\\(g = s_rr\\)\nThe saving function, which is also a positive function of profit rate.\nThus, \\(\\hat{P} = \\Phi (g^d-g)\\), means that inflation (persistent change in price level: \\(\\hat{P}\\)) is always the result of excess demand in the goods market, excess demand due to the fact that \\(g^d&gt;g\\), demand related to investment (demand for tools, machinery, equipment…).\nThis price level change equation captures an important assumption of the neo-keynesian models that inflation is always “demand-led”, that is, led by excessive demand in the goods market. Another assumption is the constancy of the capital to full capacity output ratio \\(a_1\\) (constant technology).\n\n\n\nThe equilibrium condition of the model, which will determine growth, real wage and profit rate, is the equality between price change and nominal wage change:\n\\[\\widehat{W} = \\widehat{P}\\]\n\n\n\nMarglin’s Classical-Marxian - NeoRobinsonian synthesis\n\n\nAt this equilibrium, the real wage \\(W/P\\) is constant. The other variables behave as such:\n\nIf the target real wage increase \\(\\bar{w}\\), equilibrium profit rate decreases, real wage increases, growth rate decreases and equilibrium inflation increases.\nIf \\(f_0\\), firms’ confidence in the economy, increases, equilibrium inflation increases, equilibrium profit rate and growth rate also increase and equilibrium real wage decreases.\nIf the propensity \\(s_r\\) to save increases, equilibrium profit rate and inflation decrease, but the impact on growth is ambiguous: on the one hand growth is constrained by available savings (and thus if savings increase growth can increase) but on the other hand, growth can also decrease because of the fall in profit rate.\n\nAn important contribution of this model is to make explicit the argument about inflation, which was only implicit in the previous neo-keynesian models (Robinson, Kaldor…). An crucial point is that the model assumes that inflation is always driven by demand (excess demand in the goods market). A critic adressed to this model is that inflation is not only “demand-pull” but can also be “cost-push”, that is, driven positively by an increase in firms’ markups due to an increase in costs.\nAnother critic concerns that inflation caused by the difference between \\(g^d&gt;g\\) is not very realistic because it implies that firms will always want to invest at a rate they cannot achieve."
  },
  {
    "objectID": "economics_series/neo-keynesian models/Neo-keynesian-models.html#roy-harrod-a-model-of-unstable-growth",
    "href": "economics_series/neo-keynesian models/Neo-keynesian-models.html#roy-harrod-a-model-of-unstable-growth",
    "title": "Neo-Keynesian Models",
    "section": "",
    "text": "Roy Forbes Harrod (1900-1978) is an important figure in economics. The British economist is first and foremost known for his formal description of the mechanisms of economic growth, which one of the first attempt to do so in the discipline. As we will see, the conclusion of Harrod’s model is that growth under capitalism is fundamentally unstable.\n\n\nHarrod distiguished three growth rates:\n\nThe actual growth rate\nThe rate of growth which is actually observed in the economy \\(y = \\frac{Y_{t}-Y_{t-1}}{Y_{t}}\\)\nNatural growth rate\nWhich is defined as \\(y_N = q + n\\), with n the rate of growth of labour and q the rate of growth of labor productivity.\n\nThis rate of growth comes from the definition of ouput at the full-employment level: \\(Y_N = \\frac{N}{a_0}\\) with \\(a_0 = \\frac{N}{Y}\\) the labour-output ratio (how much labour is required per unit of output). The rate of change of \\(Y_N\\) is \\(\\widehat{Y_N} = n - \\widehat{a_0}\\). Since \\(a_0\\) is also the inverse of labour productivity \\(\\frac{1}{a_0} = Y/L\\), \\(-\\widehat{a}\\) is equivalent to labour productivity growth \\(\\widehat{Q} = q\\).\nThis natural rate of growth can be interpreted as an upper limit, the maximum rate of growth that can be achieved in the long run at full employment. The limit comes from the fact that production (output level) is limited by labour supply constraint (since \\(Y_N\\) is the maximum level of output at full-employment).\n\nWarranted rate of growth\nWhich is the rate of growth when investment is equal to savings \\(S = I\\).\n\nHow can we found this third rate of growth? We must first define the investment function (how the model thinks investment decisions are made):\n\\[\nI_t = a_1(Y^e_t - Y_{t-1})\n\\]\nWith \\(I_t\\) investment at period \\(t\\), \\(a1\\) the full-capacity capital–output ratio (quantity of capital required to produce any given level of output) \\(a_1 = \\frac{K}{Y_K}\\). This ratio can also be interpreted here as the additional quantity of capital needed to produce any additional output (the “at the margins” interpretation). This equation simply means that if agents at time \\(t-1\\) expect output a the next period \\(t\\) to be higher, \\(Y^e_t &gt; Y_{t-1}\\), then investment at the next period \\(t\\) will be equal this this positive difference multiplied by the full-capacity capital–output ratio.\nWhy multiply by \\(a_1\\) ? If we define the actual quantity of capital which is utilized in the economy as \\(K_u = uK\\), \\(u\\) being the full capacity utilization rate \\(u = Y/Y_k\\), we can rewrite the actual quantity of utilized capital as \\(K_u = uK = \\frac{Y}{Y_k}K = a_1Y\\)\nThe change of the amount of capital actually used in the production process is thus \\(\\Delta{K_u} = a_1\\Delta{Y}\\)\nInvestment basically means that new capital is bought and added to the stock of capital available for production: \\(I = \\Delta{K}\\), so that when investment occurs, capital stock increases by \\(\\Delta K\\). If we consider that expectations are realized, \\(\\Delta{Y} = \\Delta{Y^e}\\), we can write\n\\[\nI = \\Delta K = a_1\\Delta Y = \\Delta K_u = a_1\\Delta{Y}\n\\] Thus, \\(a_1\\) is not arbitrary, but conformed to the known quantity of capital required to expand production through investment.\nWe have thus the investment function:\n\\[\nI_t = a_1(Y^e_t - Y_{t-1})\n\\] The saving function, on the other hand, is equal to:\n\\[\nS_t = sY_t\n\\]\nWith \\(s\\) the propensity to save. This saving function simply means that the total saving is a fixed proportion of total income (production).\nThe last steps to find the warranted rate of growth is to equate the savings and investment function, after making the assumption that expectations are realized:\n\\[\n\\begin{aligned}\nY_t = Y^e_t\n\\\\\nS_t = I_t\n\\\\\ns_tY = a_1(Y_t - Y_{t-1})\n\\\\\n\\frac{s}{a_1} = \\frac{Y_t-Y_{t-1}}{Y_t}\n\\end{aligned}\n\\]\nThe warranted rate of growth is thus\n\\[y_w = \\frac{s}{a_1}\\]\nTo sum up, the three rates of growth are\n\nActual Growth Rate\n\n\\(y = \\frac{Y_t-Y_{t-1}}{Y_t}\\)\n\nNatural Growth Rate\n\n\\(y_n = q + n\\)\n\nWarranted Growth Rate\n\n\\(y_w = \\frac{s}{a_1}\\)\n\n\n\nThe first Harrod problem states that there is no mechanisms that would ensure a persistent or non-accidental equality between the three growth rates, thus the equality\n\\[\ny = \\frac{s}{a_1} = \\bar{q} + \\bar{n}\n\\] is possible, but as the manual puts it, not likely. The reason is that what influences the warranted rate of growth and the natural rate of growth are independent of each other.\n\n\n\nThe second Harrod problem states that the warranted rate of growth \\(y_w = s/a_1\\) is unstable. That means that any deviation of the rate of growth from the warranted rate will be self-reinforcing.\nTo see this, we start with the equality between savings and investment:\n\\[\n\\begin{aligned}\nI = S\n\\\\\nI_t = sY_t\n\\\\\nY_t = \\frac{I_t}{s}\n\\\\\n\\end{aligned}\n\\]\nThen by substituting \\(I_t\\) by the investment function \\(I_t = a_1(Y^e_t - Y_{t-1})\\):\n\\[\nY_t = \\frac{a_1(Y^e_t - Y_{t-1})}{s} = \\frac{Y^e_t - Y_{t-1}}{y_w}\n\\] Note that here \\(a_1/s\\) is the inverse of the warranted rate of growth \\(y_w\\), this is why \\(y_w\\) appears at the denominator. If we divide both sides of the equation above by the expected rate of growth \\(Y^e_t\\), we get:\n\\[\\frac{Y_t}{y^e_t} = \\frac{y^e}{y_w}\\]\n\\(y^e\\) comes from the fact that we divided \\(Y^e_t - Y_{t-1}\\) by \\(Y^e_t\\), which is the rate of growth of expected growth rate. This equation above simply shows that, if expectations are realized (\\(Y_t = Y^e_t\\), this is a condition for the warranted rate of growth), and \\(y^e = y_w\\), then we have an equilibrium between the actual growth rate and warranted growth rate\n\\[y^e = y = y_w\\]\nThe second Harrod problem explains that any deviation of \\(y\\) from \\(y_w\\) will be self-reinforcing. If \\(y&lt;y_w\\), there will be a downwards pressure on \\(y\\), thus a persistent recession. Conversely, if \\(y&gt;y_w\\), there will be an upwards pressure on \\(y\\), hence a self-reinforcing economic expansion.\nThese mechanisms are the results of how the model conceive the change of the expected growth rate \\(y^e_t\\). If at period \\(t\\) the growth rate \\(y_t\\) is greater than the expected growth rate \\(y_t&gt;y^e_t\\), then at period \\(t+1\\) agents will revise their expected growth rate upward \\(y^e_{t+1} &gt; y^e_t\\). This simple behavioral principle can be summarized as:\n\\[y_t &gt; y^e_t \\Rightarrow y^e_{t+1} &gt;y^e_t\\]\n\\[y_t &lt; y^e_t \\Rightarrow y^e_{t+1} &lt;y^e_t\\]\n\n\n\nInstability of the Warranted rate of Growth\n\n\nThe graph above shows that any deviation from the equilibrium point, at which \\(y_w = y = y^e\\) will lead either to permanent boom or bust. Note that the second Harrod problem depends on how expectations are revised: here we consider expectations as adaptive: expectations will be based on what happened at the previous period.\n\n\n\nTwo scenarios can be considered.\nFirst, when \\(y_w&lt;y_N\\), the economy will be prone to boom and bust behavior. If the actual growth rate is greater than the warranted growth rate, the former will be ever-increasing until the economy will “overheat” when the actual growth rate is superior to the natural rate (which is an upper limit above which there will be exhaustion of labor force and a likely wage-inflation spiral).\nSecond, if \\(y_w&gt;y_N\\), the economy is in a situation of chronic depression. Since the actual growth rate cannot be permanently equal to the warranted rate in this situation (because of the upper limit \\(y_N\\)), the actual rate of growth would tend to permanently fall until government policies would likely start to intervene.\nFinally, this model has consequence for economic policy. Increasing the saving propensity \\(s\\) to boost growth is not a good idea if it is not accompanied by a rise in aggregate demand, in which case increased saving means reduced consumption demand and investment demand is not guaranteed to offset the fall in consumption."
  },
  {
    "objectID": "economics_series/neo-keynesian models/Neo-keynesian-models.html#the-early-kaldorian-model-ekm-of-growth-and-distribution",
    "href": "economics_series/neo-keynesian models/Neo-keynesian-models.html#the-early-kaldorian-model-ekm-of-growth-and-distribution",
    "title": "Neo-Keynesian Models",
    "section": "",
    "text": "This model is very close to the Classical-Marxian model I described here. The main difference is the fact that the early Kaldorian model (EKM) includes the saving propensity out of wages \\(s_w\\) whereas the classical-marxian model only includes savings out of profits.\nRecall that the classical-marxian saving function was\n\\(S/K= s_r(r-r_min)\\)\nThe EKM introduces savings out of wages, with a propensity lower than savings out of profits. Total savings is thus a weighted average bewteen savings out of wages and profits:\n\\[S = [(1-\\pi)s_w+\\pi s_r]Y\\]\n\\(\\pi\\) is the profit share, \\((1-\\pi)\\) the wage share, with \\(0&lt;s_w &lt; s_r&lt;1\\).\n\n\n\nThe model makes some assumptions, which are globally similar to the C-M model:\n\nEKM assumes that labor productivity is constant \\(q=0\\), hence the natural rate of growth is simply \\(y_N=n\\).\nLike the Classical-Marxian model, growth rate is the rate of capital accumulation \\(g = \\Delta K/K\\).\nFull or constant capacity rate of utilization \\(u\\)\nConstancy and exogenous capital-output and labor-output ratios \\(a_0,a_1\\)\n\nTo maintain full employment, capital accumulate rate must grow the same as the investment rate and labor supply growth rate:\n\\(g = \\frac{I}{K} = n\\)\n\n\n\nAs usual, we equate \\(S = I\\), to find the equilibrium:\n\\[g = n = \\frac{I}{K} = \\frac{S}{K} = \\frac{[(1-\\pi)s_w+\\pi s_r]Y}{K} = \\frac{(s_r-s_w)\\pi + s_w}{a_1}\\]\nThe equilibrium profit share is thus\n\\[\\pi^* = \\frac{a_1 n - s_w}{s_r - s_w}\\]\nAnd the equilibrium growth rate is\n\\[g^* = n = \\frac{s^*}{a_1}\\]\nA major difference with the C-M model is that instead of considering an exogenous wage share or real wage and deriving the profit rate and growth rate from it, Kaldor set the growth rate equal to its natural rate and derive the profit/wage share and saving propensity necessary to reach this equilibrium. For the EKM to work, wage and profit shares must be flexible, there must be active government stabilization policies (fiscal & monetary).\nEKM has the implications that for the economy to grow faster only with higher inequality."
  },
  {
    "objectID": "economics_series/neo-keynesian models/Neo-keynesian-models.html#neo-robinsonian-model",
    "href": "economics_series/neo-keynesian models/Neo-keynesian-models.html#neo-robinsonian-model",
    "title": "Neo-Keynesian Models",
    "section": "",
    "text": "The major point of this model is to introduce a separation between investment and savings. All the models we saw until now did not distinguish between savings and investment (as the classical-marxian model).\n\n\nThe main equations of the model are thus a saving function out of profits and a desired investment as a positive function of expected profits.\n\nSaving function\n\n\\[\\sigma = s_rr\\] With \\(\\sigma\\) savings.\n\nDesired investment function\n\n\\[g = f(r^e)\\]\n\\(g\\) being investment. This function is assumed to have positive but diminishing marginal impact of expected profits on desired investment (\\(f'&gt;0, f''&lt;0\\)).\n\n\n\nRobinson “banana diagram”\n\n\nPoint A on the graph above is a stable equilibrium whereas point B is an unstable one. The graph has to be read as follows: at \\(r_0\\) the saving rate is smaller than the (desired) growth rate \\(g_0\\). At \\(g_0\\), the corresponding profit rate is \\(r_1\\), which is above the profit rate related to actual saving rate (\\(r_0\\)). Firms have thus invested in way that will make profits rise and they will thus revise their profits expectations upwards: at \\(r_1\\) they will invest at a level slighlty above \\(g_0\\), which will again push up the profit rate until point A, at which \\(g=\\sigma\\) is reached.\nOn the other hand, if we start at any profit rate below point B, the desired rate of investment will lead to a growth rate that will lead in return to a lower profit rate, firms will revise their profit rate expectations downwards and thus reduce their desired rate of investment, which will then lead to an even lower growth and profit rates and so on. Below point B, this process does not end automatically, there must be for example government stabilization. Above point A, however, the process is the same as described above in this paragraph, but it stabilizes at point A. Note that what happens below point B is strongly similar to the second Harrod problem.\n\n\n\nThe Neo-Robinsonian model combines the model above with the inverse wage-profit relationship we saw in the C-M model. The desired investment rate is simplified to\n\\(g = f_0+f_1r^e\\)\nWith the same saving function \\(\\sigma = s_rr\\) and the inverse profit-wage relationship \\(w =\\frac{1-a_1 r}{a_0}\\)\n\n\n\nNeo-robinsonian model\n\n\nTo find the equilibrium levels of growth, profit and wage rates, we set investment equals to savings \\(\\sigma = g\\) and solve for:\n\\[r^* = \\frac{f_0}{s_r-f_1}\\]\nand\n\\[g^*=\\frac{s_rf_0}{s_r-f_1}\\]\nand\n\\[w^* = \\frac{1}{a_0} - \\frac{a_1}{a_0}(\\frac{f_0}{s_r-f_1})\\]\nNote that in this model the real wage and wage share are flexible, but nominal wage is not.\nIn this model, an increase in the propensity to save leads to lower equilibrium level of growth and profit rates and a higher equilibrium real wage. The mechanism is that an increase in the propensity to save, with an unchanged desired investment curve (which means that saving propensity increases without changing firms’ willingness to invest), reduces aggregate demand through a reduction in consumption demand. With falling demand, profits realized from actual investment expenditures also fall.\n\n\n\nEffect of an increase in saving propensity\n\n\nGraphically, the saving curve rotates down to the right. Why does the real wage increase? Recall that in this model, nominal wage is fixed. A change in real wage must then come from a change in the price level. In the case of higher propensity to save, the latter has depressed aggregate demand and growth, which reduces the price level \\(\\searrow P\\).\nWhat if we consider a shift in the desired investment curve? An exemple would be a rise in \\(f_0\\), which can be interpreted as the level of confidence of firms: the higher their confidence in the economy, the higher they will invest for every level of expected profit rates. A rise in \\(f_0\\) would shift the desired investment curve \\(g = f_0+f_1r\\) to the right, profit rate would rise, growth rate would rise, but real wage would decrease. Real wage decreases because inflation increases as a result of higher aggregate demand and thus excess demand in the goods market. Note that inflation in the Neo-robinsonian model is always “demand-pushed”, that is, led by excess or lower demand in the market for goods and services."
  },
  {
    "objectID": "economics_series/neo-keynesian models/Neo-keynesian-models.html#marglins-neo-marxianneo-keynesian-synthesis",
    "href": "economics_series/neo-keynesian models/Neo-keynesian-models.html#marglins-neo-marxianneo-keynesian-synthesis",
    "title": "Neo-Keynesian Models",
    "section": "",
    "text": "Since the assumption of fixed real wage was considered not very realistic, since it is very likely that workers would bargain for higher wages in the case of higher inflation as described above, Marglin has constructed model which is a mixed between the classical-marxian model and the neo-robinsonian model. Marglin’s contribution was to consider that workers bargain for nominal wages rather than the real wage or wage share.\nMarglin’s synthesis add two functions, which model the change in nominal wage and price level.\n\n\n\\(\\hat{W} = \\Omega (\\bar{w} - w)\\)\nWith \\(\\bar{w}\\) the target real wage for workers and \\(\\Omega\\) a constant describing how fast change in nominal wage adjusts to the difference between the target real wage and the real wage \\((\\bar{w} - w)\\). If the target real wage is greater than the actual real wage, workers are willing and are able to bargain for higher wages.\n\n\n\n\\(\\hat{P} = \\Phi (g^d-g)\\)\nWith\n\\(g^d = f_0 + f_1r\\) (assuming \\(r^e = r\\))\nThe desired investment function, which means that the desired investment rate (\\(g^d\\)) is a positive function of the profit rate (assumed to be equal to the expected rate).\nAnd\n\\(g = s_rr\\)\nThe saving function, which is also a positive function of profit rate.\nThus, \\(\\hat{P} = \\Phi (g^d-g)\\), means that inflation (persistent change in price level: \\(\\hat{P}\\)) is always the result of excess demand in the goods market, excess demand due to the fact that \\(g^d&gt;g\\), demand related to investment (demand for tools, machinery, equipment…).\nThis price level change equation captures an important assumption of the neo-keynesian models that inflation is always “demand-led”, that is, led by excessive demand in the goods market. Another assumption is the constancy of the capital to full capacity output ratio \\(a_1\\) (constant technology).\n\n\n\nThe equilibrium condition of the model, which will determine growth, real wage and profit rate, is the equality between price change and nominal wage change:\n\\[\\widehat{W} = \\widehat{P}\\]\n\n\n\nMarglin’s Classical-Marxian - NeoRobinsonian synthesis\n\n\nAt this equilibrium, the real wage \\(W/P\\) is constant. The other variables behave as such:\n\nIf the target real wage increase \\(\\bar{w}\\), equilibrium profit rate decreases, real wage increases, growth rate decreases and equilibrium inflation increases.\nIf \\(f_0\\), firms’ confidence in the economy, increases, equilibrium inflation increases, equilibrium profit rate and growth rate also increase and equilibrium real wage decreases.\nIf the propensity \\(s_r\\) to save increases, equilibrium profit rate and inflation decrease, but the impact on growth is ambiguous: on the one hand growth is constrained by available savings (and thus if savings increase growth can increase) but on the other hand, growth can also decrease because of the fall in profit rate.\n\nAn important contribution of this model is to make explicit the argument about inflation, which was only implicit in the previous neo-keynesian models (Robinson, Kaldor…). An crucial point is that the model assumes that inflation is always driven by demand (excess demand in the goods market). A critic adressed to this model is that inflation is not only “demand-pull” but can also be “cost-push”, that is, driven positively by an increase in firms’ markups due to an increase in costs.\nAnother critic concerns that inflation caused by the difference between \\(g^d&gt;g\\) is not very realistic because it implies that firms will always want to invest at a rate they cannot achieve."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Crises & cycles\n\n\n\nPolitical Economy\n\n\nEconomic History\n\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nEconomic stagnation in Switzerland: discussion of a growing debate\n\n\n\nEconomics\n\n\n\n\n\n\n\nCelâl Güney\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nPolitical Compass: what lies behind political science’s most famous graphs\n\n\n\nPolitical Science\n\n\nStatistics\n\n\n\n\n\n\n\nCelâl Güney\n\n\nJul 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPolitical Conflict and Inequality in Switzerland\n\n\n\nPolitical Economy\n\n\nStatistics\n\n\n\n\n\n\n\nCelâl Güney\n\n\nJul 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping with R\n\n\n\nweb scraping\n\n\ndata science\n\n\n\n\n\n\n\nCelâl Güney\n\n\nJul 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolitical stability and political crises in Switzerland: empirical evidence\n\n\n\nPolitical Economy\n\n\nStats\n\n\n\n\n\n\n\n\n\n\nJun 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIncome, education and voting outcome: a cross-sectional analysis of voting outcome in Switzerland\n\n\n\nPolitical Economy\n\n\nStats\n\n\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom income brackets to income decile\n\n\n\nPolitical Economy\n\n\nStats\n\n\nCoding\n\n\n\n\n\n\n\n\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSugar, Cochineal and power\n\n\n\nEconomic History\n\n\n\n\n\n\n\n\n\n\nDec 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInikori about Slavery and the Industrial Revolution\n\n\n\nEconomic History\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Inikori about Slavery and the Industrial Revolution/Inikori.html",
    "href": "posts/Inikori about Slavery and the Industrial Revolution/Inikori.html",
    "title": "Inikori about Slavery and the Industrial Revolution",
    "section": "",
    "text": "Book Review Africans and the Industrial Revolution in England: A Study in International Trade and Economic Development. By JOSEPH E. INIKORI. Cambridge: Cambridge University Press, 2002. 576 p.\nAfricans and the Industrial Revolution in England written and published in 2002 by Joseph Inikori is certainly one of the major contributions on the role of Africans and international trade in England’s industrialization and long run economic development. Joseph Inikori is an Anglo-Nigerian historian currently professor in the university of Rochester and conducting research in Atlantic World Economic History. He has published several articles on Atlantic slave trade and its impact on England economic development, the culmination of his work and research being Africans and the Industrial Revolution in England which received the Association of African Studies Best Book Prize in 2003, one year after publication. This illustrates the importance of Inikori’s book for African scholars and researchers who are eager to demonstrate the historical importance of Africa and slavery of Africans in the rise of Western Europe (but mostly here England) modernity, economic supremacy and development.\nThe present book has hence established itself as a reference and magnus opus on the causal links between African slavery and the Industrial Revolution in England in the 18th century. It is interesting to note that there are only few economic history books which stress with so much focus and insistence on this causal link between African slaves, Atlantic trade and Industrial Revolution. As Inikori argues in his book, the only major contribution on that matter before him was Eric Williams’ Capitalism and Slavery (1944) which explained that slave trade and production directly contributed to the Industrial Revolution thanks to huge amounts of profits made by British merchants and slavers who then re-invested in the productive sectors which were the basis of the Industrial Revolution. In other words, the accumulation of capital made through African slavery allowed for England’s industrial financial investment at the dawn of the Industrial Revolution. However, Williams’ profit-based argument was heavily criticized, the main critics being about the final use of those slavery-based profits (were they really invested in the Industrial Revolution?); the fact that Britain industrialization required no so much profits and that the funds mainly came from England’s own domestic resources. Inikori is aware of those matters and argues that Williams’ thesis is not so well suited anymore to explain England’s industrialization. He also tries to explain Williams’ focus on profits because of the influence of Keynesianism (in which profits play a great role for investments and growth and Williams wrote and published in the Keynesian Era) but adds that Keynesian theory is valid for already well-developed economies and not so much for pre-industrial England (p. 5-6).\nOne could thus consider Inikori’s book as an attempt to go beyond Williams’ argument and “modernize” the explanations of the links between African slavery, Atlantic commerce and England’s development. To reach this goal, Inikori is heavily influenced by development economics’ conceptual framework. He draws inspirations and insights from prominent development economists such as Albert Otto Hirschman (1915-2012) and Hollis Chenery (1918-1994). The main contribution Inikori draws from development economics is the famous industrialization through import substitution model (ISI), for he argues that Industrial Revolution in England was “the first successful case of import substitution industrialization” (p.10). But what are the links between ISI, African slavery, Atlantic international trade and the Industrial Revolution? Inikori has to prove two mechanisms to support his thesis on the causal links between enslavement and exploitation of Africans, the rise of Atlantic commerce and the Industrial Revolution in England. The first mechanism is the causal link between Atlantic commerce and England’s Industrial Revolution (the former causing the latter). The second is the fact that growing Atlantic international trade and commerce were based on African slave labor in the Americas. We will see that the second mechanism is rather obvious and easily proved and that the first one is the main contentious part of the argument.\nInikori argues that during the Middle Ages onwards (roughly 1086-1660 which is the focus of chapter 2) England developed progressively, from 1086 to 1300 and then from 1475 to 1660, an economy specialized in woolen textile industry and that this industry was mostly export led. This specialization in woolen textile manufactures was England’s first successful ISI accomplished in the 16th century and, along with steady improvements in agriculture, made South England relatively rich. Thus, from 1086 to 1660 England moved from agricultural subsistence production to production for market exchange (p.43). However, market demand reached a limit and England could develop its industry further only through new expanding market demand. Inikori makes an important difference between two types of ISIs: both consist in an economy initially producing primary goods for export which then tries to substitute its imports by import-replacing manufactures thanks to state policies (fist for consumer goods, then for intermediate and capital goods). The distinction between the two models is that the first rely exclusively on internal autonomous forces and the second starts to open at some point to international trade and manufactured exports take a central place. All of Inikori’s argument is to show that England’s success corresponds to the second model (p. 150).\nNew markets for British products were thus found thanks to Atlantic trade and commerce (America’s and Africa’s market demand) which were based on Africans slave labor. After constructing an impressive navy and establishing military dominance in Atlantic Sea in the late 16th century (for example after defeating the Invincible Armada in 1588) and therefore acquiring colonies in America, England obtained access to huge market outlets. However, Inikori does not elaborate further on England’s military dominance of Atlantic Sea and this decisive factor lacks an explanation in the author’s work: why was England in the end militarily superior to the Spanish or the Dutch? The book does not provide a precise answer and Inikori only focuses on shipping in chapter 6 to argue that England’s growing shipping industry was stimulated by rising demand for ships due to overseas trade.\nRegardless, it was the access to new markets in Africa and in America’s colonies which enabled England to further its industrialization through import substitution strategies. Imports of cotton textile from India, for instance, gave impetus to local English entrepreneurs to try to venture into cotton textile manufacturing. Protectionist policies, access to outlets in British American colonies and Africa and raw cotton provided by American colonies where production in plantations were based on Africans slaves labor permitted England to successfully achieve import substitution in the cotton industry which is recognized as one of the most crucial sectors of the Industrial Revolution. The reader could be surprised of the relatively few pages focusing on cotton industry in chapter 9 (about 20 pages: pp.427-451) since the Industrial Revolution mostly began with the cotton industry and its technological improvements, but Inikori is right when he argues that this industry developed under the impetus of access to overseas African and American markets and based on raw cotton provided by slave labor in British American colonies. Nonetheless, I think we could also consider the possibility of counterfactual although it may sound speculative: could have England developed its revolutionary cotton industry without African slave labor? This question is a matter of substitutability degree of labor and imports: one could argue that the specialization of Egypt in raw cotton production in the middle of the 19th shows that imports of raw cotton was perhaps more flexible than what Inikori seems to consider.\n\n\n\nRecently bought slaves in Brazil on their way to the farms of the landowners who bought them c. 1830 (credit: wikipedia)\n\n\nRegardless, Inikori’s methodological work is impressive: it offers vast amounts of quantitative data in table form as well as qualitative data through archival researches of a great varieties of historical reports and records. His quantitative and qualitative archival research and evidence allow the author to provide critical reviews of the existing literature on the subject and even to re-evaluate some estimations for example in chapter 5 in which he discusses the measurement of the importance of slave trading between 1600-1850. However, Inikori’s data analysis seems sometimes futile and can blur his main argument as well as confusing and overwhelming the reader. For instance, the author spends pages of digression discussing in vain population and GDP estimates of Domesday England in chapter 2 or Africa and America population estimates before Atlantic trade’s emergence in chapter 4.\nPerhaps the book’s boldest argument and examination to prove the link between Atlantic trade and England’s Industrial Revolution lies in its regional analysis of pre-industrial England. In fact, the regions which started the Industrial Revolution: Lancashire, Yorkshire and the West Midlands (but mostly Lancashire), were the poorest regions of England at the time with relatively low population, low wages and poor overall wealth. As soon as those regions connected to overseas markets, industry, wages and population grew. Inikori carefully shows that population growth was autonomous in those regions (no migration from agriculturally productive and wealthy South regions), that industry growth was not led by internal demand since there was no national market yet until railroads construction in the 19th century and finally that technological innovations took place in those regions and were thus stimulated by high overseas demand. The only reason left to explain North West England industrial growth is thus the access to overseas markets in America and Africa. This is the central argument of the book and I think the most important, since proving that Atlantic trade was based on Africans’ slave labor is rather indisputable, obvious and largely admitted in the literature.\nIn the end, Africans and the Industrial Revolution in England definitely shows that African slave labor significantly contributed to England’s Industrial Revolution. Notwithstanding, its precise form of contribution, whether it was the decisive and unique causal factor or a factor among many remains a contentious matter. Inikori seems to consider that it was the decisive and unique factor, taking thus, in my opinion, an excessive straightforward position. Economic history phenomenon can rarely be explained through unique factors and when one focuses too much on one possible factor, other important causes can easily be overlooked. Other research exploring the role of colonization of America by England in the Industrial Revolution without taking such exaggerated straightforward perspective can be found for example in Pomeranz’ Great Divergence (2000) which combines multiple factors such as access to overseas markets and lands through colonization of North America (joining thus partly Inikori’s argument) and access to coal to explain England’s Industrial Revolution and which is, I think, a good complement to Inikori’s work. I also believe that Inikori attributes excessive importance on Atlantic trade, discounting the important role of trade with India and China which were also crucial (as we saw, cotton textiles imported by the English East India Company which stimulated ISI strategy in this sector came from India and another example is China’s demand for silver which made America’s silver mines profitable for decades)."
  },
  {
    "objectID": "posts/Techinal notes of political cleavages and inequality/technical note piketty 2021.html",
    "href": "posts/Techinal notes of political cleavages and inequality/technical note piketty 2021.html",
    "title": "From income brackets to income decile",
    "section": "",
    "text": "To build their database on World political cleavages and inequality, Piketty and his team had to use electoral survey data. One problem with these sources is that they collect income data through brackets, without reporting the overall income average of the sample or the average per bracket. This Thus poses the question of how far one can go in terms of statistical analysis with only income brackets as a source of information for income. I will here explore what can be done with such a variable as well as the technical note that Piketty et al. (2021) provides to explain how they computed the vote share for income and education decile, which they claim is one of their main contributions on political cleavages and inequality.\n\n\nThe wpid is based on an impressive dataset compiling electoral survey data of 500 elections since 1948. Since the technical note takes Canada’s 2015 election as an example, I will use the latter here.\n\nrm(list = ls())\nca &lt;- read_dta(\"ca.dta\")\nca2015 &lt;- ca %&gt;% \n  filter(year == 2015)\nrm(ca)\n\nsort(unique(ca2015$inc))\n\n [1]  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20\n\n\nNote that there is already something going on here: in the dataset, the variable income has 19 brackets/categories here whereas it has 18 in the technical note. Furthermore, income brackets “jump” from 8 to 10. I will ignore these issues and still work with this dataset, we just won’t have the same results as in Piketty & al’s example.\nA first step in analyzing such a variable is to compute the frequency, relative frequency and the cumulative frequencies. More precisely, I will construct two tables. On the one hand, I compute the descriptive statistics for the NDP’s voters within each income brackets. On the other hand, I construct a table for the overall distribution of income brackets for the NPD’s voters only. Note that I directly add the proportion of NDP’s voters from the first table to the second one to simplify further calculations:\n\nca2015 %&gt;%\n  group_by(inc) %&gt;% \n  count(votendp) %&gt;% \n  drop_na() %&gt;% \n  mutate(\n    cum.n = cumsum(n),\n    prop = n/sum(n)) %&gt;% ungroup() -&gt; table.income.pervote\n\nca2015 %&gt;% filter(votendp == 1) %&gt;% \n  group_by(inc) %&gt;% \n  count(votendp) %&gt;% \n  drop_na() %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    prop_vote = table.income.pervote$prop[table.income.pervote$votendp == 1],\n    cum.n = cumsum(n),\n    prop = n/sum(n),\n    rangeleft = lag(cumsum(prop), default = 0),\n    cumrelfreqN = cumsum(prop),\n    cumrelfreqInc = cumsum(inc/sum(inc))) -&gt; table.income\n\ntable.income.pervote %&gt;% \n  gt(caption = \"Distribution of the vote for the NDP by income group\") %&gt;% \n  gt_theme_dark()\n\n\n\n\n\nDistribution of the vote for the NDP by income group\n\n\ninc\nvotendp\nn\ncum.n\nprop\n\n\n\n\n1\n0\n114\n114\n0.6826347\n\n\n1\n1\n53\n167\n0.3173653\n\n\n2\n0\n161\n161\n0.7419355\n\n\n2\n1\n56\n217\n0.2580645\n\n\n3\n0\n49\n49\n0.7656250\n\n\n3\n1\n15\n64\n0.2343750\n\n\n4\n0\n129\n129\n0.7865854\n\n\n4\n1\n35\n164\n0.2134146\n\n\n5\n0\n108\n108\n0.6708075\n\n\n5\n1\n53\n161\n0.3291925\n\n\n6\n0\n112\n112\n0.7044025\n\n\n6\n1\n47\n159\n0.2955975\n\n\n7\n0\n114\n114\n0.7215190\n\n\n7\n1\n44\n158\n0.2784810\n\n\n8\n0\n244\n244\n0.7746032\n\n\n8\n1\n71\n315\n0.2253968\n\n\n10\n0\n140\n140\n0.8045977\n\n\n10\n1\n34\n174\n0.1954023\n\n\n11\n0\n114\n114\n0.7651007\n\n\n11\n1\n35\n149\n0.2348993\n\n\n12\n0\n154\n154\n0.7549020\n\n\n12\n1\n50\n204\n0.2450980\n\n\n13\n0\n105\n105\n0.7894737\n\n\n13\n1\n28\n133\n0.2105263\n\n\n14\n0\n208\n208\n0.7878788\n\n\n14\n1\n56\n264\n0.2121212\n\n\n15\n0\n95\n95\n0.8333333\n\n\n15\n1\n19\n114\n0.1666667\n\n\n16\n0\n113\n113\n0.8129496\n\n\n16\n1\n26\n139\n0.1870504\n\n\n17\n0\n171\n171\n0.8300971\n\n\n17\n1\n35\n206\n0.1699029\n\n\n18\n0\n83\n83\n0.7830189\n\n\n18\n1\n23\n106\n0.2169811\n\n\n19\n0\n143\n143\n0.8171429\n\n\n19\n1\n32\n175\n0.1828571\n\n\n20\n0\n145\n145\n0.8734940\n\n\n20\n1\n21\n166\n0.1265060\n\n\n\n\n\n\ntable.income %&gt;% \n  gt(caption =  \"Distribution of income group among NDP's voters\") %&gt;% \n  gt_theme_dark()\n\n\n\n\n\nDistribution of income group among NDP's voters\n\n\ninc\nvotendp\nn\nprop_vote\ncum.n\nprop\nrangeleft\ncumrelfreqN\ncumrelfreqInc\n\n\n\n\n1\n1\n53\n0.3173653\n53\n0.07230559\n0.00000000\n0.07230559\n0.004975124\n\n\n2\n1\n56\n0.2580645\n109\n0.07639836\n0.07230559\n0.14870396\n0.014925373\n\n\n3\n1\n15\n0.2343750\n124\n0.02046385\n0.14870396\n0.16916780\n0.029850746\n\n\n4\n1\n35\n0.2134146\n159\n0.04774898\n0.16916780\n0.21691678\n0.049751244\n\n\n5\n1\n53\n0.3291925\n212\n0.07230559\n0.21691678\n0.28922237\n0.074626866\n\n\n6\n1\n47\n0.2955975\n259\n0.06412005\n0.28922237\n0.35334243\n0.104477612\n\n\n7\n1\n44\n0.2784810\n303\n0.06002729\n0.35334243\n0.41336971\n0.139303483\n\n\n8\n1\n71\n0.2253968\n374\n0.09686221\n0.41336971\n0.51023192\n0.179104478\n\n\n10\n1\n34\n0.1954023\n408\n0.04638472\n0.51023192\n0.55661664\n0.228855721\n\n\n11\n1\n35\n0.2348993\n443\n0.04774898\n0.55661664\n0.60436562\n0.283582090\n\n\n12\n1\n50\n0.2450980\n493\n0.06821282\n0.60436562\n0.67257844\n0.343283582\n\n\n13\n1\n28\n0.2105263\n521\n0.03819918\n0.67257844\n0.71077763\n0.407960199\n\n\n14\n1\n56\n0.2121212\n577\n0.07639836\n0.71077763\n0.78717599\n0.477611940\n\n\n15\n1\n19\n0.1666667\n596\n0.02592087\n0.78717599\n0.81309686\n0.552238806\n\n\n16\n1\n26\n0.1870504\n622\n0.03547067\n0.81309686\n0.84856753\n0.631840796\n\n\n17\n1\n35\n0.1699029\n657\n0.04774898\n0.84856753\n0.89631651\n0.716417910\n\n\n18\n1\n23\n0.2169811\n680\n0.03137790\n0.89631651\n0.92769441\n0.805970149\n\n\n19\n1\n32\n0.1828571\n712\n0.04365621\n0.92769441\n0.97135061\n0.900497512\n\n\n20\n1\n21\n0.1265060\n733\n0.02864939\n0.97135061\n1.00000000\n1.000000000\n\n\n\n\n\n\n\nWe can then plot income groups against their proportion\n\ntable.income.pervote %&gt;% filter(votendp == 1) %&gt;% \n  ggplot()+\n  aes(x = factor(inc), y = prop)+\n  geom_col()+\n  theme_bw()+\n  xlab(\"Income group\") -&gt; income.group.plot\n\ntable.income %&gt;% \n  ggplot()+\n  aes(x = factor(inc), y = prop) %&gt;% \n  geom_col()+\n  theme_bw()+\n  ylab(\"\")+\n  xlab(\"\") -&gt; income.group.plot2\n  \n\ncowplot::plot_grid(income.group.plot, income.group.plot2)\n\n\n\n\nDistribution of Income groups: Canadian 2015 election survey\n\n\n\n\nOn the left, we have a graph very similar to the one of the technical note. The right-sided graph is different, because the proportion are for the overall NPD’s voters whereas the left-sided graph represents the proportion within the income group. For example, 30% of income bracket 1 voted for the NDP, but they represent about 7.4% of total NDP’s voters.\nWhat makes Piketty’s team approach special and interesting is their systematic analysis in terms of quantile groups. This is, according to them, their main contribution and this approach has the advantage to allow for systematic comparison accross space and time. We will try to reproduce here their conversion of income group into quantiles.\nIn R, the decile for each observation can be added to the dataset with the function ntile():\n\nca2015 &lt;- ca2015 %&gt;% \n  mutate(\n  decile = ntile(inc, 10)\n)\n\nNow, the last column of ca2015 is the decile for each observation in the dataset.\n\nca2015 %&gt;% \n  group_by(decile) %&gt;% \n  count(votendp) %&gt;% \n  filter(votendp == 1) %&gt;% \n  drop_na() %&gt;% \n  ungroup() %&gt;% \n  mutate(prop = n/sum(n),\n         cumsumlag = (lag(cumsum(prop))),\n         cumsum = cumsum(prop)) -&gt; table.income.vote\n\ntable.income.vote %&gt;% \n  gt(caption = \"Decile and income bracket\") %&gt;% \n  gt_theme_dark()\n\n\n\n\n\nDecile and income bracket\n\n\ndecile\nvotendp\nn\nprop\ncumsumlag\ncumsum\n\n\n\n\n1\n1\n76\n0.10368349\nNA\n0.1036835\n\n\n2\n1\n70\n0.09549795\n0.1036835\n0.1991814\n\n\n3\n1\n106\n0.14461119\n0.1991814\n0.3437926\n\n\n4\n1\n81\n0.11050477\n0.3437926\n0.4542974\n\n\n5\n1\n57\n0.07776262\n0.4542974\n0.5320600\n\n\n6\n1\n88\n0.12005457\n0.5320600\n0.6521146\n\n\n7\n1\n70\n0.09549795\n0.6521146\n0.7476126\n\n\n8\n1\n67\n0.09140518\n0.7476126\n0.8390177\n\n\n9\n1\n63\n0.08594816\n0.8390177\n0.9249659\n\n\n10\n1\n55\n0.07503411\n0.9249659\n1.0000000\n\n\n\n\n\n\n\nHowever, it is straightforward to see that the ntile() function has flaws in decile computing.\nMore generally, computing income decile when the income variable is in brackets seems complicated, but the technical note proposes a re-weighting average approach to partially solve this problem.\nTo see how the re-weighing approach works, let’s go back to the first table:\n\ntable.income %&gt;%\n  gt() %&gt;% \n  gt_theme_dark()\n\n\n\n\n\n\n\ninc\nvotendp\nn\nprop_vote\ncum.n\nprop\nrangeleft\ncumrelfreqN\ncumrelfreqInc\n\n\n\n\n1\n1\n53\n0.3173653\n53\n0.07230559\n0.00000000\n0.07230559\n0.004975124\n\n\n2\n1\n56\n0.2580645\n109\n0.07639836\n0.07230559\n0.14870396\n0.014925373\n\n\n3\n1\n15\n0.2343750\n124\n0.02046385\n0.14870396\n0.16916780\n0.029850746\n\n\n4\n1\n35\n0.2134146\n159\n0.04774898\n0.16916780\n0.21691678\n0.049751244\n\n\n5\n1\n53\n0.3291925\n212\n0.07230559\n0.21691678\n0.28922237\n0.074626866\n\n\n6\n1\n47\n0.2955975\n259\n0.06412005\n0.28922237\n0.35334243\n0.104477612\n\n\n7\n1\n44\n0.2784810\n303\n0.06002729\n0.35334243\n0.41336971\n0.139303483\n\n\n8\n1\n71\n0.2253968\n374\n0.09686221\n0.41336971\n0.51023192\n0.179104478\n\n\n10\n1\n34\n0.1954023\n408\n0.04638472\n0.51023192\n0.55661664\n0.228855721\n\n\n11\n1\n35\n0.2348993\n443\n0.04774898\n0.55661664\n0.60436562\n0.283582090\n\n\n12\n1\n50\n0.2450980\n493\n0.06821282\n0.60436562\n0.67257844\n0.343283582\n\n\n13\n1\n28\n0.2105263\n521\n0.03819918\n0.67257844\n0.71077763\n0.407960199\n\n\n14\n1\n56\n0.2121212\n577\n0.07639836\n0.71077763\n0.78717599\n0.477611940\n\n\n15\n1\n19\n0.1666667\n596\n0.02592087\n0.78717599\n0.81309686\n0.552238806\n\n\n16\n1\n26\n0.1870504\n622\n0.03547067\n0.81309686\n0.84856753\n0.631840796\n\n\n17\n1\n35\n0.1699029\n657\n0.04774898\n0.84856753\n0.89631651\n0.716417910\n\n\n18\n1\n23\n0.2169811\n680\n0.03137790\n0.89631651\n0.92769441\n0.805970149\n\n\n19\n1\n32\n0.1828571\n712\n0.04365621\n0.92769441\n0.97135061\n0.900497512\n\n\n20\n1\n21\n0.1265060\n733\n0.02864939\n0.97135061\n1.00000000\n1.000000000\n\n\n\n\n\n\n\nWe can directly see the problem posed by income brackets: for example, we can see that all of income bracket one belongs to the first decile since its relative range is between 0 and 0.0723. However, the relative range of bracket two is [0.0723 - 0.1487]. Some part of it belong to the first decile ([0 - 0.1]), but some belong to the second ([0 - 0.2]). The approach to compute the proportion of observations belonging to the any given decile is to compute the share of each income bracket belonging to this decile and then compute a weighted average. For example, if I want to compute the share of observation of the first decile (D10), I already know that 100% of income bracket one belongs to D10 but I need to know the share of bracket 2 (B2) belonging to D10.\nTo estimate this, let’s assume the distribution of B2 is uniform \\(x \\sim U[0.0723; 0.1487]\\), with x the observation within this range. We want to know \\(P(x&lt;0.1)\\), that is to say, the probability that x belongs to the first decile. We use the uniform cumulative distribution function with parameters min = 0.073 and max = 0.1487: \\(P(x&lt;0.1) = \\frac{0.1-0.0723}{0.1487-0.0723} = 0.3626\\). This means that 36.26% of B2 belongs to D10. Then, the weighted average for the proportion of observation within D10: \\(\\frac{1*0.317+0.3626*0.26}{1+0.3626} = 0.3018\\). 30.2% the first decile voters voted for the NDP in 2015.\nHere are the computations in R:\n\npunif(0.1, table.income$cumrelfreqN[1], table.income$cumrelfreqN[2])\n\n[1] 0.3625\n\n(1*0.317+0.3626*0.26)/(1+0.3626)\n\n[1] 0.3018318\n\nweighted.mean(x = c(table.income$prop_vote[1], table.income$prop_vote[2]), w = c(1, punif(0.1, table.income$cumrelfreqN[1], table.income$cumrelfreqN[2])))\n\n[1] 0.301588\n\n\nUnfortunately, there is to my knowledge no function in R that will compute the weights automatically. I can nonetheless compute them through a tedious for loop:\n\nweight &lt;- rep(NA, length(table.income$inc))\n\nfor (i in 1:length(table.income$inc)) {\n  weight[i] &lt;- ifelse(table.income$cumrelfreqN[i] &lt; 0.1 | table.income$cumrelfreqN[i] == 1, 1,\n                  ifelse(table.income$cumrelfreqN[i] &gt; 0.1 & table.income$cumrelfreqN[i] &lt; 0.2, punif(0.1, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]),\n                         ifelse(table.income$cumrelfreqN[i] &gt; 0.2 & table.income$cumrelfreqN[i] &lt; 0.3, punif(0.2, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]),\n                                ifelse(table.income$cumrelfreqN[i] &gt; 0.3 & table.income$cumrelfreqN[i] &lt; 0.4, punif(0.3, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]),\n                                       ifelse(table.income$cumrelfreqN[i] &gt; 0.4 & table.income$cumrelfreqN[i] &lt; 0.5, punif(0.4, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]), \n                                              ifelse(table.income$cumrelfreqN[i] &gt; 0.5 & table.income$cumrelfreqN[i] &lt; 0.6, punif(0.5, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]), \n                                                     ifelse(table.income$cumrelfreqN[i] &gt; 0.6 & table.income$cumrelfreqN[i] &lt; 0.7, punif(0.6, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]),\n                                                            ifelse(table.income$cumrelfreqN[i] &gt; 0.7 & table.income$cumrelfreqN[i] &lt; 0.8, punif(0.7, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]),\n                                                                   ifelse(table.income$cumrelfreqN[i] &gt; 0.8 & table.income$cumrelfreqN[i] &lt; 0.9, punif(0.8, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]), \n                                                                          ifelse(table.income$cumrelfreqN[i] &gt; 0.9 & table.income$cumrelfreqN[i] &lt; 1, punif(0.9, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]), 1))))))))))\n}\nweight\n\n [1] 1.0000000 0.3625000 0.0000000 0.6457143 0.0000000 0.1680851 0.7772727\n [8] 0.8943662 0.0000000 0.9085714 0.0000000 0.7178571 0.0000000 0.4947368\n[15] 0.0000000 0.0000000 0.1173913 0.0000000 1.0000000\n\n\nI replace the 0 with 1:\n\nweight &lt;- ifelse(weight == 0, 1, weight)\n\ntable.income &lt;- table.income %&gt;% \n  mutate(share_decile = weight)\n\ntable.income %&gt;% \n  mutate(share_rest = 1 - weight) -&gt; table.income\n\ntable.income %&gt;% mutate(\n  prop_vote = table.income.pervote$prop[table.income.pervote$votendp == 1]\n) %&gt;% gt() %&gt;% \n  gt_theme_dark()\n\n\n\n\n\n\n\ninc\nvotendp\nn\nprop_vote\ncum.n\nprop\nrangeleft\ncumrelfreqN\ncumrelfreqInc\nshare_decile\nshare_rest\n\n\n\n\n1\n1\n53\n0.3173653\n53\n0.07230559\n0.00000000\n0.07230559\n0.004975124\n1.0000000\n0.00000000\n\n\n2\n1\n56\n0.2580645\n109\n0.07639836\n0.07230559\n0.14870396\n0.014925373\n0.3625000\n0.63750000\n\n\n3\n1\n15\n0.2343750\n124\n0.02046385\n0.14870396\n0.16916780\n0.029850746\n1.0000000\n0.00000000\n\n\n4\n1\n35\n0.2134146\n159\n0.04774898\n0.16916780\n0.21691678\n0.049751244\n0.6457143\n0.35428571\n\n\n5\n1\n53\n0.3291925\n212\n0.07230559\n0.21691678\n0.28922237\n0.074626866\n1.0000000\n0.00000000\n\n\n6\n1\n47\n0.2955975\n259\n0.06412005\n0.28922237\n0.35334243\n0.104477612\n0.1680851\n0.83191489\n\n\n7\n1\n44\n0.2784810\n303\n0.06002729\n0.35334243\n0.41336971\n0.139303483\n0.7772727\n0.22272727\n\n\n8\n1\n71\n0.2253968\n374\n0.09686221\n0.41336971\n0.51023192\n0.179104478\n0.8943662\n0.10563380\n\n\n10\n1\n34\n0.1954023\n408\n0.04638472\n0.51023192\n0.55661664\n0.228855721\n1.0000000\n0.00000000\n\n\n11\n1\n35\n0.2348993\n443\n0.04774898\n0.55661664\n0.60436562\n0.283582090\n0.9085714\n0.09142857\n\n\n12\n1\n50\n0.2450980\n493\n0.06821282\n0.60436562\n0.67257844\n0.343283582\n1.0000000\n0.00000000\n\n\n13\n1\n28\n0.2105263\n521\n0.03819918\n0.67257844\n0.71077763\n0.407960199\n0.7178571\n0.28214286\n\n\n14\n1\n56\n0.2121212\n577\n0.07639836\n0.71077763\n0.78717599\n0.477611940\n1.0000000\n0.00000000\n\n\n15\n1\n19\n0.1666667\n596\n0.02592087\n0.78717599\n0.81309686\n0.552238806\n0.4947368\n0.50526316\n\n\n16\n1\n26\n0.1870504\n622\n0.03547067\n0.81309686\n0.84856753\n0.631840796\n1.0000000\n0.00000000\n\n\n17\n1\n35\n0.1699029\n657\n0.04774898\n0.84856753\n0.89631651\n0.716417910\n1.0000000\n0.00000000\n\n\n18\n1\n23\n0.2169811\n680\n0.03137790\n0.89631651\n0.92769441\n0.805970149\n0.1173913\n0.88260870\n\n\n19\n1\n32\n0.1828571\n712\n0.04365621\n0.92769441\n0.97135061\n0.900497512\n1.0000000\n0.00000000\n\n\n20\n1\n21\n0.1265060\n733\n0.02864939\n0.97135061\n1.00000000\n1.000000000\n1.0000000\n0.00000000\n\n\n\n\n\n\n\nThere can be mistakes, but the results seem to make sense\nSince I do not want to ever do this computation again (😅), I put all of this into a function:\n\nweight_share &lt;- function(x){\n  weight &lt;- rep(NA, length(x))\n  \n  for (i in 1:length(x)) {\n   weight[i] &lt;-  ifelse(x[i] &lt; 0.1 | x[i] == 1, 1,\n                  ifelse(x[i] &gt; 0.1 & x[i] &lt; 0.2, punif(0.1, min = x[i-1], max = x[i]),\n                         ifelse(x[i] &gt; 0.2 & x[i] &lt; 0.3, punif(0.2, min = x[i-1], max = x[i]),\n                                ifelse(x[i] &gt; 0.3 & x[i] &lt; 0.4, punif(0.3, min = x[i-1], max = x[i]),\n                                       ifelse(x[i] &gt; 0.4 & x[i] &lt; 0.5, punif(0.4, min = x[i-1], max = x[i]), \n                                              ifelse(x[i] &gt; 0.5 & x[i] &lt; 0.6, punif(0.5, min = x[i-1], max = x[i]), \n                                                     ifelse(x[i] &gt; 0.6 & x[i] &lt; 0.7, punif(0.6, min = x[i-1], max = x[i]),\n                                                            ifelse(x[i] &gt; 0.7 & x[i] &lt; 0.8, punif(0.7, min = x[i-1], max = x[i]),\n                                                                   ifelse(x[i] &gt; 0.8 & x[i] &lt; 0.9, punif(0.8, min = x[i-1], max = x[i]),\n                                                                                                         ifelse(x[i] &gt; 0.9 & x[i] &lt; 1, punif(0.9, min = x[i-1], max = x[i]), 1))))))))))\n  }\n  weight &lt;- ifelse(weight == 0, 1, weight)\n  print(weight)\n}\n\nLet’s check if the function works\n\nweight_share(x = table.income$cumrelfreqN)\n\n [1] 1.0000000 0.3625000 1.0000000 0.6457143 1.0000000 0.1680851 0.7772727\n [8] 0.8943662 1.0000000 0.9085714 1.0000000 0.7178571 1.0000000 0.4947368\n[15] 1.0000000 1.0000000 0.1173913 1.0000000 1.0000000\n\n\nWe are almost done, there are only the weighted averages for each decile left to compute. One further step is to compute dummy variables to show to which decile income brackets belong to. This will produce a table close to the one from the technical note.\n\ntable.income %&gt;% \n  mutate(d1 = ifelse(table.income$rangeleft &gt;= 0 & table.income$rangeleft &lt; 0.1, 1, 0),\n         d2 = ifelse(table.income$rangeleft %[]% c(0.1, 0.2) | table.income$cumrelfreqN %[]% c(0.1, 0.2), 1, 0),  # the %[]% is an \"between\" operator from the Desctools package. for example, x %[]% c(a, b) checks whether x belong to the interval [a, b] with a&lt;b \n         d3 = ifelse(table.income$rangeleft %[]% c(0.2, 0.3) | table.income$cumrelfreqN %[]% c(0.2, 0.3), 1, 0),\n         d4 = ifelse(table.income$rangeleft %[]% c(0.3, 0.4) | table.income$cumrelfreqN %[]% c(0.3, 0.4), 1, 0),\n         d5 = ifelse(table.income$rangeleft %[]% c(0.4, 0.5) | table.income$cumrelfreqN %[]% c(0.4, 0.5), 1, 0),\n         d6 = ifelse(table.income$rangeleft %[]% c(0.5, 0.6) | table.income$cumrelfreqN %[]% c(0.5, 0.6), 1, 0),\n         d7 = ifelse(table.income$rangeleft %[]% c(0.6, 0.7) | table.income$cumrelfreqN %[]% c(0.6, 0.7), 1, 0),\n         d8 = ifelse(table.income$rangeleft %[]% c(0.7, 0.8) | table.income$cumrelfreqN %[]% c(0.7, 0.8), 1, 0),\n         d9 = ifelse(table.income$rangeleft %[]% c(0.8, 0.9) | table.income$cumrelfreqN %[]% c(0.8, 0.9), 1, 0),\n         d10 = ifelse(table.income$rangeleft %[]% c(0.9, 1) | table.income$cumrelfreqN %[]% c(0.9, 1), 1, 0)) -&gt; table.income\n\ntable.income %&gt;% \n  gt() %&gt;% \n  gt_theme_dark()\n\n\n\n\n\n\n\ninc\nvotendp\nn\nprop_vote\ncum.n\nprop\nrangeleft\ncumrelfreqN\ncumrelfreqInc\nshare_decile\nshare_rest\nd1\nd2\nd3\nd4\nd5\nd6\nd7\nd8\nd9\nd10\n\n\n\n\n1\n1\n53\n0.3173653\n53\n0.07230559\n0.00000000\n0.07230559\n0.004975124\n1.0000000\n0.00000000\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n56\n0.2580645\n109\n0.07639836\n0.07230559\n0.14870396\n0.014925373\n0.3625000\n0.63750000\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n1\n15\n0.2343750\n124\n0.02046385\n0.14870396\n0.16916780\n0.029850746\n1.0000000\n0.00000000\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n1\n35\n0.2134146\n159\n0.04774898\n0.16916780\n0.21691678\n0.049751244\n0.6457143\n0.35428571\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n5\n1\n53\n0.3291925\n212\n0.07230559\n0.21691678\n0.28922237\n0.074626866\n1.0000000\n0.00000000\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n6\n1\n47\n0.2955975\n259\n0.06412005\n0.28922237\n0.35334243\n0.104477612\n0.1680851\n0.83191489\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n7\n1\n44\n0.2784810\n303\n0.06002729\n0.35334243\n0.41336971\n0.139303483\n0.7772727\n0.22272727\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n\n\n8\n1\n71\n0.2253968\n374\n0.09686221\n0.41336971\n0.51023192\n0.179104478\n0.8943662\n0.10563380\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n10\n1\n34\n0.1954023\n408\n0.04638472\n0.51023192\n0.55661664\n0.228855721\n1.0000000\n0.00000000\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n11\n1\n35\n0.2348993\n443\n0.04774898\n0.55661664\n0.60436562\n0.283582090\n0.9085714\n0.09142857\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n\n\n12\n1\n50\n0.2450980\n493\n0.06821282\n0.60436562\n0.67257844\n0.343283582\n1.0000000\n0.00000000\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n13\n1\n28\n0.2105263\n521\n0.03819918\n0.67257844\n0.71077763\n0.407960199\n0.7178571\n0.28214286\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n\n\n14\n1\n56\n0.2121212\n577\n0.07639836\n0.71077763\n0.78717599\n0.477611940\n1.0000000\n0.00000000\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n15\n1\n19\n0.1666667\n596\n0.02592087\n0.78717599\n0.81309686\n0.552238806\n0.4947368\n0.50526316\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n16\n1\n26\n0.1870504\n622\n0.03547067\n0.81309686\n0.84856753\n0.631840796\n1.0000000\n0.00000000\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n17\n1\n35\n0.1699029\n657\n0.04774898\n0.84856753\n0.89631651\n0.716417910\n1.0000000\n0.00000000\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n18\n1\n23\n0.2169811\n680\n0.03137790\n0.89631651\n0.92769441\n0.805970149\n0.1173913\n0.88260870\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n19\n1\n32\n0.1828571\n712\n0.04365621\n0.92769441\n0.97135061\n0.900497512\n1.0000000\n0.00000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n20\n1\n21\n0.1265060\n733\n0.02864939\n0.97135061\n1.00000000\n1.000000000\n1.0000000\n0.00000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\nTu put the code above into a function:\n\ndecile_dummies &lt;- function(data, rangeleft, rangeright){\n\ndata %&gt;% \n  mutate(d1 = ifelse({{rangeleft}} &gt;= 0 & {{rangeleft}} &lt; 0.1, 1, 0),\n         d2 = ifelse({{rangeleft}} %[]% c(0.1, 0.2) | {{rangeright}} %[]% c(0.1, 0.2), 1, 0),  #%[]% is an within bracket operator from the Desctools package. for example, x %[]% c(a, b) checks whether x belong to the interval [a, b] with a&lt;b \n         d3 = ifelse({{rangeleft}} %[]% c(0.2, 0.3) | {{rangeright}} %[]% c(0.2, 0.3), 1, 0),\n         d4 = ifelse({{rangeleft}} %[]% c(0.3, 0.4) | {{rangeright}} %[]% c(0.3, 0.4), 1, 0),\n         d5 = ifelse({{rangeleft}} %[]% c(0.4, 0.5) | {{rangeright}} %[]% c(0.4, 0.5), 1, 0),\n         d6 = ifelse({{rangeleft}} %[]% c(0.5, 0.6) | {{rangeright}} %[]% c(0.5, 0.6), 1, 0),\n         d7 = ifelse({{rangeleft}} %[]% c(0.6, 0.7) | {{rangeright}} %[]% c(0.6, 0.7), 1, 0),\n         d8 = ifelse({{rangeleft}} %[]% c(0.7, 0.8) | {{rangeright}} %[]% c(0.7, 0.8), 1, 0),\n         d9 = ifelse({{rangeleft}} %[]% c(0.8, 0.9) | {{rangeright}} %[]% c(0.8, 0.9), 1, 0),\n         d10 = ifelse({{rangeleft}} %[]% c(0.9, 1) | {{rangeright}} %[]% c(0.9, 1), 1, 0))\n  \n}\n\nThis compute the proportion of the first decile directly from the table above:\n\nweighted.mean(x = table.income$prop_vote[table.income$d1 == 1], w = table.income$share_decile[table.income$d1 == 1]) # d1\n\n[1] 0.301588\n\n\nLet’s try to compute the 3 first decile in a way that can then be put into a for loop or even into a function later\n\n# for d_i\n\nweighted.mean(x = c(table.income$prop_vote[table.income[,12] == 1]),\n              w = c(table.income$share_decile[table.income[,12] == 1])) #d1: take columns 11 (D1) and the values of prop vote and share decile for which D11 == 1\n\n[1] 0.301588\n\nweighted.mean(x = c(table.income$prop_vote[table.income[,13] == 1]),\n              w = c(table.income$share_rest[table.income[,12] == 1 & table.income[, 13] == 1],\n                    table.income$share_decile[table.income[,12] == 0 & table.income[,13] == 1])) #d2\n\n[1] 0.2350616\n\nweighted.mean(x = c(table.income$prop_vote[table.income[,14] == 1]),\n              w = c(table.income$share_rest[table.income[,14-1] == 1 & table.income[, 14] == 1], #d3\n                    table.income$share_decile[table.income[14-1] == 0 & table.income[,14] == 1]))\n\n[1] 0.2985395\n\n\nLet’s try the for loop\n\ndecile_vec &lt;- rep(NA, 10)\ndecile &lt;- c()\n#11:20 are the decile dummies columns in the dataset\ndecile_vec &lt;- capture.output(for (i in 12:21) {\n\n  if(i == 12){\n    \n  decile =  c(weighted.mean(x = c(table.income$prop_vote[table.income[,i] == 1]),\n              w = c(table.income$share_decile[table.income[,i] == 1])))\n  }else{\n    \n   decile = c(weighted.mean(x = c(table.income$prop_vote[table.income[,i] == 1]),\n              w = c(table.income$share_rest[table.income[,i-1] == 1 & table.income[, i] == 1], \n                    table.income$share_decile[table.income[i-1] == 0 & table.income[,i] == 1])))\n  }\n \ncat(decile,\"\\n\")\n})\n\ndecile_vec &lt;- as.numeric(decile_vec)\ndecile &lt;- data.frame(decile = 1:10,\n                         prop = decile_vec)\ndecile\n\n   decile      prop\n1       1 0.3015880\n2       2 0.2350616\n3       3 0.2985395\n4       4 0.2873299\n5       5 0.2359808\n6       6 0.2147917\n7       7 0.2308659\n8       8 0.1992121\n9       9 0.1779249\n10     10 0.1737567\n\n\nThis seems to work, the 3 first values are the same as the ones computed above\n\ndecile %&gt;% \n  ggplot()+\n  aes(x = factor(decile), y = prop)+\n  geom_col()+\n  theme_bw()\n\n\n\n\n\n\n\n\nThis graph is very close to the one from the techninal note (figure one, right-sided graph)\n\n\n\n\n\nFor now, let’s put the for loop into a function: this function would require the decile dummies columns, the columns for the proportion, the column for decile share and the column for the rest’s share:\n\ndecile &lt;- function(data, columns, prop, share_decile, share_rest){\n  \n  decile_vec &lt;- rep(NA, 10)\ndecile &lt;- c()\n\ndecile_vec &lt;- capture.output(for (i in min(columns):max(columns)) {\n\n  if(i == min(columns)){\n    \n  decile =  c(weighted.mean(x = c(prop[data[,i] == 1]),\n              w = c(share_decile[data[,i] == 1])))\n  }else{\n    \n   decile = c(weighted.mean(x = c(prop[data[,i] == 1]),\n              w = c(share_rest[data[,i-1] == 1 & data[, i] == 1], \n                    share_decile[data[i-1] == 0 & data[,i] == 1])))\n  }\n \ncat(decile,\"\\n\")\n})\n\ndecile_vec &lt;- as.numeric(decile_vec)\ndecile &lt;- data.frame(decile = 1:10,\n                         prop = decile_vec)\ndecile &lt;- decile %&gt;% \n  mutate(across(everything(), ~replace(.x, is.nan(.x), 0))) # if an income group has no individuals who has votendp == 1, make the function return 0 instead of NaNs\n}\n\n\ndecile(data = table.income, columns = 12:21, prop = table.income$prop_vote, share_decile = table.income$share_decile, share_rest = table.income$share_rest)\n\nThe function seems to work, but requires a specific dataframe (formatted as table.income)."
  },
  {
    "objectID": "posts/Techinal notes of political cleavages and inequality/technical note piketty 2021.html#how-far-can-we-go-with-income-brackets-in-progress",
    "href": "posts/Techinal notes of political cleavages and inequality/technical note piketty 2021.html#how-far-can-we-go-with-income-brackets-in-progress",
    "title": "From income brackets to income decile",
    "section": "",
    "text": "To build their database on World political cleavages and inequality, Piketty and his team had to use electoral survey data. One problem with these sources is that they collect income data through brackets, without reporting the overall income average of the sample or the average per bracket. This Thus poses the question of how far one can go in terms of statistical analysis with only income brackets as a source of information for income. I will here explore what can be done with such a variable as well as the technical note that Piketty et al. (2021) provides to explain how they computed the vote share for income and education decile, which they claim is one of their main contributions on political cleavages and inequality.\n\n\nThe wpid is based on an impressive dataset compiling electoral survey data of 500 elections since 1948. Since the technical note takes Canada’s 2015 election as an example, I will use the latter here.\n\nrm(list = ls())\nca &lt;- read_dta(\"ca.dta\")\nca2015 &lt;- ca %&gt;% \n  filter(year == 2015)\nrm(ca)\n\nsort(unique(ca2015$inc))\n\n [1]  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20\n\n\nNote that there is already something going on here: in the dataset, the variable income has 19 brackets/categories here whereas it has 18 in the technical note. Furthermore, income brackets “jump” from 8 to 10. I will ignore these issues and still work with this dataset, we just won’t have the same results as in Piketty & al’s example.\nA first step in analyzing such a variable is to compute the frequency, relative frequency and the cumulative frequencies. More precisely, I will construct two tables. On the one hand, I compute the descriptive statistics for the NDP’s voters within each income brackets. On the other hand, I construct a table for the overall distribution of income brackets for the NPD’s voters only. Note that I directly add the proportion of NDP’s voters from the first table to the second one to simplify further calculations:\n\nca2015 %&gt;%\n  group_by(inc) %&gt;% \n  count(votendp) %&gt;% \n  drop_na() %&gt;% \n  mutate(\n    cum.n = cumsum(n),\n    prop = n/sum(n)) %&gt;% ungroup() -&gt; table.income.pervote\n\nca2015 %&gt;% filter(votendp == 1) %&gt;% \n  group_by(inc) %&gt;% \n  count(votendp) %&gt;% \n  drop_na() %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    prop_vote = table.income.pervote$prop[table.income.pervote$votendp == 1],\n    cum.n = cumsum(n),\n    prop = n/sum(n),\n    rangeleft = lag(cumsum(prop), default = 0),\n    cumrelfreqN = cumsum(prop),\n    cumrelfreqInc = cumsum(inc/sum(inc))) -&gt; table.income\n\ntable.income.pervote %&gt;% \n  gt(caption = \"Distribution of the vote for the NDP by income group\") %&gt;% \n  gt_theme_dark()\n\n\n\n\n\nDistribution of the vote for the NDP by income group\n\n\ninc\nvotendp\nn\ncum.n\nprop\n\n\n\n\n1\n0\n114\n114\n0.6826347\n\n\n1\n1\n53\n167\n0.3173653\n\n\n2\n0\n161\n161\n0.7419355\n\n\n2\n1\n56\n217\n0.2580645\n\n\n3\n0\n49\n49\n0.7656250\n\n\n3\n1\n15\n64\n0.2343750\n\n\n4\n0\n129\n129\n0.7865854\n\n\n4\n1\n35\n164\n0.2134146\n\n\n5\n0\n108\n108\n0.6708075\n\n\n5\n1\n53\n161\n0.3291925\n\n\n6\n0\n112\n112\n0.7044025\n\n\n6\n1\n47\n159\n0.2955975\n\n\n7\n0\n114\n114\n0.7215190\n\n\n7\n1\n44\n158\n0.2784810\n\n\n8\n0\n244\n244\n0.7746032\n\n\n8\n1\n71\n315\n0.2253968\n\n\n10\n0\n140\n140\n0.8045977\n\n\n10\n1\n34\n174\n0.1954023\n\n\n11\n0\n114\n114\n0.7651007\n\n\n11\n1\n35\n149\n0.2348993\n\n\n12\n0\n154\n154\n0.7549020\n\n\n12\n1\n50\n204\n0.2450980\n\n\n13\n0\n105\n105\n0.7894737\n\n\n13\n1\n28\n133\n0.2105263\n\n\n14\n0\n208\n208\n0.7878788\n\n\n14\n1\n56\n264\n0.2121212\n\n\n15\n0\n95\n95\n0.8333333\n\n\n15\n1\n19\n114\n0.1666667\n\n\n16\n0\n113\n113\n0.8129496\n\n\n16\n1\n26\n139\n0.1870504\n\n\n17\n0\n171\n171\n0.8300971\n\n\n17\n1\n35\n206\n0.1699029\n\n\n18\n0\n83\n83\n0.7830189\n\n\n18\n1\n23\n106\n0.2169811\n\n\n19\n0\n143\n143\n0.8171429\n\n\n19\n1\n32\n175\n0.1828571\n\n\n20\n0\n145\n145\n0.8734940\n\n\n20\n1\n21\n166\n0.1265060\n\n\n\n\n\n\ntable.income %&gt;% \n  gt(caption =  \"Distribution of income group among NDP's voters\") %&gt;% \n  gt_theme_dark()\n\n\n\n\n\nDistribution of income group among NDP's voters\n\n\ninc\nvotendp\nn\nprop_vote\ncum.n\nprop\nrangeleft\ncumrelfreqN\ncumrelfreqInc\n\n\n\n\n1\n1\n53\n0.3173653\n53\n0.07230559\n0.00000000\n0.07230559\n0.004975124\n\n\n2\n1\n56\n0.2580645\n109\n0.07639836\n0.07230559\n0.14870396\n0.014925373\n\n\n3\n1\n15\n0.2343750\n124\n0.02046385\n0.14870396\n0.16916780\n0.029850746\n\n\n4\n1\n35\n0.2134146\n159\n0.04774898\n0.16916780\n0.21691678\n0.049751244\n\n\n5\n1\n53\n0.3291925\n212\n0.07230559\n0.21691678\n0.28922237\n0.074626866\n\n\n6\n1\n47\n0.2955975\n259\n0.06412005\n0.28922237\n0.35334243\n0.104477612\n\n\n7\n1\n44\n0.2784810\n303\n0.06002729\n0.35334243\n0.41336971\n0.139303483\n\n\n8\n1\n71\n0.2253968\n374\n0.09686221\n0.41336971\n0.51023192\n0.179104478\n\n\n10\n1\n34\n0.1954023\n408\n0.04638472\n0.51023192\n0.55661664\n0.228855721\n\n\n11\n1\n35\n0.2348993\n443\n0.04774898\n0.55661664\n0.60436562\n0.283582090\n\n\n12\n1\n50\n0.2450980\n493\n0.06821282\n0.60436562\n0.67257844\n0.343283582\n\n\n13\n1\n28\n0.2105263\n521\n0.03819918\n0.67257844\n0.71077763\n0.407960199\n\n\n14\n1\n56\n0.2121212\n577\n0.07639836\n0.71077763\n0.78717599\n0.477611940\n\n\n15\n1\n19\n0.1666667\n596\n0.02592087\n0.78717599\n0.81309686\n0.552238806\n\n\n16\n1\n26\n0.1870504\n622\n0.03547067\n0.81309686\n0.84856753\n0.631840796\n\n\n17\n1\n35\n0.1699029\n657\n0.04774898\n0.84856753\n0.89631651\n0.716417910\n\n\n18\n1\n23\n0.2169811\n680\n0.03137790\n0.89631651\n0.92769441\n0.805970149\n\n\n19\n1\n32\n0.1828571\n712\n0.04365621\n0.92769441\n0.97135061\n0.900497512\n\n\n20\n1\n21\n0.1265060\n733\n0.02864939\n0.97135061\n1.00000000\n1.000000000\n\n\n\n\n\n\n\nWe can then plot income groups against their proportion\n\ntable.income.pervote %&gt;% filter(votendp == 1) %&gt;% \n  ggplot()+\n  aes(x = factor(inc), y = prop)+\n  geom_col()+\n  theme_bw()+\n  xlab(\"Income group\") -&gt; income.group.plot\n\ntable.income %&gt;% \n  ggplot()+\n  aes(x = factor(inc), y = prop) %&gt;% \n  geom_col()+\n  theme_bw()+\n  ylab(\"\")+\n  xlab(\"\") -&gt; income.group.plot2\n  \n\ncowplot::plot_grid(income.group.plot, income.group.plot2)\n\n\n\n\nDistribution of Income groups: Canadian 2015 election survey\n\n\n\n\nOn the left, we have a graph very similar to the one of the technical note. The right-sided graph is different, because the proportion are for the overall NPD’s voters whereas the left-sided graph represents the proportion within the income group. For example, 30% of income bracket 1 voted for the NDP, but they represent about 7.4% of total NDP’s voters.\nWhat makes Piketty’s team approach special and interesting is their systematic analysis in terms of quantile groups. This is, according to them, their main contribution and this approach has the advantage to allow for systematic comparison accross space and time. We will try to reproduce here their conversion of income group into quantiles.\nIn R, the decile for each observation can be added to the dataset with the function ntile():\n\nca2015 &lt;- ca2015 %&gt;% \n  mutate(\n  decile = ntile(inc, 10)\n)\n\nNow, the last column of ca2015 is the decile for each observation in the dataset.\n\nca2015 %&gt;% \n  group_by(decile) %&gt;% \n  count(votendp) %&gt;% \n  filter(votendp == 1) %&gt;% \n  drop_na() %&gt;% \n  ungroup() %&gt;% \n  mutate(prop = n/sum(n),\n         cumsumlag = (lag(cumsum(prop))),\n         cumsum = cumsum(prop)) -&gt; table.income.vote\n\ntable.income.vote %&gt;% \n  gt(caption = \"Decile and income bracket\") %&gt;% \n  gt_theme_dark()\n\n\n\n\n\nDecile and income bracket\n\n\ndecile\nvotendp\nn\nprop\ncumsumlag\ncumsum\n\n\n\n\n1\n1\n76\n0.10368349\nNA\n0.1036835\n\n\n2\n1\n70\n0.09549795\n0.1036835\n0.1991814\n\n\n3\n1\n106\n0.14461119\n0.1991814\n0.3437926\n\n\n4\n1\n81\n0.11050477\n0.3437926\n0.4542974\n\n\n5\n1\n57\n0.07776262\n0.4542974\n0.5320600\n\n\n6\n1\n88\n0.12005457\n0.5320600\n0.6521146\n\n\n7\n1\n70\n0.09549795\n0.6521146\n0.7476126\n\n\n8\n1\n67\n0.09140518\n0.7476126\n0.8390177\n\n\n9\n1\n63\n0.08594816\n0.8390177\n0.9249659\n\n\n10\n1\n55\n0.07503411\n0.9249659\n1.0000000\n\n\n\n\n\n\n\nHowever, it is straightforward to see that the ntile() function has flaws in decile computing.\nMore generally, computing income decile when the income variable is in brackets seems complicated, but the technical note proposes a re-weighting average approach to partially solve this problem.\nTo see how the re-weighing approach works, let’s go back to the first table:\n\ntable.income %&gt;%\n  gt() %&gt;% \n  gt_theme_dark()\n\n\n\n\n\n\n\ninc\nvotendp\nn\nprop_vote\ncum.n\nprop\nrangeleft\ncumrelfreqN\ncumrelfreqInc\n\n\n\n\n1\n1\n53\n0.3173653\n53\n0.07230559\n0.00000000\n0.07230559\n0.004975124\n\n\n2\n1\n56\n0.2580645\n109\n0.07639836\n0.07230559\n0.14870396\n0.014925373\n\n\n3\n1\n15\n0.2343750\n124\n0.02046385\n0.14870396\n0.16916780\n0.029850746\n\n\n4\n1\n35\n0.2134146\n159\n0.04774898\n0.16916780\n0.21691678\n0.049751244\n\n\n5\n1\n53\n0.3291925\n212\n0.07230559\n0.21691678\n0.28922237\n0.074626866\n\n\n6\n1\n47\n0.2955975\n259\n0.06412005\n0.28922237\n0.35334243\n0.104477612\n\n\n7\n1\n44\n0.2784810\n303\n0.06002729\n0.35334243\n0.41336971\n0.139303483\n\n\n8\n1\n71\n0.2253968\n374\n0.09686221\n0.41336971\n0.51023192\n0.179104478\n\n\n10\n1\n34\n0.1954023\n408\n0.04638472\n0.51023192\n0.55661664\n0.228855721\n\n\n11\n1\n35\n0.2348993\n443\n0.04774898\n0.55661664\n0.60436562\n0.283582090\n\n\n12\n1\n50\n0.2450980\n493\n0.06821282\n0.60436562\n0.67257844\n0.343283582\n\n\n13\n1\n28\n0.2105263\n521\n0.03819918\n0.67257844\n0.71077763\n0.407960199\n\n\n14\n1\n56\n0.2121212\n577\n0.07639836\n0.71077763\n0.78717599\n0.477611940\n\n\n15\n1\n19\n0.1666667\n596\n0.02592087\n0.78717599\n0.81309686\n0.552238806\n\n\n16\n1\n26\n0.1870504\n622\n0.03547067\n0.81309686\n0.84856753\n0.631840796\n\n\n17\n1\n35\n0.1699029\n657\n0.04774898\n0.84856753\n0.89631651\n0.716417910\n\n\n18\n1\n23\n0.2169811\n680\n0.03137790\n0.89631651\n0.92769441\n0.805970149\n\n\n19\n1\n32\n0.1828571\n712\n0.04365621\n0.92769441\n0.97135061\n0.900497512\n\n\n20\n1\n21\n0.1265060\n733\n0.02864939\n0.97135061\n1.00000000\n1.000000000\n\n\n\n\n\n\n\nWe can directly see the problem posed by income brackets: for example, we can see that all of income bracket one belongs to the first decile since its relative range is between 0 and 0.0723. However, the relative range of bracket two is [0.0723 - 0.1487]. Some part of it belong to the first decile ([0 - 0.1]), but some belong to the second ([0 - 0.2]). The approach to compute the proportion of observations belonging to the any given decile is to compute the share of each income bracket belonging to this decile and then compute a weighted average. For example, if I want to compute the share of observation of the first decile (D10), I already know that 100% of income bracket one belongs to D10 but I need to know the share of bracket 2 (B2) belonging to D10.\nTo estimate this, let’s assume the distribution of B2 is uniform \\(x \\sim U[0.0723; 0.1487]\\), with x the observation within this range. We want to know \\(P(x&lt;0.1)\\), that is to say, the probability that x belongs to the first decile. We use the uniform cumulative distribution function with parameters min = 0.073 and max = 0.1487: \\(P(x&lt;0.1) = \\frac{0.1-0.0723}{0.1487-0.0723} = 0.3626\\). This means that 36.26% of B2 belongs to D10. Then, the weighted average for the proportion of observation within D10: \\(\\frac{1*0.317+0.3626*0.26}{1+0.3626} = 0.3018\\). 30.2% the first decile voters voted for the NDP in 2015.\nHere are the computations in R:\n\npunif(0.1, table.income$cumrelfreqN[1], table.income$cumrelfreqN[2])\n\n[1] 0.3625\n\n(1*0.317+0.3626*0.26)/(1+0.3626)\n\n[1] 0.3018318\n\nweighted.mean(x = c(table.income$prop_vote[1], table.income$prop_vote[2]), w = c(1, punif(0.1, table.income$cumrelfreqN[1], table.income$cumrelfreqN[2])))\n\n[1] 0.301588\n\n\nUnfortunately, there is to my knowledge no function in R that will compute the weights automatically. I can nonetheless compute them through a tedious for loop:\n\nweight &lt;- rep(NA, length(table.income$inc))\n\nfor (i in 1:length(table.income$inc)) {\n  weight[i] &lt;- ifelse(table.income$cumrelfreqN[i] &lt; 0.1 | table.income$cumrelfreqN[i] == 1, 1,\n                  ifelse(table.income$cumrelfreqN[i] &gt; 0.1 & table.income$cumrelfreqN[i] &lt; 0.2, punif(0.1, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]),\n                         ifelse(table.income$cumrelfreqN[i] &gt; 0.2 & table.income$cumrelfreqN[i] &lt; 0.3, punif(0.2, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]),\n                                ifelse(table.income$cumrelfreqN[i] &gt; 0.3 & table.income$cumrelfreqN[i] &lt; 0.4, punif(0.3, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]),\n                                       ifelse(table.income$cumrelfreqN[i] &gt; 0.4 & table.income$cumrelfreqN[i] &lt; 0.5, punif(0.4, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]), \n                                              ifelse(table.income$cumrelfreqN[i] &gt; 0.5 & table.income$cumrelfreqN[i] &lt; 0.6, punif(0.5, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]), \n                                                     ifelse(table.income$cumrelfreqN[i] &gt; 0.6 & table.income$cumrelfreqN[i] &lt; 0.7, punif(0.6, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]),\n                                                            ifelse(table.income$cumrelfreqN[i] &gt; 0.7 & table.income$cumrelfreqN[i] &lt; 0.8, punif(0.7, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]),\n                                                                   ifelse(table.income$cumrelfreqN[i] &gt; 0.8 & table.income$cumrelfreqN[i] &lt; 0.9, punif(0.8, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]), \n                                                                          ifelse(table.income$cumrelfreqN[i] &gt; 0.9 & table.income$cumrelfreqN[i] &lt; 1, punif(0.9, min = table.income$cumrelfreqN[i-1], max = table.income$cumrelfreqN[i]), 1))))))))))\n}\nweight\n\n [1] 1.0000000 0.3625000 0.0000000 0.6457143 0.0000000 0.1680851 0.7772727\n [8] 0.8943662 0.0000000 0.9085714 0.0000000 0.7178571 0.0000000 0.4947368\n[15] 0.0000000 0.0000000 0.1173913 0.0000000 1.0000000\n\n\nI replace the 0 with 1:\n\nweight &lt;- ifelse(weight == 0, 1, weight)\n\ntable.income &lt;- table.income %&gt;% \n  mutate(share_decile = weight)\n\ntable.income %&gt;% \n  mutate(share_rest = 1 - weight) -&gt; table.income\n\ntable.income %&gt;% mutate(\n  prop_vote = table.income.pervote$prop[table.income.pervote$votendp == 1]\n) %&gt;% gt() %&gt;% \n  gt_theme_dark()\n\n\n\n\n\n\n\ninc\nvotendp\nn\nprop_vote\ncum.n\nprop\nrangeleft\ncumrelfreqN\ncumrelfreqInc\nshare_decile\nshare_rest\n\n\n\n\n1\n1\n53\n0.3173653\n53\n0.07230559\n0.00000000\n0.07230559\n0.004975124\n1.0000000\n0.00000000\n\n\n2\n1\n56\n0.2580645\n109\n0.07639836\n0.07230559\n0.14870396\n0.014925373\n0.3625000\n0.63750000\n\n\n3\n1\n15\n0.2343750\n124\n0.02046385\n0.14870396\n0.16916780\n0.029850746\n1.0000000\n0.00000000\n\n\n4\n1\n35\n0.2134146\n159\n0.04774898\n0.16916780\n0.21691678\n0.049751244\n0.6457143\n0.35428571\n\n\n5\n1\n53\n0.3291925\n212\n0.07230559\n0.21691678\n0.28922237\n0.074626866\n1.0000000\n0.00000000\n\n\n6\n1\n47\n0.2955975\n259\n0.06412005\n0.28922237\n0.35334243\n0.104477612\n0.1680851\n0.83191489\n\n\n7\n1\n44\n0.2784810\n303\n0.06002729\n0.35334243\n0.41336971\n0.139303483\n0.7772727\n0.22272727\n\n\n8\n1\n71\n0.2253968\n374\n0.09686221\n0.41336971\n0.51023192\n0.179104478\n0.8943662\n0.10563380\n\n\n10\n1\n34\n0.1954023\n408\n0.04638472\n0.51023192\n0.55661664\n0.228855721\n1.0000000\n0.00000000\n\n\n11\n1\n35\n0.2348993\n443\n0.04774898\n0.55661664\n0.60436562\n0.283582090\n0.9085714\n0.09142857\n\n\n12\n1\n50\n0.2450980\n493\n0.06821282\n0.60436562\n0.67257844\n0.343283582\n1.0000000\n0.00000000\n\n\n13\n1\n28\n0.2105263\n521\n0.03819918\n0.67257844\n0.71077763\n0.407960199\n0.7178571\n0.28214286\n\n\n14\n1\n56\n0.2121212\n577\n0.07639836\n0.71077763\n0.78717599\n0.477611940\n1.0000000\n0.00000000\n\n\n15\n1\n19\n0.1666667\n596\n0.02592087\n0.78717599\n0.81309686\n0.552238806\n0.4947368\n0.50526316\n\n\n16\n1\n26\n0.1870504\n622\n0.03547067\n0.81309686\n0.84856753\n0.631840796\n1.0000000\n0.00000000\n\n\n17\n1\n35\n0.1699029\n657\n0.04774898\n0.84856753\n0.89631651\n0.716417910\n1.0000000\n0.00000000\n\n\n18\n1\n23\n0.2169811\n680\n0.03137790\n0.89631651\n0.92769441\n0.805970149\n0.1173913\n0.88260870\n\n\n19\n1\n32\n0.1828571\n712\n0.04365621\n0.92769441\n0.97135061\n0.900497512\n1.0000000\n0.00000000\n\n\n20\n1\n21\n0.1265060\n733\n0.02864939\n0.97135061\n1.00000000\n1.000000000\n1.0000000\n0.00000000\n\n\n\n\n\n\n\nThere can be mistakes, but the results seem to make sense\nSince I do not want to ever do this computation again (😅), I put all of this into a function:\n\nweight_share &lt;- function(x){\n  weight &lt;- rep(NA, length(x))\n  \n  for (i in 1:length(x)) {\n   weight[i] &lt;-  ifelse(x[i] &lt; 0.1 | x[i] == 1, 1,\n                  ifelse(x[i] &gt; 0.1 & x[i] &lt; 0.2, punif(0.1, min = x[i-1], max = x[i]),\n                         ifelse(x[i] &gt; 0.2 & x[i] &lt; 0.3, punif(0.2, min = x[i-1], max = x[i]),\n                                ifelse(x[i] &gt; 0.3 & x[i] &lt; 0.4, punif(0.3, min = x[i-1], max = x[i]),\n                                       ifelse(x[i] &gt; 0.4 & x[i] &lt; 0.5, punif(0.4, min = x[i-1], max = x[i]), \n                                              ifelse(x[i] &gt; 0.5 & x[i] &lt; 0.6, punif(0.5, min = x[i-1], max = x[i]), \n                                                     ifelse(x[i] &gt; 0.6 & x[i] &lt; 0.7, punif(0.6, min = x[i-1], max = x[i]),\n                                                            ifelse(x[i] &gt; 0.7 & x[i] &lt; 0.8, punif(0.7, min = x[i-1], max = x[i]),\n                                                                   ifelse(x[i] &gt; 0.8 & x[i] &lt; 0.9, punif(0.8, min = x[i-1], max = x[i]),\n                                                                                                         ifelse(x[i] &gt; 0.9 & x[i] &lt; 1, punif(0.9, min = x[i-1], max = x[i]), 1))))))))))\n  }\n  weight &lt;- ifelse(weight == 0, 1, weight)\n  print(weight)\n}\n\nLet’s check if the function works\n\nweight_share(x = table.income$cumrelfreqN)\n\n [1] 1.0000000 0.3625000 1.0000000 0.6457143 1.0000000 0.1680851 0.7772727\n [8] 0.8943662 1.0000000 0.9085714 1.0000000 0.7178571 1.0000000 0.4947368\n[15] 1.0000000 1.0000000 0.1173913 1.0000000 1.0000000\n\n\nWe are almost done, there are only the weighted averages for each decile left to compute. One further step is to compute dummy variables to show to which decile income brackets belong to. This will produce a table close to the one from the technical note.\n\ntable.income %&gt;% \n  mutate(d1 = ifelse(table.income$rangeleft &gt;= 0 & table.income$rangeleft &lt; 0.1, 1, 0),\n         d2 = ifelse(table.income$rangeleft %[]% c(0.1, 0.2) | table.income$cumrelfreqN %[]% c(0.1, 0.2), 1, 0),  # the %[]% is an \"between\" operator from the Desctools package. for example, x %[]% c(a, b) checks whether x belong to the interval [a, b] with a&lt;b \n         d3 = ifelse(table.income$rangeleft %[]% c(0.2, 0.3) | table.income$cumrelfreqN %[]% c(0.2, 0.3), 1, 0),\n         d4 = ifelse(table.income$rangeleft %[]% c(0.3, 0.4) | table.income$cumrelfreqN %[]% c(0.3, 0.4), 1, 0),\n         d5 = ifelse(table.income$rangeleft %[]% c(0.4, 0.5) | table.income$cumrelfreqN %[]% c(0.4, 0.5), 1, 0),\n         d6 = ifelse(table.income$rangeleft %[]% c(0.5, 0.6) | table.income$cumrelfreqN %[]% c(0.5, 0.6), 1, 0),\n         d7 = ifelse(table.income$rangeleft %[]% c(0.6, 0.7) | table.income$cumrelfreqN %[]% c(0.6, 0.7), 1, 0),\n         d8 = ifelse(table.income$rangeleft %[]% c(0.7, 0.8) | table.income$cumrelfreqN %[]% c(0.7, 0.8), 1, 0),\n         d9 = ifelse(table.income$rangeleft %[]% c(0.8, 0.9) | table.income$cumrelfreqN %[]% c(0.8, 0.9), 1, 0),\n         d10 = ifelse(table.income$rangeleft %[]% c(0.9, 1) | table.income$cumrelfreqN %[]% c(0.9, 1), 1, 0)) -&gt; table.income\n\ntable.income %&gt;% \n  gt() %&gt;% \n  gt_theme_dark()\n\n\n\n\n\n\n\ninc\nvotendp\nn\nprop_vote\ncum.n\nprop\nrangeleft\ncumrelfreqN\ncumrelfreqInc\nshare_decile\nshare_rest\nd1\nd2\nd3\nd4\nd5\nd6\nd7\nd8\nd9\nd10\n\n\n\n\n1\n1\n53\n0.3173653\n53\n0.07230559\n0.00000000\n0.07230559\n0.004975124\n1.0000000\n0.00000000\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n56\n0.2580645\n109\n0.07639836\n0.07230559\n0.14870396\n0.014925373\n0.3625000\n0.63750000\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n1\n15\n0.2343750\n124\n0.02046385\n0.14870396\n0.16916780\n0.029850746\n1.0000000\n0.00000000\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n1\n35\n0.2134146\n159\n0.04774898\n0.16916780\n0.21691678\n0.049751244\n0.6457143\n0.35428571\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n5\n1\n53\n0.3291925\n212\n0.07230559\n0.21691678\n0.28922237\n0.074626866\n1.0000000\n0.00000000\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n6\n1\n47\n0.2955975\n259\n0.06412005\n0.28922237\n0.35334243\n0.104477612\n0.1680851\n0.83191489\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n7\n1\n44\n0.2784810\n303\n0.06002729\n0.35334243\n0.41336971\n0.139303483\n0.7772727\n0.22272727\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n\n\n8\n1\n71\n0.2253968\n374\n0.09686221\n0.41336971\n0.51023192\n0.179104478\n0.8943662\n0.10563380\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n10\n1\n34\n0.1954023\n408\n0.04638472\n0.51023192\n0.55661664\n0.228855721\n1.0000000\n0.00000000\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n11\n1\n35\n0.2348993\n443\n0.04774898\n0.55661664\n0.60436562\n0.283582090\n0.9085714\n0.09142857\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n\n\n12\n1\n50\n0.2450980\n493\n0.06821282\n0.60436562\n0.67257844\n0.343283582\n1.0000000\n0.00000000\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n13\n1\n28\n0.2105263\n521\n0.03819918\n0.67257844\n0.71077763\n0.407960199\n0.7178571\n0.28214286\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n\n\n14\n1\n56\n0.2121212\n577\n0.07639836\n0.71077763\n0.78717599\n0.477611940\n1.0000000\n0.00000000\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n15\n1\n19\n0.1666667\n596\n0.02592087\n0.78717599\n0.81309686\n0.552238806\n0.4947368\n0.50526316\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n16\n1\n26\n0.1870504\n622\n0.03547067\n0.81309686\n0.84856753\n0.631840796\n1.0000000\n0.00000000\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n17\n1\n35\n0.1699029\n657\n0.04774898\n0.84856753\n0.89631651\n0.716417910\n1.0000000\n0.00000000\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n18\n1\n23\n0.2169811\n680\n0.03137790\n0.89631651\n0.92769441\n0.805970149\n0.1173913\n0.88260870\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n19\n1\n32\n0.1828571\n712\n0.04365621\n0.92769441\n0.97135061\n0.900497512\n1.0000000\n0.00000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n20\n1\n21\n0.1265060\n733\n0.02864939\n0.97135061\n1.00000000\n1.000000000\n1.0000000\n0.00000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\nTu put the code above into a function:\n\ndecile_dummies &lt;- function(data, rangeleft, rangeright){\n\ndata %&gt;% \n  mutate(d1 = ifelse({{rangeleft}} &gt;= 0 & {{rangeleft}} &lt; 0.1, 1, 0),\n         d2 = ifelse({{rangeleft}} %[]% c(0.1, 0.2) | {{rangeright}} %[]% c(0.1, 0.2), 1, 0),  #%[]% is an within bracket operator from the Desctools package. for example, x %[]% c(a, b) checks whether x belong to the interval [a, b] with a&lt;b \n         d3 = ifelse({{rangeleft}} %[]% c(0.2, 0.3) | {{rangeright}} %[]% c(0.2, 0.3), 1, 0),\n         d4 = ifelse({{rangeleft}} %[]% c(0.3, 0.4) | {{rangeright}} %[]% c(0.3, 0.4), 1, 0),\n         d5 = ifelse({{rangeleft}} %[]% c(0.4, 0.5) | {{rangeright}} %[]% c(0.4, 0.5), 1, 0),\n         d6 = ifelse({{rangeleft}} %[]% c(0.5, 0.6) | {{rangeright}} %[]% c(0.5, 0.6), 1, 0),\n         d7 = ifelse({{rangeleft}} %[]% c(0.6, 0.7) | {{rangeright}} %[]% c(0.6, 0.7), 1, 0),\n         d8 = ifelse({{rangeleft}} %[]% c(0.7, 0.8) | {{rangeright}} %[]% c(0.7, 0.8), 1, 0),\n         d9 = ifelse({{rangeleft}} %[]% c(0.8, 0.9) | {{rangeright}} %[]% c(0.8, 0.9), 1, 0),\n         d10 = ifelse({{rangeleft}} %[]% c(0.9, 1) | {{rangeright}} %[]% c(0.9, 1), 1, 0))\n  \n}\n\nThis compute the proportion of the first decile directly from the table above:\n\nweighted.mean(x = table.income$prop_vote[table.income$d1 == 1], w = table.income$share_decile[table.income$d1 == 1]) # d1\n\n[1] 0.301588\n\n\nLet’s try to compute the 3 first decile in a way that can then be put into a for loop or even into a function later\n\n# for d_i\n\nweighted.mean(x = c(table.income$prop_vote[table.income[,12] == 1]),\n              w = c(table.income$share_decile[table.income[,12] == 1])) #d1: take columns 11 (D1) and the values of prop vote and share decile for which D11 == 1\n\n[1] 0.301588\n\nweighted.mean(x = c(table.income$prop_vote[table.income[,13] == 1]),\n              w = c(table.income$share_rest[table.income[,12] == 1 & table.income[, 13] == 1],\n                    table.income$share_decile[table.income[,12] == 0 & table.income[,13] == 1])) #d2\n\n[1] 0.2350616\n\nweighted.mean(x = c(table.income$prop_vote[table.income[,14] == 1]),\n              w = c(table.income$share_rest[table.income[,14-1] == 1 & table.income[, 14] == 1], #d3\n                    table.income$share_decile[table.income[14-1] == 0 & table.income[,14] == 1]))\n\n[1] 0.2985395\n\n\nLet’s try the for loop\n\ndecile_vec &lt;- rep(NA, 10)\ndecile &lt;- c()\n#11:20 are the decile dummies columns in the dataset\ndecile_vec &lt;- capture.output(for (i in 12:21) {\n\n  if(i == 12){\n    \n  decile =  c(weighted.mean(x = c(table.income$prop_vote[table.income[,i] == 1]),\n              w = c(table.income$share_decile[table.income[,i] == 1])))\n  }else{\n    \n   decile = c(weighted.mean(x = c(table.income$prop_vote[table.income[,i] == 1]),\n              w = c(table.income$share_rest[table.income[,i-1] == 1 & table.income[, i] == 1], \n                    table.income$share_decile[table.income[i-1] == 0 & table.income[,i] == 1])))\n  }\n \ncat(decile,\"\\n\")\n})\n\ndecile_vec &lt;- as.numeric(decile_vec)\ndecile &lt;- data.frame(decile = 1:10,\n                         prop = decile_vec)\ndecile\n\n   decile      prop\n1       1 0.3015880\n2       2 0.2350616\n3       3 0.2985395\n4       4 0.2873299\n5       5 0.2359808\n6       6 0.2147917\n7       7 0.2308659\n8       8 0.1992121\n9       9 0.1779249\n10     10 0.1737567\n\n\nThis seems to work, the 3 first values are the same as the ones computed above\n\ndecile %&gt;% \n  ggplot()+\n  aes(x = factor(decile), y = prop)+\n  geom_col()+\n  theme_bw()\n\n\n\n\n\n\n\n\nThis graph is very close to the one from the techninal note (figure one, right-sided graph)\n\n\n\n\n\nFor now, let’s put the for loop into a function: this function would require the decile dummies columns, the columns for the proportion, the column for decile share and the column for the rest’s share:\n\ndecile &lt;- function(data, columns, prop, share_decile, share_rest){\n  \n  decile_vec &lt;- rep(NA, 10)\ndecile &lt;- c()\n\ndecile_vec &lt;- capture.output(for (i in min(columns):max(columns)) {\n\n  if(i == min(columns)){\n    \n  decile =  c(weighted.mean(x = c(prop[data[,i] == 1]),\n              w = c(share_decile[data[,i] == 1])))\n  }else{\n    \n   decile = c(weighted.mean(x = c(prop[data[,i] == 1]),\n              w = c(share_rest[data[,i-1] == 1 & data[, i] == 1], \n                    share_decile[data[i-1] == 0 & data[,i] == 1])))\n  }\n \ncat(decile,\"\\n\")\n})\n\ndecile_vec &lt;- as.numeric(decile_vec)\ndecile &lt;- data.frame(decile = 1:10,\n                         prop = decile_vec)\ndecile &lt;- decile %&gt;% \n  mutate(across(everything(), ~replace(.x, is.nan(.x), 0))) # if an income group has no individuals who has votendp == 1, make the function return 0 instead of NaNs\n}\n\n\ndecile(data = table.income, columns = 12:21, prop = table.income$prop_vote, share_decile = table.income$share_decile, share_rest = table.income$share_rest)\n\nThe function seems to work, but requires a specific dataframe (formatted as table.income)."
  },
  {
    "objectID": "posts/Techinal notes of political cleavages and inequality/technical note piketty 2021.html#summing-up-all-of-this-into-one-function",
    "href": "posts/Techinal notes of political cleavages and inequality/technical note piketty 2021.html#summing-up-all-of-this-into-one-function",
    "title": "From income brackets to income decile",
    "section": "Summing up all of this into one function",
    "text": "Summing up all of this into one function\nThe goal here is to write all the steps above into one function, such that only the variable from the dataset must be given to compute the deciles.\n\n# note: the function needs the weight_share() function and the decile_dummies function and the decile function\n\nweight_share &lt;- function(x){\n  weight &lt;- rep(NA, length(x))\n  \n  for (i in 1:length(x)) {\n   weight[i] &lt;-  ifelse(x[i] &lt; 0.1 | x[i] == 1, 1,\n                  ifelse(x[i] &gt; 0.1 & x[i] &lt; 0.2, punif(0.1, min = x[i-1], max = x[i]),\n                         ifelse(x[i] &gt; 0.2 & x[i] &lt; 0.3, punif(0.2, min = x[i-1], max = x[i]),\n                                ifelse(x[i] &gt; 0.3 & x[i] &lt; 0.4, punif(0.3, min = x[i-1], max = x[i]),\n                                       ifelse(x[i] &gt; 0.4 & x[i] &lt; 0.5, punif(0.4, min = x[i-1], max = x[i]), \n                                              ifelse(x[i] &gt; 0.5 & x[i] &lt; 0.6, punif(0.5, min = x[i-1], max = x[i]), \n                                                     ifelse(x[i] &gt; 0.6 & x[i] &lt; 0.7, punif(0.6, min = x[i-1], max = x[i]),\n                                                            ifelse(x[i] &gt; 0.7 & x[i] &lt; 0.8, punif(0.7, min = x[i-1], max = x[i]),\n                                                                   ifelse(x[i] &gt; 0.8 & x[i] &lt; 0.9, punif(0.8, min = x[i-1], max = x[i]),\n                                                                                                         ifelse(x[i] &gt; 0.9 & x[i] &lt; 1, punif(0.9, min = x[i-1], max = x[i]), 1))))))))))\n  }\n  weight &lt;- ifelse(weight == 0, 1, weight)\n}\n\n\ndecile_dummies &lt;- function(data, rangeleft, rangeright){\n\ndata %&gt;% \n  mutate(d1 = ifelse({{rangeleft}} &gt;= 0 & {{rangeleft}} &lt; 0.1, 1, 0),\n         d2 = ifelse({{rangeleft}} %[]% c(0.1, 0.2) | {{rangeright}} %[]% c(0.1, 0.2), 1, 0),  #%[]% is an within bracket operator from the Desctools package. for example, x %[]% c(a, b) checks whether x belong to the interval [a, b] with a&lt;b \n         d3 = ifelse({{rangeleft}} %[]% c(0.2, 0.3) | {{rangeright}} %[]% c(0.2, 0.3), 1, 0),\n         d4 = ifelse({{rangeleft}} %[]% c(0.3, 0.4) | {{rangeright}} %[]% c(0.3, 0.4), 1, 0),\n         d5 = ifelse({{rangeleft}} %[]% c(0.4, 0.5) | {{rangeright}} %[]% c(0.4, 0.5), 1, 0),\n         d6 = ifelse({{rangeleft}} %[]% c(0.5, 0.6) | {{rangeright}} %[]% c(0.5, 0.6), 1, 0),\n         d7 = ifelse({{rangeleft}} %[]% c(0.6, 0.7) | {{rangeright}} %[]% c(0.6, 0.7), 1, 0),\n         d8 = ifelse({{rangeleft}} %[]% c(0.7, 0.8) | {{rangeright}} %[]% c(0.7, 0.8), 1, 0),\n         d9 = ifelse({{rangeleft}} %[]% c(0.8, 0.9) | {{rangeright}} %[]% c(0.8, 0.9), 1, 0),\n         d10 = ifelse({{rangeleft}} %[]% c(0.9, 1) | {{rangeright}} %[]% c(0.9, 1), 1, 0))\n  \n}\n\ndecile &lt;- function(data, columns, prop, share_decile, share_rest){\n  \n  decile_vec &lt;- rep(NA, 10)\ndecile &lt;- c()\n\ndecile_vec &lt;- capture.output(for (i in min(columns):max(columns)) {\n\n  if(i == min(columns)){\n    \n  decile =  c(weighted.mean(x = c(prop[data[,i] == 1]),\n              w = c(share_decile[data[,i] == 1])))\n  }else{\n    \n   decile = c(weighted.mean(x = c(prop[data[,i] == 1]),\n              w = c(share_rest[data[,i-1] == 1 & data[, i] == 1], \n                    share_decile[data[i-1] == 0 & data[,i] == 1])))\n  }\n \ncat(decile,\"\\n\")\n})\n\ndecile_vec &lt;- as.numeric(decile_vec)\ndecile &lt;- data.frame(decile = 1:10,\n                         prop = decile_vec)\ndecile &lt;- decile %&gt;% \n  mutate(across(everything(), ~replace(.x, is.nan(.x), 0))) ### if a group has no observation in the count, replace the NaNs by zeros\n\ndecile\n}\n\ndecile_final &lt;- function(data, x, by, value){\n  \ntable1 &lt;- \n  {{data}} %&gt;%\n  group_by({{by}}) %&gt;% \n  count({{x}}) %&gt;% \n  drop_na() %&gt;% \n  mutate(\n    cum_x = cumsum(n),\n    prop_x = n/sum(n)) %&gt;% \n  ungroup() %&gt;% \n  filter({{x}} == value)\n\n\ntable &lt;- \n{{data}} %&gt;% \n  filter({{x}} == value) %&gt;% \n  group_by({{by}}) %&gt;% \n  count({{x}}) %&gt;% \n  drop_na() %&gt;% \n  ungroup() %&gt;% \n  mutate(\n         prop_x = table1$prop_x,\n         prop = n/sum(n),\n         rangeleft = lag(cumsum(prop), default = 0),\n         cumrelfreq = cumsum(prop),\n         share_decile = weight_share(cumrelfreq),\n         share_rest = 1 - weight_share(cumrelfreq)) %&gt;% \n    decile_dummies(rangeleft = rangeleft, rangeright = cumrelfreq)\n\n\ndecile_columns &lt;- which(colnames(table)==\"d1\" ):which(colnames(table)==\"d10\" )\n\ndecile(data = table, columns = decile_columns, prop = table$prop_x, share_decile = table$share_decile, share_rest = table$share_rest)\n\n}\n\n\ndecile_final(data = ca2015, by = inc, x = votendp, value = 1)\n\n   decile      prop\n1       1 0.3015880\n2       2 0.2350616\n3       3 0.2985395\n4       4 0.2873299\n5       5 0.2359808\n6       6 0.2147917\n7       7 0.2308659\n8       8 0.1992121\n9       9 0.1779249\n10     10 0.1737567\n\ndecile_final(data = ca2015, by = inc, x = votelib, value = 1)\n\n   decile      prop\n1       1 0.3807777\n2       2 0.4229278\n3       3 0.3830486\n4       4 0.3831860\n5       5 0.4397704\n6       6 0.3894084\n7       7 0.4532439\n8       8 0.4113987\n9       9 0.4132309\n10     10 0.4833679\n\n\n\nWhat’s Next?\nEven if the goal of reproducing the WPID’s methodology is now done, I am still unsatisfied with some aspects of the work done above. I will try in the future to do the following:\n\nImprove the code of all the functions (see below)\nImprove the functions to allow for greater flexibility (for example allowing to compute not only decile, but also quartiles, quintiles…)\nCreate a function which assigns each individual of the dataset to a quantile group (Piketty and Co-authors provide the function only in Stata…)\n\n\n# note: the function needs the weight_share() function and the decile_dummies function and the decile function\n# I am Still working on improving these functions: simplify the code and allow for a lot more flexibility, for instance:\n      # 1. compute different quantile groups (quartile, quintiles... instead of always deciles)\nweight_share &lt;- function(x){\n  weight &lt;- rep(NA, length(x))\n  \n  for (i in 1:length(x)) {\n   weight[i] &lt;-  ifelse(x[i] &lt; 0.1 | x[i] == 1, 1,\n                  ifelse(x[i] &gt; 0.1 & x[i] &lt; 0.2, punif(0.1, min = x[i-1], max = x[i]),\n                         ifelse(x[i] &gt; 0.2 & x[i] &lt; 0.3, punif(0.2, min = x[i-1], max = x[i]),\n                                ifelse(x[i] &gt; 0.3 & x[i] &lt; 0.4, punif(0.3, min = x[i-1], max = x[i]),\n                                       ifelse(x[i] &gt; 0.4 & x[i] &lt; 0.5, punif(0.4, min = x[i-1], max = x[i]), \n                                              ifelse(x[i] &gt; 0.5 & x[i] &lt; 0.6, punif(0.5, min = x[i-1], max = x[i]), \n                                                     ifelse(x[i] &gt; 0.6 & x[i] &lt; 0.7, punif(0.6, min = x[i-1], max = x[i]),\n                                                            ifelse(x[i] &gt; 0.7 & x[i] &lt; 0.8, punif(0.7, min = x[i-1], max = x[i]),\n                                                                   ifelse(x[i] &gt; 0.8 & x[i] &lt; 0.9, punif(0.8, min = x[i-1], max = x[i]),\n                                                                                                         ifelse(x[i] &gt; 0.9 & x[i] &lt; 1, punif(0.9, min = x[i-1], max = x[i]), 1))))))))))\n  }\n  weight &lt;- ifelse(weight == 0, 1, weight)\n}\n\n\ndecile_dummies &lt;- function(data, rangeleft, rangeright){\n\ndata %&gt;% \n  mutate(d1 = ifelse({{rangeleft}} &gt;= 0 & {{rangeleft}} &lt; 0.1, 1, 0),\n         d2 = ifelse({{rangeleft}} %[]% c(0.1, 0.2) | {{rangeright}} %[]% c(0.1, 0.2), 1, 0),  #%[]% is an within bracket operator from the Desctools package. for example, x %[]% c(a, b) checks whether x belong to the interval [a, b] with a&lt;b \n         d3 = ifelse({{rangeleft}} %[]% c(0.2, 0.3) | {{rangeright}} %[]% c(0.2, 0.3), 1, 0),\n         d4 = ifelse({{rangeleft}} %[]% c(0.3, 0.4) | {{rangeright}} %[]% c(0.3, 0.4), 1, 0),\n         d5 = ifelse({{rangeleft}} %[]% c(0.4, 0.5) | {{rangeright}} %[]% c(0.4, 0.5), 1, 0),\n         d6 = ifelse({{rangeleft}} %[]% c(0.5, 0.6) | {{rangeright}} %[]% c(0.5, 0.6), 1, 0),\n         d7 = ifelse({{rangeleft}} %[]% c(0.6, 0.7) | {{rangeright}} %[]% c(0.6, 0.7), 1, 0),\n         d8 = ifelse({{rangeleft}} %[]% c(0.7, 0.8) | {{rangeright}} %[]% c(0.7, 0.8), 1, 0),\n         d9 = ifelse({{rangeleft}} %[]% c(0.8, 0.9) | {{rangeright}} %[]% c(0.8, 0.9), 1, 0),\n         d10 = ifelse({{rangeleft}} %[]% c(0.9, 1) | {{rangeright}} %[]% c(0.9, 1), 1, 0))\n  \n}\n\ndecile &lt;- function(data, columns, prop, share_decile, share_rest){\n  \n  decile_vec &lt;- rep(NA, 10)\ndecile &lt;- c()\n\ndecile_vec &lt;- capture.output(for (i in min(columns):max(columns)) {\n\n  if(i == min(columns)){\n    \n  decile =  c(weighted.mean(x = c(prop[data[,i] == 1]),\n              w = c(share_decile[data[,i] == 1])))\n  }else{\n    \n   decile = c(weighted.mean(x = c(prop[data[,i] == 1]),\n              w = c(share_rest[data[,i-1] == 1 & data[, i] == 1], \n                    share_decile[data[i-1] == 0 & data[,i] == 1])))\n  }\n \ncat(decile,\"\\n\")\n})\n\ndecile_vec &lt;- as.numeric(decile_vec)\ndecile &lt;- data.frame(decile = 1:10,\n                         prop = decile_vec)\ndecile\n}\n\ntable_decile &lt;- function(data, x, by, value){\n  \ntable1 &lt;- \n  {{data}} %&gt;%\n  group_by({{by}}) %&gt;% \n  count({{x}}) %&gt;% \n  drop_na() %&gt;% \n  mutate(\n    cum_x = cumsum(n),\n    prop_x = n/sum(n)) %&gt;% \n  ungroup() %&gt;% \n  filter({{x}} == value)\n\n\ntable &lt;- \n{{data}} %&gt;% \n  filter({{x}} == value) %&gt;% \n  group_by({{by}}) %&gt;% \n  count({{x}}) %&gt;% \n  drop_na() %&gt;% \n  ungroup() %&gt;% \n  mutate(\n         prop_x = table1$prop_x,\n         prop = n/sum(n),\n         rangeleft = lag(cumsum(prop), default = 0),\n         cumrelfreq = cumsum(prop),\n         share_decile = weight_share(cumrelfreq),\n         share_rest = 1 - weight_share(cumrelfreq)) %&gt;% \n    decile_dummies(rangeleft = rangeleft, rangeright = cumrelfreq)\n\n\ndecile_columns &lt;- which(colnames(table)==\"d1\" ):which(colnames(table)==\"d10\")\n\nlist(table = table,\n     decile = decile(data = table, columns = decile_columns, prop = table$prop_x, share_decile = table$share_decile, share_rest = table$share_rest))\n\n\n}"
  },
  {
    "objectID": "posts/trust institution switzerland/trust institution switzerland.html",
    "href": "posts/trust institution switzerland/trust institution switzerland.html",
    "title": "Political stability and political crises in Switzerland: empirical evidence",
    "section": "",
    "text": "Switzerland is often considered as a successful and well-performing economy. The country typically ranks among the firsts in a wide variety of living standard measures such as the human development index, GDP per capita and so on. Along with the good performances of the economy, one expects that Swiss people also have more trust in their institutions compared with other countries. If one looks at, for instance, the data provided by the OECD on the confidence in government and other institutions, Switzerland is indeed one of the countries with the highest level of trust:\n\n\n\nSource: OECD (Government at Glance, 2021)\n\n\nIn Switzerland, the confidence in national government in 2020 was the highest among OECD countries with a level of 85%. Other countries like France and Italy have a level of confidence lower than the average.\n\n\n\nSource: OECD (Government at Glance, 2021)\n\n\nIf one looks at other data provided by the OECD, Switzerland still ranks among the highest. In the graph above, Swiss people show a great deal of confidence in the police (about 85%), parliament (60%) and government (70%). This stands in sharp contrast with France, Italy and the US (which shows very low level of confidence in parliament with 15%). If one tries therefore to assess the evolution of trust in institutions in Switzerland, one thus expects to find a great stability. Nonetheless, one period in recent Swiss history represents an exception: the crisis of the 1990s. This period was marked by recession and was accompanied by a decline in trust in institutions (Brunner and Sgier 1997).\nBut why is the evolution of trust in institutions important? For my research, I am interested in social conflict, political cleavages and institutional change. The Neorealism approach developped by Amable and Palombarini (2005; 2008), which is dedicated to the analysis of those topics, offers insightful concepts and methods that I will mobilize here in assessing the evolution of Switzerland’s trust in its institutions in the last decades. Until now, Neorealism has been first and foremost mobilized by its founders to analyse France and Italy (Bruno Amable and Palombarini 2014, 2018). In their work, they typically start with the fact that Italy and France went under periods of deep political crises. Leaving their conceptual definitions of crisis (which are really interesting) aside and focusing only on how they identify it empirically, they typically expose data on trust in politicians, institutions, government and in political turnover. However, it is rather straightforward to anyone interested in politics and contemporary history that Italy and France underwent political crises, and data like the ones above only serve to describe well-known facts. The situation is totally different for Switzerland, which is why it is interesting to study this country in a Neorealist approach.\nThis post will hence be an attempt to periodize, analyse and assess the evolution of trust in institutions in Switzerland as a way to inderectly measure the country’s political stability through time. The objective here is, by using post-electoral survey, to identify the trends in trust in institutions and assess whether and when Switzerland can be characterized as a politically stable country, as opposed to France and Italy. The figures above already show that Switzerland is characterized with a relatively high level of trust, so the goal here is to explore other data which could also allow to see the temporal evolution and thus identify the periods of political stability and crisis in Switzerland.\n\n\nLet’s first see what Selects data offer. The Swiss Election Studies (SELECTS) contains a cumulative dataset which is of particular interest.\nThis dataset is in fact a harmonized dataset of all the successive post-electoral studies that have been conducted in Switzerland between 1971 and 2019 by different universities. This dataset contains a set of “trust in political institutions” variables which are of interest, but not all of them can be used because they are not avaliable for each year of the dataset. The ones which have the most time coverage are the following variables and that I will analyse here are described in the following table:\n\n\n\n\n\n\n\n\n\nCode\nname\nvalues\nYears\n\n\n\n\ntrust1\ntrust in federal council\n0 (no trust) to 10 (full trust) with decimal values\nnot 1979, 2015, 2019\n\n\ntrust2\ntrust in national parliament\n0 (no trust) to 10 (full trust) with decimal values\n1991, 1995, 1999, 2003, 2007, 2011\n\n\ntrust3\ntrust in cantonal authorities\n0 (no trust) to 10 (full trust) with decimal values\n1995, 1999, 2003, 2007, 2011\n\n\ntrust5\ntrust in national political parties\n0 (no trust) to 10 (full trust) with decimal values\n1999, 2003, 2007, 2011\n\n\neps1\nevaluation: political system/democracy\nscale from 0 to 1, with decimal values\n1971, 1979, 1995, 1999, 2003, 2007, 2011, 2015, 2019"
  },
  {
    "objectID": "posts/trust institution switzerland/trust institution switzerland.html#political-stability-and-crisis-in-switzerland",
    "href": "posts/trust institution switzerland/trust institution switzerland.html#political-stability-and-crisis-in-switzerland",
    "title": "Political stability and political crises in Switzerland: empirical evidence",
    "section": "",
    "text": "Switzerland is often considered as a successful and well-performing economy. The country typically ranks among the firsts in a wide variety of living standard measures such as the human development index, GDP per capita and so on. Along with the good performances of the economy, one expects that Swiss people also have more trust in their institutions compared with other countries. If one looks at, for instance, the data provided by the OECD on the confidence in government and other institutions, Switzerland is indeed one of the countries with the highest level of trust:\n\n\n\nSource: OECD (Government at Glance, 2021)\n\n\nIn Switzerland, the confidence in national government in 2020 was the highest among OECD countries with a level of 85%. Other countries like France and Italy have a level of confidence lower than the average.\n\n\n\nSource: OECD (Government at Glance, 2021)\n\n\nIf one looks at other data provided by the OECD, Switzerland still ranks among the highest. In the graph above, Swiss people show a great deal of confidence in the police (about 85%), parliament (60%) and government (70%). This stands in sharp contrast with France, Italy and the US (which shows very low level of confidence in parliament with 15%). If one tries therefore to assess the evolution of trust in institutions in Switzerland, one thus expects to find a great stability. Nonetheless, one period in recent Swiss history represents an exception: the crisis of the 1990s. This period was marked by recession and was accompanied by a decline in trust in institutions (Brunner and Sgier 1997).\nBut why is the evolution of trust in institutions important? For my research, I am interested in social conflict, political cleavages and institutional change. The Neorealism approach developped by Amable and Palombarini (2005; 2008), which is dedicated to the analysis of those topics, offers insightful concepts and methods that I will mobilize here in assessing the evolution of Switzerland’s trust in its institutions in the last decades. Until now, Neorealism has been first and foremost mobilized by its founders to analyse France and Italy (Bruno Amable and Palombarini 2014, 2018). In their work, they typically start with the fact that Italy and France went under periods of deep political crises. Leaving their conceptual definitions of crisis (which are really interesting) aside and focusing only on how they identify it empirically, they typically expose data on trust in politicians, institutions, government and in political turnover. However, it is rather straightforward to anyone interested in politics and contemporary history that Italy and France underwent political crises, and data like the ones above only serve to describe well-known facts. The situation is totally different for Switzerland, which is why it is interesting to study this country in a Neorealist approach.\nThis post will hence be an attempt to periodize, analyse and assess the evolution of trust in institutions in Switzerland as a way to inderectly measure the country’s political stability through time. The objective here is, by using post-electoral survey, to identify the trends in trust in institutions and assess whether and when Switzerland can be characterized as a politically stable country, as opposed to France and Italy. The figures above already show that Switzerland is characterized with a relatively high level of trust, so the goal here is to explore other data which could also allow to see the temporal evolution and thus identify the periods of political stability and crisis in Switzerland.\n\n\nLet’s first see what Selects data offer. The Swiss Election Studies (SELECTS) contains a cumulative dataset which is of particular interest.\nThis dataset is in fact a harmonized dataset of all the successive post-electoral studies that have been conducted in Switzerland between 1971 and 2019 by different universities. This dataset contains a set of “trust in political institutions” variables which are of interest, but not all of them can be used because they are not avaliable for each year of the dataset. The ones which have the most time coverage are the following variables and that I will analyse here are described in the following table:\n\n\n\n\n\n\n\n\n\nCode\nname\nvalues\nYears\n\n\n\n\ntrust1\ntrust in federal council\n0 (no trust) to 10 (full trust) with decimal values\nnot 1979, 2015, 2019\n\n\ntrust2\ntrust in national parliament\n0 (no trust) to 10 (full trust) with decimal values\n1991, 1995, 1999, 2003, 2007, 2011\n\n\ntrust3\ntrust in cantonal authorities\n0 (no trust) to 10 (full trust) with decimal values\n1995, 1999, 2003, 2007, 2011\n\n\ntrust5\ntrust in national political parties\n0 (no trust) to 10 (full trust) with decimal values\n1999, 2003, 2007, 2011\n\n\neps1\nevaluation: political system/democracy\nscale from 0 to 1, with decimal values\n1971, 1979, 1995, 1999, 2003, 2007, 2011, 2015, 2019"
  },
  {
    "objectID": "posts/trust institution switzerland/trust institution switzerland.html#trust-in-federal-council",
    "href": "posts/trust institution switzerland/trust institution switzerland.html#trust-in-federal-council",
    "title": "Political stability and political crises in Switzerland: empirical evidence",
    "section": "Trust in Federal Council",
    "text": "Trust in Federal Council\n\n\n\n\n\n\n\n\n\nHere is a basic visualisation to show the distribution of the variable for each year. It is strinking how the distribution is almost the same for the years 1971-1975-1987 and then from 1995 to 2011. This suggests a great stability of trust in Federal Council, which seems rather strong. However, since the graph above is rather difficult to interpret due to the fact that it carries too much information and the scales change through the years, I will create an indicator variable to simplify the information. The dummy variable will take value 1 if the respondent takes a value higher than 5 (the middle point level of indifference), and 0 otherwise.\n\n\n\n\n\n\n\n\n\nThe graph above represents the percentage of respondents who declared to always or most of the time trust the federal council. The only period this trust declined was between 1975 and 1991. However, one should be cautious with the strong decline in 1991, because the scale (number of values the ordinal variable can take) changes for the first time for this year (see the precedent graph above). In fact, the strong decline in 1991 is surely due to the fact that the scale, by including more response choices in the survey (from 4 to 7 points scale), perhaps induced more “middle-point” responses (the middle answer is the mode for 1991). From 1995 to 2011, the trust in federal council shows a great stability, and even increases between 2007 and 2011. Nonetheless, the variable does not include data after 2011, which is unfortunate. Fortunately, there are other variables which cover a bigger time frame. In this regard, the variable on the evaluation of the Swiss democracy/political system (eps1) is interesting:\n\n\n\n\n\n\n\n\n\nThe graph above shows how strong trust in institutions is in Switzerland and that the latter has even increased in the last years until 2019. Note that the data for 1995 have the same problem as for the year 1991 in the previous graph (change in the variable’s number of categories). I can thus conclude that the trust in institutions is not only stable in Switzerland, but even increasing. One further step, however, is to look at how trust changes if one takes education and income levels into account: do poorer people and less educated people have less trust in institutions in Switzerland? We will see that the picture is slightly more contrasted trust is analyzed for each income level.\n\n\n\n\n\n\n\n\n\nTaking into account income quintiles offer a slightly more contrasted results, but trust is high even for low income levels. There is no strong differences in trust if we compare the lowest quintile with the highest (with still somme difference in each year), but one can see that trust increases with income group: richer Swiss people (in terms of income) have more confidence in the political system/democracy in this sample.\nSince the dataset also gives the education group, one can do the same graph above with education:\n\n\n\n\n\n\n\n\n\nThere is overall a positive association between education level and trust in democracy/political system, even though trust is high and systematically above 50% in each year (with one exception for 1979).\n\nAlternative data source: the European Social Survey (ESS)\nAs we have seen, Selects data have some shortcomings which prevent to conduct a serious evaluation of the evolution of trust in institutions. One thus has to look at other data. To do so, I will now analyse the Swiss trust in institutions using ESS data.\n\n\n\n\n\n\n\n\nvariable\nname\nscale\n\n\n\n\ntrstplt\ntrust in politician\n0 = no trust, 10 = complete trust (discrete scale)\n\n\ntrstprl\nTrust in country’s parliament\n0 = no trust, 10 = complete trust (discrete scale)\n\n\nstfeco\nHow satisfied with present state of economy in country\n0 = extremely dissatisfied, 1 = extremely satisfied (discrete scale)\n\n\nstfgov\nHow satisfied with the national government\n0 = extremely dissatisfied, 1 = extremely satisfied (discrete scale)\n\n\nstfdem\nHow satisfied with the way democracy works in country\n0 = extremely dissatisfied, 1 = extremely satisfied (discrete scale)\n\n\nagea\nAge of respondent, calculated\n\n\n\nedulvla\nHighest level of education\n0 = not possible to harmonize, 1 = less than secondary, 5 = tertiary education completed\n\n\nedlvch\nHighest level of education (Switzerland)\n1 = incomplete compulsory school, 15 = university, 16 = other (discrete scale)\n\n\nhinctnt\nHousehold’s total net income, all sources\n\n\n\n\nAn advantage of the ESS data compared with Selects is the comparability of the trust in institutions variables. As we saw in the first graphs, Selects have changed the scale of their variables through the years, which makes comparison difficult even with a cumulative and harmonized dataset. ESS variables (see table above) do not change their scale, so one can compare data from 2002 and 2020 without facing the problems I had with Selects data.\n\n\nWarning: le package 'naniar' a été compilé avec la version R 4.2.3\n\n\nLet’s first have a look at how the variable is distributed for a given year, for instance 2020:\n\n\n\n\n\n\n\n\n\nThe graph is coherent with all the previous data we saw: most Swiss people have confidence and trust in the politicians of their country in 2020. What about the evolution of this trust if one looks at all the rounds provided by ESS since 2002? To have a look a this, I load all the ESS rounds, create an indicator variable (still from the same variable, trstplt) taking value 1 if the respondent takes at least value 5 and plot the proportion for each year:\n\n\n\n\n\nIf one considers trust in politicians as a good indicator of political stability, Switzerland shows a great and increasing level of stability and it would be difficult to say that the country has known any deep political crisis in the last years. The political stability of Switzerland is even more striking when compared with countries which have known instability and systemic crises such as Italy and France:\n\n\n\n\n\n\n\n\n\n\n\nHowever, ESS data are rather limited with respect to the time frame: only data from 2002 to 2020 are available. Ideally, I would like to have data which come back at least to the 1980s. In my opinion, both Selects and ESS data have serious shortcomings that prevent any straightforward conclusion regarding the evolution of political stability in Switzerland. For this reason, I searched other data source and found one which can give better results and insights.\n\n\nLast data source: Vox data\nA final and last alternative data source to assess the temporal evolution of trust in institutions in Switzerland (and thus its period of political stability/crises) are the “Vox” studies, which are conducted for after Swiss referendum (“votation populaire” in French) since 1981. Kriesi, Brunner and Lorétan compiled and standardized every Vox studies from 1981 up to 2016 to create a cumulative dataset (Kriesi, Brunner, and Lorétan 2017). This dataset has a variable of interest, “a22 (confidence in government)”, which is a binary variable (either confidence of mistrust in government). The graph below plots the percentage of respondents having confidence in government for each year:\n\n\n\n\n\nWe finally have a satisfying and valuable graph: the time frame is large enough, the variable is the same for each year and the results are far more contrasted and interesting than the previous graphs. One can see clearly different periods of declining and increasing confidence in government, which lead to the conclusion that Switzerland knew different waves of decreasing and increasing political stability as well as clear periods of political crisis. For instance, the graph illustrates well the political crisis of the 1990s: the percentage of people who declared having confidence in government fell from 65% in 1998 to 38.9% in 1995. Confidence in government then fell a for second time from 2001 (64%) to 2005 (38.4%), a period which can thus be considered as a second political crisis due to the very low level of confidence in 2005. But about the period post-2016? The shortcoming of this dataset is that it does not go beyond 2016. Fortunately, another dataset gathered data for the period 2016-2020."
  },
  {
    "objectID": "posts/vox and voto analysis/vox_voto_analysis.html",
    "href": "posts/vox and voto analysis/vox_voto_analysis.html",
    "title": "Political Conflict and Inequality in Switzerland",
    "section": "",
    "text": "How does social inequality affect political cleavages and political conflict? A recent trend in political economy has developed and built a tremendous analysis of the evolution of political cleavages in almost all democratic countries around the world. This sudden interest, whose origin can be traced back to Piketty’s last book (Piketty 2019), gave rise to the World Political Cleavages and Inequality Database (WPID) and to a book which analyses the effect of income, social and educational inequality on the structure and transformation of political cleavages from 1948 to 2020 in 50 countries (Gethin, Martinez-Toledano, and Piketty 2021).\nWhy have political cleavages and political conflict become popular among economists such as Piketty, who were originally more focused on inequality analysis? An answer could be the inequality paradox: the fact that rising inequality in almost all developed countries in the world has no paved the way to growing support for left and far-left parties, income and wealth redistribution and for political contest in general. Political economists such as Piketty have spent years studying inequality, how the latter has evolved and can disturb political and economic stability and how they can be reduced to make society more equal. However, whereas the technical solutions are here, political outcomes are not. Political economists have thus realized that the persistence of inequality is not only technical problem, but also a political one and that inequality analysis musts thus take politics into account.\nIt is interesting to note how the same reasoning can be applied to ecology. Ecologists face indeed the same issue: global warming becomes increasingly more threatening, and scholars know how it could be stopped or at least reduced, but there is paradoxically no political outcome and no serious political measure which has been taken despite the emergency.\nThe work of Piketty and coauthors can be summarized as follows: in the post-war period, political cleavages were bipolar with respectively a left a right bloc. Support for social-democratic parties was negatively correlated with income and education: the lower the income and level of education, the higher the probability to support the left and left economic policies. Conversely, support for right-wing parties was positively linked with income and educational level. Political cleavage was thus “classist” and bipolar in the sense that the political positioning of an individual on the left-right axis was mainly determined by his position in the economic structure.\nThe situation changed around the 1970s and 1980s: the support for the left became positively correlated with education whereas the support for the right remained linked positively with income and education. Political cleavages became thus “multipolar” by combining an economic cleavage (degree of income and wealth), which became relatively less salient, with a growing “cultural” cleavage due to the increase in average educational level. People who are familiar with political science literature will here recognize that Piketty is simply re-asserting Inglehart’s famous “post-materialist” divide over cultural issue, but giving much importance to education as a source of this cleavage.\nThe left has thus become “brahmin” because supported by high education voters. The right remains “merchant” because it still is supported by the rich. Finally, a new bloc, the “social-nativist” bloc, gathers voters with low income and low educational level. Piketty shows great concern towards the latter, calling it often the “social-nativist trap” (Piketty 2019).\nAmable and Darcillon (2022) offer more contrasted results. Income levels are found to be still an important factor in the formation of political preferences in the classic left-right divide. They identify the possible alliance between the most educated and rich groups, giving rise to a bourgeois bloc. Amable and Palombarini notably described and analyzed how the traditional left and right blocs collapsed in France and how this phenomena allowed the emergence of a bourgeois bloc (led by Macron and his party) gathering the most educated and wealthy voters. In this perspective, the emergence of a bloc bourgeois does not imply the end of class conflict, but rather its reinforcement (Amable and Palombarini 2018).\n\n\n\nSwitzerland is considered by this literature as the country in which those transformations have been one of the most important. For instance, Durrer de la Sota et al. explain that “Switzerland is the country where the shift of the higher educated towards the left has been the most dramatic” (Durrer de la Sota, Gethin, and Martinez-Toledano 2021, 4). Switzerland is infamous for being the home of one of the most successful social-nativist party, the SVP which now the first political party in Switzerland. Green parties have also become an important force and they are separated between the liberal ecologists and social ecologists. The important elements of the WPID’s analysis of Switzerland are the following:\n\nThe authors used Swiss Election Studies (Selects) data, which are data collected after each parliamentary election.\nThe educational shift happened in the late 80s, the period 1983-7 being the last “class-based” period. Switzerland’s political landscape is now a multi-polar and multi-elite one: the support for the greens and socialists is linked positively with education, the rich still support radical/liberal parties and the SVP represents the social-nativist bloc gathering the low income and low education voters.\nThe traditional parties, the Socialists (Swiss Socialist Party SSP), the Liberals and Radicals, and the Christian Democrats are in a declining trend, if not collapsing.\nThe Greens andradical right are the new strong emergent parties, the latter being the most important bloc.\n\n\n\n\n\n\nThe present article will test Piketty’s Brahmin left vs Merchant right divide as well as the WPID’s findings on Switzerland. The objective is to explore the following research question:\n\nTo what extent is the support for the left linked positively with the level of education and negatively with the level of income\nTo what extent is the support for right parties linked positively with the level of income\nAre there parties for which the support increase with both the level of income and education?\n\n\n\n\nThese research questions can be given temporary hypothetical answers:\n\nBrahmin left hypothesis:\n\nThe support for left-wing parties (mostly SSP and the Greens) is linked positively with the level of education, with no clear link with the level of income.\n\nMerchant right hypothesis:\n\nThe support for traditional right-wing parties (Liberal-Radicals, Christian Democrats which have become recenty “the center”) is linked positively with the level of income.\n\nBourgeois bloc hypothesis\n\nThe support for right-wing parties is linked positively with both the level of income and education.\n\n\n\n\n\nAs mentioned above, the WPID’s findings for Switzerland are based on Selects data. Since using Selects data again would not make great sense, unless a very different and specific methodology is used, I tried my best to find another data source. I have thus found an interesting dataset, the standardized Voxlt surveys, which are post-electoral survey data that were collected after each referendum voting from 1981 to 2016. In those surveys, not only referendum voting decisions were collected, but also information on party identification, left-right positioning and various soc-dem characteristics.\nParty identification will obviously be the dependent variable. I create a dummy variable for each party support.\n\n\n\n\n\n\n\n\np02\nn\nprop\nlabel\n\n\n\n\n0\n1938\n0.0177440029\nPDC+PCS groupés\n\n\n1\n227\n0.0020783739\nPCS/CSP\n\n\n2\n6148\n0.0562900568\nPDC/CVP\n\n\n3\n905\n0.0082860282\nPEP/EVP\n\n\n4\n7511\n0.0687694561\nPRD/FDP\n\n\n5\n376\n0.0034425929\nPdL (PA)/FPS (AP)\n\n\n6\n3175\n0.0290697674\nPES/GPS\n\n\n7\n663\n0.0060703168\nAdI/LdU\n\n\n8\n580\n0.0053103827\nLega\n\n\n9\n963\n0.0088170665\nPLS/LPS\n\n\n10\n672\n0.0061527193\nPdT [POCH, PSA]/PdA [POCH, PSA]\n\n\n11\n282\n0.0025819447\nDS (AN)/SD (NA)\n\n\n12\n16326\n0.1494781176\nPSS/SPS\n\n\n13\n10603\n0.0970792895\nUDC/SVP\n\n\n14\n79\n0.0007233107\nFraP\n\n\n15\n161\n0.0014740890\nAV/GB\n\n\n16\n15\n0.0001373375\nAdG\n\n\n17\n143\n0.0013092840\nUDF/EDU\n\n\n18\n541\n0.0049533053\nPBD/BDP\n\n\n19\n836\n0.0076542758\nVerts Libéraux/GLP\n\n\n20\n4368\n0.0399926753\nPLR/FDP Die Liberalen\n\n\n30\n915\n0.0083775865\nAutre parti\n\n\n31\n4495\n0.0411554660\nPlusieurs partis\n\n\n32\n2123\n0.0194378319\nPas un parti, mais une personne\n\n\n33\n34607\n0.3168558872\nAucun parti\n\n\nNA\n10568\n0.0967588354\nNA\n\n\n\n\n\n\n\nThe two main explanatory variable will be income and educational level. On the one hand, the dataset contains the variable “educ” which is a 1-6 point scale variable on the highest achieved level of education of the respondant, 1 being mandatory school and 6 university. I do not here perform any modification on this variable and I will consider it as continuous. Considering this variable as continuous is not a ideal option since the distance between each group is not really the same, but since I will perform complex multilevel models with years as a level and varying slopes for education and income, considering this variable as a factor would make computation in R very hard and tedious (especially the glmer function, which can take a long time).\nOn the other hand, the dataset offers two variables for income: “nivmena” and “revenu”.\n\n\n\n\n\n\n\n\neduc\nn\nprop\nlabel\n\n\n\n\n1\n14982\n0.137172679\nEcole obligatoire\n\n\n2\n50054\n0.458286028\nApprentissage\n\n\n3\n9926\n0.090880791\nMaturité + école normale\n\n\n4\n11006\n0.100769090\nFormation prof. supérieure\n\n\n5\n5695\n0.052142465\nHaute école spécialisée\n\n\n6\n16640\n0.152353049\nUniversité + Polytechnique\n\n\nNA\n917\n0.008395898\nNA\n\n\n\n\n\n\n\nOn the other hand, the dataset offers two variables for income: “nivmena” and “revenu”. Nivmena, a 4 point scale ordinal variable (in categories), was collected until 1993 and revenu was then collected. the latter is a 5 points scale ordinal income brackets variable.\n\nvox %&gt;% \n  count(nivmena) %&gt;% \n  mutate(prop = n/sum(n), \n         label = as_character(nivmena)) %&gt;% \n  gt() %&gt;% \n  gt_theme_dark()\n\n\n\n\n\n\n\nnivmena\nn\nprop\nlabel\n\n\n\n\n1\n1887\n0.01727706\nElevé\n\n\n2\n6687\n0.06122505\nMoyen supérieur\n\n\n3\n12438\n0.11388024\nMoyen inférieur\n\n\n4\n3283\n0.03005860\nBas\n\n\nNA\n84925\n0.77755906\nNA\n\n\n\n\n\n\nvox %&gt;% \n  count(revenu) %&gt;% \n  mutate(prop = n/sum(n), \n         label = as_character(revenu)) %&gt;% \n  gt() %&gt;% \n  gt_theme_dark()\n\n\n\n\n\n\n\nrevenu\nn\nprop\nlabel\n\n\n\n\n1\n7886\n0.07220289\n3'000.- CHF et moins\n\n\n2\n17535\n0.16054752\nde 3'001 à 5'000.- CHF\n\n\n3\n18302\n0.16757004\nde 5'001 à 7'000.- CHF\n\n\n4\n11988\n0.10976012\nde 7'001 à 9'000.- CHF\n\n\n5\n12390\n0.11344076\n9'001.- CHF et plus\n\n\n8\n4784\n0.04380150\nnsp\n\n\nNA\n36335\n0.33267717\nNA\n\n\n\n\n\n\n\nI made the choice to merge the two variables by recoding the revenu variable so that it fits into the categories of the first variable. I do so by considering the first income bracket as the “low” category, the second as “inferior-middle”, the third and fourth as the “upper-middle” category and the fifth as the “upper” category.\n\n# Two ways to create the income variable\nvox &lt;- vox %&gt;% \n  mutate(income = if_else(revenu == 1 & is.na(nivmena), 4,\n                          if_else(revenu == 2 & is.na(nivmena), 3,\n                                  if_else(revenu %in% c(3,4) & is.na(nivmena), 2,\n                                          if_else(revenu == 5 & is.na(nivmena), 1,\n                                                  if_else(is.na(revenu) & annee &lt;= 1993, nivmena, NA))))))\n\n         \nvox$income &lt;- ifelse(vox$income == 4, -1,\n                    ifelse(vox$income == 3, 0,\n                        ifelse(vox$income == 2, 1,\n                          ifelse(vox$income == 1, 2, NA)))) # centering around zero, this will make the intercept more interpretable \n\n\n# for education, rescale the values so that the lowest level is equal to zero\nvox &lt;- vox %&gt;% \n  mutate(education = case_when(\n    educ == 1 ~ 0,\n    educ == 2 ~ 1,\n    educ == 3 ~ 2,\n    educ == 4 ~ 3,\n    educ == 5 ~ 4,\n    educ == 6 ~ 5,\n    is.na(educ) ~ NA\n  ))\n\n\n\n\n\nLet’s first see how support for the Swiss Socialist Party (SSP) evolved across income groups in the last decades. I expect the lower groups to support the SSP until the 1990s, when the poor have supposedly shifted their support from the SSP to other parties (mainly the SVP) or to abstention.\n\n\n\n\n\n\n\n\n\nThe graph above shows well the reversal of the SSP support. From 1981 to around 1992, the lowest income groups (1 and 2) were the groups that supported the SSP the most. The richest group (4) was clearly not supporting the SSP a lot until the middle of the 1990s, when it became the most supportive group!. A reversal has thus clearly happened during the 1990s: the support for SSP declined among the poorest and increased among the upper middle class and the richest.\nLet’s now see how the support for the radicals/Liberals evolved. The Radicals are surely one of the most important political bloc historically in Switzerland. Radicals literally created modern Switzerland in the middle of the 19th century. Radicals underwent a lot of transformations, fusions and alliances with other parties. From 1984 to 2008, their party was the “radical-democratic party” after having merged with Swiss democrates. In 2009, this party merged again with another, the Swiss Liberal party, to create the Liberal-Radical (PLR) party which is the party which exists until nowadays. The bourgeois element of this bloc has never been a taboo and the PLR can clearly be named a bourgeois party.\nThe bourgeois dimension of the PLR is also made clear by the graph below:\n\n\n\n\n\n\n\n\n\nWe can see that the radical-democrates (from 1981 to 2008) and then PLR (2009-2016) was always mainly supported by the richest income groups. Support among the poorest rarely went beyond the 10%.\nRegarding the support for Swiss People’s party (SVP), here is the graph below:\n\n\n\n\n\n\n\n\n\nThe graph illustrates well how the avarage support for SVP increased in almost all income groups, except for the highest group. The latter used to support SVP the most until the late 1990s. Moreover, we can see that the common and widespread idea that the SVP is the most successful among the poorest voters is an exaggeration. The support for SVP also increased among the third and second income group. It is interesting to note how a reversal happened in 1992, the year of the voting on the adhesion to the European Economic Area (EEA). Before 1992, the richest group used to be the most supportive of the SVP. After 1992, the support among the richest declined and is the lowest in almost every year.\n\n\n\nTo analyze the relationship between party support and income and education, let’s now run some regressions. I will run separate binary logit regression for each party support dummies with income and education as explanatory variable. Since we have a timeframe which goes from 1981 to 2016, a choice has to be made regarding which regression model to run in order to take years into account. The first and most simple option would be to run separate regressions for each year. This is the repeated cross-sectional regression option. Another option would be to run a multilevel regression with years as a level and with varying slopes for education and income.\nThe first option is a lot more convenient than the multilevel model, because it is a lot faster to run in R, especially when control variables are added in the model. However, multilevel model is more appropriated and gives more robust results and better inference. Since it would be also interesting to compare the results between the two options, both repeated cross-sectional and multilevel regressions will be run here.\n\n\nLet’s first run binary logit regression separately for each year of the dataset. The package purrr and map() function allows to do this quiet easily:\n\nglmer_ssp &lt;- \n  vox %&gt;% \n  split(.$annee_f) %&gt;% \n  map(~ glmer(pss ~ income + education + gender + (1 | canton_f),\n              data = .x, family = binomial(link = \"logit\")))\n\n\n\name_glmer_ssp_inc &lt;- \nglmer_ssp %&gt;% \n  map(~slopes(.x ,variables = \"income\")) %&gt;% \n  map_df(~summarise(.x, ame = mean(estimate),\n            high = mean(conf.high),\n            low = mean(conf.low))) %&gt;% \n           mutate(year = 1981:2016,\n                  variable = \"income\")\n\name_glmer_ssp_educ &lt;- \nglmer_ssp %&gt;% \n  map(~slopes(.x ,variables = \"education\")) %&gt;% \n  map_df(~summarise(.x, ame = mean(estimate),\n            high = mean(conf.high),\n            low = mean(conf.low))) %&gt;% \n           mutate(year = 1981:2016,\n                  variable = \"education\")\n\n\name_glmer_ssp &lt;- rbind(ame_glmer_ssp_inc, ame_glmer_ssp_educ)\n\name_glmer_ssp %&gt;% plot_ame()\n\n\n\n\n\n\n\n\n\nnprcs_ssp &lt;- \nvox %&gt;%\n  split(.$annee_f) %&gt;% \n  map(~ glm(pss ~ income + education, data = .x, family = binomial(link = \"logit\")))\n\nTo present the results, I compute average marginal effects (AME) for each year:\n\n# computing average marginal effect for each year\name_income_nprcrs_ssp &lt;- \nnprcs_ssp %&gt;% \n  map(~slopes(.x ,variables = \"income\")) %&gt;% \n  map_df(~summarise(.x, ame = mean(estimate),\n            high = mean(conf.high),\n            low = mean(conf.low))) %&gt;% \n           mutate(year = 1981:2016,\n                  variable = \"income\")\n\name_educ_nprcrs_ssp &lt;- \nnprcs_ssp %&gt;% \n  map(~slopes(.x ,variables = \"education\")) %&gt;% \n  map_df(~summarise(.x, ame = mean(estimate),\n            high = mean(conf.high),\n            low = mean(conf.low))) %&gt;% \n           mutate(year = 1981:2016, \n                  variable = \"educ\")\n\n\name_nprcrs_ssp &lt;- rbind(ame_income_nprcrs_ssp, ame_educ_nprcrs_ssp)\n\n\n## plot the AME for each year\nplot_ame_ssp &lt;- \name_nprcrs_ssp %&gt;% \n  plot_ame()+\n  labs(title = \"Probability to vote for SSP\",\n       subtitle = \"Repeated Cross-sectional logit regressions\",\n       y = \"\", x = \"Average Marginal Effect\")\nplot_ame_ssp\n\n\n\n\n\n\n\n\nThis graph offers interesting results. The support for SSP used to be strongly negatively associated with income in 1981-2, but this negative relationship declined a lot during the 80s. It recovered a bit during the 90s but then even became positive in the middle of the 1990s and positive again in the early 2000s. On the other hand, the support for SSP seems to be always linked positively with the level of education. We can thus say that the SSP was and still is indeed a brahmin party, and even endorse the role of a bourgeois party for some years when its support is linked positively with income and education.\nAnother way to represent the relationship between support for SSP and income and education is to estimates of probability differences between given values of the predictors using the comparisons() function, which compute (in our case) probability differences between values at certain levels. The graph below show comparisons between the highest and lowest income groups (4-1). For education, I compare the highest level of education (university) not with the lowest level (primary school), but with vocational trainees/apprenticeship because the latter is the most widespread highest achieved level of education in Switzerland, even nowadays.\n\n\n\n\n\n\n\n\n\nIn 1981, an individual of the highest income group was 28% less probable to support the SSP compared with someone from the lowest income income. This difference decreased substantially over the years, and is rather close to zero nowadays. This is not the case for education. In 1981, a highly educated individual from either university of polytechnic school had around 19% more probabilities to support the SSP than someone who completed an apprenticeship. This difference has decreased over the years, but is still quiet high, around 10% in 2016. This show that the SSP was and still is the party of the most educated, but not necessarily of the poorest or the richest.\nTo check whether the results above are robust, I run and present a second model which controls for soc-dem characteristics (age, gender, marital status, region):\n\nplot_ame_nprcs_ssp_controls &lt;- \name_nprcs_ssp_controls %&gt;% \n  plot_ame()+\n  labs(title = \"Probability to support the SSP\",\n       subtitle = \"Repeated Cross-sectional logit regressions, after controls\",\n       y = \"\", x = \"Average Marginal Effect\")\n\nplot_ame_nprcs_ssp_controls\n\n\n\n\n\n\n\n\nThe graph above does not imply any change to the broad picture, let’s see for comparisons\n\n\n\n\n\n\n\n\n\nThe differences in probability have slighlty change, but the conclusions here are in fine the same as the no-controls regression.\n\n\n\nThe results show how the support for SVP is clearly linked negatively with educational level. The relationship with income is less clear since a lot of the results are not statistically significant (the confidence interval cross 0 for most years). Here I will not give any conclusion before fitting the multilevel regression.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncomparisons_plr_educ &lt;- \n  nprcs_plr_controls %&gt;% \n  comparisons_educ()\n\ncomparisons_plr_income &lt;- \n  nprcs_plr_controls %&gt;% \n  comparisons_income()\n\n\ncomparisons_plr &lt;- rbind(comparisons_plr_educ, comparisons_plr_income)\n\ncomparisons_plr %&gt;% \n  plot_ame()+\n  labs(title = \"Difference in Predicted Probabilities: vote for radicals/liberals\",\n       subtitle = \"Repeated Cross-sectional regressions, with controls\")\n\n\n\n\n\n\n\n\nFinally, here are the results for PLR support. We can see that the PLR clearly fits into the bourgeois category with support linked positively with both education and income, the latter being the important factor even if it declined nonetheless.\nSome observations can still be made regarding the comparisons plot. The difference in probability between the richest and poorest in supporting the Radicals/Liberals was highly positive in the 1980s (around 28% in 1981). This difference decreased, but is still high. Most of the time, this difference is also positive between the highest educational group and the second lowest one (apprenticeship).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegarding the support for the Greens, it is associated positively with education. The relationship with income is not clear, the multilevel model will hopefully give better results.\n\n\n\nAll the models computed and presented have major drawbacks. First, estimates are surely biased because not all important and potentially confounding variables are included in the model. Since we are not here interested in causality, this does not constitute a major problem either. A bigger problem is the inference: many estimates are not statistically significant. To try to solve this issue, I will now run multilevel regressions. There are many choices regarding the levels: one can choose the canton as a group and run separate multilevel regression for each year. Alternatively, both years and canton can be included as a level in the regression.\n\n\nThe multilevel regressions, with years as a level, lead globally to the same conclusions as above.\n\nglmer_pss &lt;- glmer(data = vox,\n                   formula = pss ~ income + educ + gender + age_cat + married + regionL + religion + (income + educ | annee_f),\n                   family = binomial(link = \"logit\"))\n\n\n\n\n\n\n\n\n\n\nThe results are very close to the repeated cross-section model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis hypothesis is globally confirmed. The Swiss socialist party was clearing gathering the most educated and the lowest income groups in the beginning of the 1980s. Then, the negative association between ssp support and income declined during the 1980s, recovered a bit in the late 80s, then declined further and even became positive in the 90s and the beginning of the 2000s, making the SSP a bourgeois party during this period.\n\n\n\nThe Radicals-Liberals represent clearly a bourgeois bloc, gathering the support of the wealthiest and most educated in almost every year of the dataset. The SVP was a merchant right party until 1992. After this period, the SVP gather mainly support from low income and low level of education groups\n\ntbl_corr_plr &lt;- \nvox %&gt;% \n  group_by(annee, income) %&gt;%\n  count(plr) %&gt;% \n  drop_na() %&gt;% \n  mutate(prop = n/sum(n)) %&gt;% \n  filter(plr == 1) %&gt;% \n  pivot_wider(names_from = c(\"income\"), values_from = c(\"prop\", \"n\")) %&gt;% \n  drop_na() %&gt;% \n  ungroup() %&gt;% \n  mutate(party = \"plr\") %&gt;% \n  dplyr::select(-plr)\n\ntbl_corr_pss &lt;- \nvox %&gt;% \n  group_by(annee, income) %&gt;%\n  count(pss) %&gt;% \n  drop_na() %&gt;% \n  mutate(prop = n/sum(n)) %&gt;% \n  filter(pss == 1) %&gt;% \n  pivot_wider(names_from = c(\"income\"), values_from = c(\"prop\", \"n\")) %&gt;% \n  drop_na() %&gt;% \n  ungroup() %&gt;% \n  mutate(party = \"pss\") %&gt;% \n  dplyr::select(-pss)\n\ntbl_corr_udc &lt;- \nvox %&gt;% \n  group_by(annee, income) %&gt;%\n  count(udc) %&gt;% \n  drop_na() %&gt;% \n  mutate(prop = n/sum(n)) %&gt;% \n  filter(udc == 1) %&gt;% \n  pivot_wider(names_from = c(\"income\"), values_from = c(\"prop\", \"n\")) %&gt;% \n  drop_na() %&gt;% \n  ungroup() %&gt;% \n  mutate(party = \"udc\") %&gt;% \n  dplyr::select(-udc)\n\ntbl_corr &lt;- rbind(tbl_corr_plr, tbl_corr_pss, tbl_corr_udc)\n\ntbl_corr2 &lt;- \n  tbl_corr %&gt;% \n  dplyr::select(-matches(\"n_[1-4]\")) %&gt;% \n  pivot_wider(names_from = \"party\", values_from = matches(\"prop_[1-4]\"))\n\n  cor(tbl_corr2[,2:13]) %&gt;% \n    corrplot.mixed(order = \"hclust\", lower = \"ellipse\", upper = 'number', tl.cex = 0.6)"
  },
  {
    "objectID": "posts/vox and voto analysis/vox_voto_analysis.html#introduction-and-problématique",
    "href": "posts/vox and voto analysis/vox_voto_analysis.html#introduction-and-problématique",
    "title": "Political Conflict and Inequality in Switzerland",
    "section": "",
    "text": "How does social inequality affect political cleavages and political conflict? A recent trend in political economy has developed and built a tremendous analysis of the evolution of political cleavages in almost all democratic countries around the world. This sudden interest, whose origin can be traced back to Piketty’s last book (Piketty 2019), gave rise to the World Political Cleavages and Inequality Database (WPID) and to a book which analyses the effect of income, social and educational inequality on the structure and transformation of political cleavages from 1948 to 2020 in 50 countries (Gethin, Martinez-Toledano, and Piketty 2021).\nWhy have political cleavages and political conflict become popular among economists such as Piketty, who were originally more focused on inequality analysis? An answer could be the inequality paradox: the fact that rising inequality in almost all developed countries in the world has no paved the way to growing support for left and far-left parties, income and wealth redistribution and for political contest in general. Political economists such as Piketty have spent years studying inequality, how the latter has evolved and can disturb political and economic stability and how they can be reduced to make society more equal. However, whereas the technical solutions are here, political outcomes are not. Political economists have thus realized that the persistence of inequality is not only technical problem, but also a political one and that inequality analysis musts thus take politics into account.\nIt is interesting to note how the same reasoning can be applied to ecology. Ecologists face indeed the same issue: global warming becomes increasingly more threatening, and scholars know how it could be stopped or at least reduced, but there is paradoxically no political outcome and no serious political measure which has been taken despite the emergency.\nThe work of Piketty and coauthors can be summarized as follows: in the post-war period, political cleavages were bipolar with respectively a left a right bloc. Support for social-democratic parties was negatively correlated with income and education: the lower the income and level of education, the higher the probability to support the left and left economic policies. Conversely, support for right-wing parties was positively linked with income and educational level. Political cleavage was thus “classist” and bipolar in the sense that the political positioning of an individual on the left-right axis was mainly determined by his position in the economic structure.\nThe situation changed around the 1970s and 1980s: the support for the left became positively correlated with education whereas the support for the right remained linked positively with income and education. Political cleavages became thus “multipolar” by combining an economic cleavage (degree of income and wealth), which became relatively less salient, with a growing “cultural” cleavage due to the increase in average educational level. People who are familiar with political science literature will here recognize that Piketty is simply re-asserting Inglehart’s famous “post-materialist” divide over cultural issue, but giving much importance to education as a source of this cleavage.\nThe left has thus become “brahmin” because supported by high education voters. The right remains “merchant” because it still is supported by the rich. Finally, a new bloc, the “social-nativist” bloc, gathers voters with low income and low educational level. Piketty shows great concern towards the latter, calling it often the “social-nativist trap” (Piketty 2019).\nAmable and Darcillon (2022) offer more contrasted results. Income levels are found to be still an important factor in the formation of political preferences in the classic left-right divide. They identify the possible alliance between the most educated and rich groups, giving rise to a bourgeois bloc. Amable and Palombarini notably described and analyzed how the traditional left and right blocs collapsed in France and how this phenomena allowed the emergence of a bourgeois bloc (led by Macron and his party) gathering the most educated and wealthy voters. In this perspective, the emergence of a bloc bourgeois does not imply the end of class conflict, but rather its reinforcement (Amable and Palombarini 2018)."
  },
  {
    "objectID": "posts/vox and voto analysis/vox_voto_analysis.html#political-cleavages-political-conflict-and-inequality-in-switzerland",
    "href": "posts/vox and voto analysis/vox_voto_analysis.html#political-cleavages-political-conflict-and-inequality-in-switzerland",
    "title": "Political Conflict and Inequality in Switzerland",
    "section": "",
    "text": "Switzerland is considered by this literature as the country in which those transformations have been one of the most important. For instance, Durrer de la Sota et al. explain that “Switzerland is the country where the shift of the higher educated towards the left has been the most dramatic” (Durrer de la Sota, Gethin, and Martinez-Toledano 2021, 4). Switzerland is infamous for being the home of one of the most successful social-nativist party, the SVP which now the first political party in Switzerland. Green parties have also become an important force and they are separated between the liberal ecologists and social ecologists. The important elements of the WPID’s analysis of Switzerland are the following:\n\nThe authors used Swiss Election Studies (Selects) data, which are data collected after each parliamentary election.\nThe educational shift happened in the late 80s, the period 1983-7 being the last “class-based” period. Switzerland’s political landscape is now a multi-polar and multi-elite one: the support for the greens and socialists is linked positively with education, the rich still support radical/liberal parties and the SVP represents the social-nativist bloc gathering the low income and low education voters.\nThe traditional parties, the Socialists (Swiss Socialist Party SSP), the Liberals and Radicals, and the Christian Democrats are in a declining trend, if not collapsing.\nThe Greens andradical right are the new strong emergent parties, the latter being the most important bloc.\n\n\n\n\n\n\nThe present article will test Piketty’s Brahmin left vs Merchant right divide as well as the WPID’s findings on Switzerland. The objective is to explore the following research question:\n\nTo what extent is the support for the left linked positively with the level of education and negatively with the level of income\nTo what extent is the support for right parties linked positively with the level of income\nAre there parties for which the support increase with both the level of income and education?\n\n\n\n\nThese research questions can be given temporary hypothetical answers:\n\nBrahmin left hypothesis:\n\nThe support for left-wing parties (mostly SSP and the Greens) is linked positively with the level of education, with no clear link with the level of income.\n\nMerchant right hypothesis:\n\nThe support for traditional right-wing parties (Liberal-Radicals, Christian Democrats which have become recenty “the center”) is linked positively with the level of income.\n\nBourgeois bloc hypothesis\n\nThe support for right-wing parties is linked positively with both the level of income and education.\n\n\n\n\n\nAs mentioned above, the WPID’s findings for Switzerland are based on Selects data. Since using Selects data again would not make great sense, unless a very different and specific methodology is used, I tried my best to find another data source. I have thus found an interesting dataset, the standardized Voxlt surveys, which are post-electoral survey data that were collected after each referendum voting from 1981 to 2016. In those surveys, not only referendum voting decisions were collected, but also information on party identification, left-right positioning and various soc-dem characteristics.\nParty identification will obviously be the dependent variable. I create a dummy variable for each party support.\n\n\n\n\n\n\n\n\np02\nn\nprop\nlabel\n\n\n\n\n0\n1938\n0.0177440029\nPDC+PCS groupés\n\n\n1\n227\n0.0020783739\nPCS/CSP\n\n\n2\n6148\n0.0562900568\nPDC/CVP\n\n\n3\n905\n0.0082860282\nPEP/EVP\n\n\n4\n7511\n0.0687694561\nPRD/FDP\n\n\n5\n376\n0.0034425929\nPdL (PA)/FPS (AP)\n\n\n6\n3175\n0.0290697674\nPES/GPS\n\n\n7\n663\n0.0060703168\nAdI/LdU\n\n\n8\n580\n0.0053103827\nLega\n\n\n9\n963\n0.0088170665\nPLS/LPS\n\n\n10\n672\n0.0061527193\nPdT [POCH, PSA]/PdA [POCH, PSA]\n\n\n11\n282\n0.0025819447\nDS (AN)/SD (NA)\n\n\n12\n16326\n0.1494781176\nPSS/SPS\n\n\n13\n10603\n0.0970792895\nUDC/SVP\n\n\n14\n79\n0.0007233107\nFraP\n\n\n15\n161\n0.0014740890\nAV/GB\n\n\n16\n15\n0.0001373375\nAdG\n\n\n17\n143\n0.0013092840\nUDF/EDU\n\n\n18\n541\n0.0049533053\nPBD/BDP\n\n\n19\n836\n0.0076542758\nVerts Libéraux/GLP\n\n\n20\n4368\n0.0399926753\nPLR/FDP Die Liberalen\n\n\n30\n915\n0.0083775865\nAutre parti\n\n\n31\n4495\n0.0411554660\nPlusieurs partis\n\n\n32\n2123\n0.0194378319\nPas un parti, mais une personne\n\n\n33\n34607\n0.3168558872\nAucun parti\n\n\nNA\n10568\n0.0967588354\nNA\n\n\n\n\n\n\n\nThe two main explanatory variable will be income and educational level. On the one hand, the dataset contains the variable “educ” which is a 1-6 point scale variable on the highest achieved level of education of the respondant, 1 being mandatory school and 6 university. I do not here perform any modification on this variable and I will consider it as continuous. Considering this variable as continuous is not a ideal option since the distance between each group is not really the same, but since I will perform complex multilevel models with years as a level and varying slopes for education and income, considering this variable as a factor would make computation in R very hard and tedious (especially the glmer function, which can take a long time).\nOn the other hand, the dataset offers two variables for income: “nivmena” and “revenu”.\n\n\n\n\n\n\n\n\neduc\nn\nprop\nlabel\n\n\n\n\n1\n14982\n0.137172679\nEcole obligatoire\n\n\n2\n50054\n0.458286028\nApprentissage\n\n\n3\n9926\n0.090880791\nMaturité + école normale\n\n\n4\n11006\n0.100769090\nFormation prof. supérieure\n\n\n5\n5695\n0.052142465\nHaute école spécialisée\n\n\n6\n16640\n0.152353049\nUniversité + Polytechnique\n\n\nNA\n917\n0.008395898\nNA\n\n\n\n\n\n\n\nOn the other hand, the dataset offers two variables for income: “nivmena” and “revenu”. Nivmena, a 4 point scale ordinal variable (in categories), was collected until 1993 and revenu was then collected. the latter is a 5 points scale ordinal income brackets variable.\n\nvox %&gt;% \n  count(nivmena) %&gt;% \n  mutate(prop = n/sum(n), \n         label = as_character(nivmena)) %&gt;% \n  gt() %&gt;% \n  gt_theme_dark()\n\n\n\n\n\n\n\nnivmena\nn\nprop\nlabel\n\n\n\n\n1\n1887\n0.01727706\nElevé\n\n\n2\n6687\n0.06122505\nMoyen supérieur\n\n\n3\n12438\n0.11388024\nMoyen inférieur\n\n\n4\n3283\n0.03005860\nBas\n\n\nNA\n84925\n0.77755906\nNA\n\n\n\n\n\n\nvox %&gt;% \n  count(revenu) %&gt;% \n  mutate(prop = n/sum(n), \n         label = as_character(revenu)) %&gt;% \n  gt() %&gt;% \n  gt_theme_dark()\n\n\n\n\n\n\n\nrevenu\nn\nprop\nlabel\n\n\n\n\n1\n7886\n0.07220289\n3'000.- CHF et moins\n\n\n2\n17535\n0.16054752\nde 3'001 à 5'000.- CHF\n\n\n3\n18302\n0.16757004\nde 5'001 à 7'000.- CHF\n\n\n4\n11988\n0.10976012\nde 7'001 à 9'000.- CHF\n\n\n5\n12390\n0.11344076\n9'001.- CHF et plus\n\n\n8\n4784\n0.04380150\nnsp\n\n\nNA\n36335\n0.33267717\nNA\n\n\n\n\n\n\n\nI made the choice to merge the two variables by recoding the revenu variable so that it fits into the categories of the first variable. I do so by considering the first income bracket as the “low” category, the second as “inferior-middle”, the third and fourth as the “upper-middle” category and the fifth as the “upper” category.\n\n# Two ways to create the income variable\nvox &lt;- vox %&gt;% \n  mutate(income = if_else(revenu == 1 & is.na(nivmena), 4,\n                          if_else(revenu == 2 & is.na(nivmena), 3,\n                                  if_else(revenu %in% c(3,4) & is.na(nivmena), 2,\n                                          if_else(revenu == 5 & is.na(nivmena), 1,\n                                                  if_else(is.na(revenu) & annee &lt;= 1993, nivmena, NA))))))\n\n         \nvox$income &lt;- ifelse(vox$income == 4, -1,\n                    ifelse(vox$income == 3, 0,\n                        ifelse(vox$income == 2, 1,\n                          ifelse(vox$income == 1, 2, NA)))) # centering around zero, this will make the intercept more interpretable \n\n\n# for education, rescale the values so that the lowest level is equal to zero\nvox &lt;- vox %&gt;% \n  mutate(education = case_when(\n    educ == 1 ~ 0,\n    educ == 2 ~ 1,\n    educ == 3 ~ 2,\n    educ == 4 ~ 3,\n    educ == 5 ~ 4,\n    educ == 6 ~ 5,\n    is.na(educ) ~ NA\n  ))"
  },
  {
    "objectID": "posts/vox and voto analysis/vox_voto_analysis.html#exploratory-analysis",
    "href": "posts/vox and voto analysis/vox_voto_analysis.html#exploratory-analysis",
    "title": "Political Conflict and Inequality in Switzerland",
    "section": "",
    "text": "Let’s first see how support for the Swiss Socialist Party (SSP) evolved across income groups in the last decades. I expect the lower groups to support the SSP until the 1990s, when the poor have supposedly shifted their support from the SSP to other parties (mainly the SVP) or to abstention.\n\n\n\n\n\n\n\n\n\nThe graph above shows well the reversal of the SSP support. From 1981 to around 1992, the lowest income groups (1 and 2) were the groups that supported the SSP the most. The richest group (4) was clearly not supporting the SSP a lot until the middle of the 1990s, when it became the most supportive group!. A reversal has thus clearly happened during the 1990s: the support for SSP declined among the poorest and increased among the upper middle class and the richest.\nLet’s now see how the support for the radicals/Liberals evolved. The Radicals are surely one of the most important political bloc historically in Switzerland. Radicals literally created modern Switzerland in the middle of the 19th century. Radicals underwent a lot of transformations, fusions and alliances with other parties. From 1984 to 2008, their party was the “radical-democratic party” after having merged with Swiss democrates. In 2009, this party merged again with another, the Swiss Liberal party, to create the Liberal-Radical (PLR) party which is the party which exists until nowadays. The bourgeois element of this bloc has never been a taboo and the PLR can clearly be named a bourgeois party.\nThe bourgeois dimension of the PLR is also made clear by the graph below:\n\n\n\n\n\n\n\n\n\nWe can see that the radical-democrates (from 1981 to 2008) and then PLR (2009-2016) was always mainly supported by the richest income groups. Support among the poorest rarely went beyond the 10%.\nRegarding the support for Swiss People’s party (SVP), here is the graph below:\n\n\n\n\n\n\n\n\n\nThe graph illustrates well how the avarage support for SVP increased in almost all income groups, except for the highest group. The latter used to support SVP the most until the late 1990s. Moreover, we can see that the common and widespread idea that the SVP is the most successful among the poorest voters is an exaggeration. The support for SVP also increased among the third and second income group. It is interesting to note how a reversal happened in 1992, the year of the voting on the adhesion to the European Economic Area (EEA). Before 1992, the richest group used to be the most supportive of the SVP. After 1992, the support among the richest declined and is the lowest in almost every year."
  },
  {
    "objectID": "posts/vox and voto analysis/vox_voto_analysis.html#modelling",
    "href": "posts/vox and voto analysis/vox_voto_analysis.html#modelling",
    "title": "Political Conflict and Inequality in Switzerland",
    "section": "",
    "text": "To analyze the relationship between party support and income and education, let’s now run some regressions. I will run separate binary logit regression for each party support dummies with income and education as explanatory variable. Since we have a timeframe which goes from 1981 to 2016, a choice has to be made regarding which regression model to run in order to take years into account. The first and most simple option would be to run separate regressions for each year. This is the repeated cross-sectional regression option. Another option would be to run a multilevel regression with years as a level and with varying slopes for education and income.\nThe first option is a lot more convenient than the multilevel model, because it is a lot faster to run in R, especially when control variables are added in the model. However, multilevel model is more appropriated and gives more robust results and better inference. Since it would be also interesting to compare the results between the two options, both repeated cross-sectional and multilevel regressions will be run here.\n\n\nLet’s first run binary logit regression separately for each year of the dataset. The package purrr and map() function allows to do this quiet easily:\n\nglmer_ssp &lt;- \n  vox %&gt;% \n  split(.$annee_f) %&gt;% \n  map(~ glmer(pss ~ income + education + gender + (1 | canton_f),\n              data = .x, family = binomial(link = \"logit\")))\n\n\n\name_glmer_ssp_inc &lt;- \nglmer_ssp %&gt;% \n  map(~slopes(.x ,variables = \"income\")) %&gt;% \n  map_df(~summarise(.x, ame = mean(estimate),\n            high = mean(conf.high),\n            low = mean(conf.low))) %&gt;% \n           mutate(year = 1981:2016,\n                  variable = \"income\")\n\name_glmer_ssp_educ &lt;- \nglmer_ssp %&gt;% \n  map(~slopes(.x ,variables = \"education\")) %&gt;% \n  map_df(~summarise(.x, ame = mean(estimate),\n            high = mean(conf.high),\n            low = mean(conf.low))) %&gt;% \n           mutate(year = 1981:2016,\n                  variable = \"education\")\n\n\name_glmer_ssp &lt;- rbind(ame_glmer_ssp_inc, ame_glmer_ssp_educ)\n\name_glmer_ssp %&gt;% plot_ame()\n\n\n\n\n\n\n\n\n\nnprcs_ssp &lt;- \nvox %&gt;%\n  split(.$annee_f) %&gt;% \n  map(~ glm(pss ~ income + education, data = .x, family = binomial(link = \"logit\")))\n\nTo present the results, I compute average marginal effects (AME) for each year:\n\n# computing average marginal effect for each year\name_income_nprcrs_ssp &lt;- \nnprcs_ssp %&gt;% \n  map(~slopes(.x ,variables = \"income\")) %&gt;% \n  map_df(~summarise(.x, ame = mean(estimate),\n            high = mean(conf.high),\n            low = mean(conf.low))) %&gt;% \n           mutate(year = 1981:2016,\n                  variable = \"income\")\n\name_educ_nprcrs_ssp &lt;- \nnprcs_ssp %&gt;% \n  map(~slopes(.x ,variables = \"education\")) %&gt;% \n  map_df(~summarise(.x, ame = mean(estimate),\n            high = mean(conf.high),\n            low = mean(conf.low))) %&gt;% \n           mutate(year = 1981:2016, \n                  variable = \"educ\")\n\n\name_nprcrs_ssp &lt;- rbind(ame_income_nprcrs_ssp, ame_educ_nprcrs_ssp)\n\n\n## plot the AME for each year\nplot_ame_ssp &lt;- \name_nprcrs_ssp %&gt;% \n  plot_ame()+\n  labs(title = \"Probability to vote for SSP\",\n       subtitle = \"Repeated Cross-sectional logit regressions\",\n       y = \"\", x = \"Average Marginal Effect\")\nplot_ame_ssp\n\n\n\n\n\n\n\n\nThis graph offers interesting results. The support for SSP used to be strongly negatively associated with income in 1981-2, but this negative relationship declined a lot during the 80s. It recovered a bit during the 90s but then even became positive in the middle of the 1990s and positive again in the early 2000s. On the other hand, the support for SSP seems to be always linked positively with the level of education. We can thus say that the SSP was and still is indeed a brahmin party, and even endorse the role of a bourgeois party for some years when its support is linked positively with income and education.\nAnother way to represent the relationship between support for SSP and income and education is to estimates of probability differences between given values of the predictors using the comparisons() function, which compute (in our case) probability differences between values at certain levels. The graph below show comparisons between the highest and lowest income groups (4-1). For education, I compare the highest level of education (university) not with the lowest level (primary school), but with vocational trainees/apprenticeship because the latter is the most widespread highest achieved level of education in Switzerland, even nowadays.\n\n\n\n\n\n\n\n\n\nIn 1981, an individual of the highest income group was 28% less probable to support the SSP compared with someone from the lowest income income. This difference decreased substantially over the years, and is rather close to zero nowadays. This is not the case for education. In 1981, a highly educated individual from either university of polytechnic school had around 19% more probabilities to support the SSP than someone who completed an apprenticeship. This difference has decreased over the years, but is still quiet high, around 10% in 2016. This show that the SSP was and still is the party of the most educated, but not necessarily of the poorest or the richest.\nTo check whether the results above are robust, I run and present a second model which controls for soc-dem characteristics (age, gender, marital status, region):\n\nplot_ame_nprcs_ssp_controls &lt;- \name_nprcs_ssp_controls %&gt;% \n  plot_ame()+\n  labs(title = \"Probability to support the SSP\",\n       subtitle = \"Repeated Cross-sectional logit regressions, after controls\",\n       y = \"\", x = \"Average Marginal Effect\")\n\nplot_ame_nprcs_ssp_controls\n\n\n\n\n\n\n\n\nThe graph above does not imply any change to the broad picture, let’s see for comparisons\n\n\n\n\n\n\n\n\n\nThe differences in probability have slighlty change, but the conclusions here are in fine the same as the no-controls regression.\n\n\n\nThe results show how the support for SVP is clearly linked negatively with educational level. The relationship with income is less clear since a lot of the results are not statistically significant (the confidence interval cross 0 for most years). Here I will not give any conclusion before fitting the multilevel regression.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncomparisons_plr_educ &lt;- \n  nprcs_plr_controls %&gt;% \n  comparisons_educ()\n\ncomparisons_plr_income &lt;- \n  nprcs_plr_controls %&gt;% \n  comparisons_income()\n\n\ncomparisons_plr &lt;- rbind(comparisons_plr_educ, comparisons_plr_income)\n\ncomparisons_plr %&gt;% \n  plot_ame()+\n  labs(title = \"Difference in Predicted Probabilities: vote for radicals/liberals\",\n       subtitle = \"Repeated Cross-sectional regressions, with controls\")\n\n\n\n\n\n\n\n\nFinally, here are the results for PLR support. We can see that the PLR clearly fits into the bourgeois category with support linked positively with both education and income, the latter being the important factor even if it declined nonetheless.\nSome observations can still be made regarding the comparisons plot. The difference in probability between the richest and poorest in supporting the Radicals/Liberals was highly positive in the 1980s (around 28% in 1981). This difference decreased, but is still high. Most of the time, this difference is also positive between the highest educational group and the second lowest one (apprenticeship).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegarding the support for the Greens, it is associated positively with education. The relationship with income is not clear, the multilevel model will hopefully give better results.\n\n\n\nAll the models computed and presented have major drawbacks. First, estimates are surely biased because not all important and potentially confounding variables are included in the model. Since we are not here interested in causality, this does not constitute a major problem either. A bigger problem is the inference: many estimates are not statistically significant. To try to solve this issue, I will now run multilevel regressions. There are many choices regarding the levels: one can choose the canton as a group and run separate multilevel regression for each year. Alternatively, both years and canton can be included as a level in the regression.\n\n\nThe multilevel regressions, with years as a level, lead globally to the same conclusions as above.\n\nglmer_pss &lt;- glmer(data = vox,\n                   formula = pss ~ income + educ + gender + age_cat + married + regionL + religion + (income + educ | annee_f),\n                   family = binomial(link = \"logit\"))\n\n\n\n\n\n\n\n\n\n\nThe results are very close to the repeated cross-section model."
  },
  {
    "objectID": "posts/vox and voto analysis/vox_voto_analysis.html#conclusion",
    "href": "posts/vox and voto analysis/vox_voto_analysis.html#conclusion",
    "title": "Political Conflict and Inequality in Switzerland",
    "section": "",
    "text": "This hypothesis is globally confirmed. The Swiss socialist party was clearing gathering the most educated and the lowest income groups in the beginning of the 1980s. Then, the negative association between ssp support and income declined during the 1980s, recovered a bit in the late 80s, then declined further and even became positive in the 90s and the beginning of the 2000s, making the SSP a bourgeois party during this period.\n\n\n\nThe Radicals-Liberals represent clearly a bourgeois bloc, gathering the support of the wealthiest and most educated in almost every year of the dataset. The SVP was a merchant right party until 1992. After this period, the SVP gather mainly support from low income and low level of education groups\n\ntbl_corr_plr &lt;- \nvox %&gt;% \n  group_by(annee, income) %&gt;%\n  count(plr) %&gt;% \n  drop_na() %&gt;% \n  mutate(prop = n/sum(n)) %&gt;% \n  filter(plr == 1) %&gt;% \n  pivot_wider(names_from = c(\"income\"), values_from = c(\"prop\", \"n\")) %&gt;% \n  drop_na() %&gt;% \n  ungroup() %&gt;% \n  mutate(party = \"plr\") %&gt;% \n  dplyr::select(-plr)\n\ntbl_corr_pss &lt;- \nvox %&gt;% \n  group_by(annee, income) %&gt;%\n  count(pss) %&gt;% \n  drop_na() %&gt;% \n  mutate(prop = n/sum(n)) %&gt;% \n  filter(pss == 1) %&gt;% \n  pivot_wider(names_from = c(\"income\"), values_from = c(\"prop\", \"n\")) %&gt;% \n  drop_na() %&gt;% \n  ungroup() %&gt;% \n  mutate(party = \"pss\") %&gt;% \n  dplyr::select(-pss)\n\ntbl_corr_udc &lt;- \nvox %&gt;% \n  group_by(annee, income) %&gt;%\n  count(udc) %&gt;% \n  drop_na() %&gt;% \n  mutate(prop = n/sum(n)) %&gt;% \n  filter(udc == 1) %&gt;% \n  pivot_wider(names_from = c(\"income\"), values_from = c(\"prop\", \"n\")) %&gt;% \n  drop_na() %&gt;% \n  ungroup() %&gt;% \n  mutate(party = \"udc\") %&gt;% \n  dplyr::select(-udc)\n\ntbl_corr &lt;- rbind(tbl_corr_plr, tbl_corr_pss, tbl_corr_udc)\n\ntbl_corr2 &lt;- \n  tbl_corr %&gt;% \n  dplyr::select(-matches(\"n_[1-4]\")) %&gt;% \n  pivot_wider(names_from = \"party\", values_from = matches(\"prop_[1-4]\"))\n\n  cor(tbl_corr2[,2:13]) %&gt;% \n    corrplot.mixed(order = \"hclust\", lower = \"ellipse\", upper = 'number', tl.cex = 0.6)"
  }
]